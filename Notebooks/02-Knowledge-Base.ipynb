{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9745409-7ce2-4059-a710-b58b9c105931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Vector Search Index from the processed/Chunked Data stored in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b5e1ab-230d-460f-aa5f-1c0615c30254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-vectorsearch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (0.63)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from databricks-vectorsearch) (2.1.0)\nRequirement already satisfied: mlflow-skinny<4,>=2.11.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from databricks-vectorsearch) (3.2.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from databricks-vectorsearch) (4.25.8)\nRequirement already satisfied: requests>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from databricks-vectorsearch) (2.32.5)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch) (24.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.76.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (8.6.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.32.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.32.1)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (2.12.5)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (6.0.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.15.0)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.34.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2025.1.31)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (2.40.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.0.11)\nRequirement already satisfied: zipp>=3.20 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.53b1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.4.2)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.14.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.9.1)\nRequirement already satisfied: anyio<5,>=3.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-980855f2-a4d0-4f32-b83d-55a431cd096c/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.12.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U -qqqq mlflow>=3.1.4 langchain==0.3.27 langgraph==0.6.11 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] databricks-feature-engineering==0.12.1 protobuf<5  cryptography<43 databricks-mcp\n",
    "# dbutils.library.restartPython()\n",
    "\n",
    "%pip install databricks-vectorsearch\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a1254b3-9448-4b37-a5c0-a9176e81eb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Vector Index in existing endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b72a95-8d0d-4493-98dd-1e7c83e0361e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VECTOR_SEARCH_ENDPOINT_NAME = \"databricks_doc_index\"\n",
    "catalog = \"workspace\"\n",
    "dbName = \"default\"\n",
    "\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{dbName}.databricks_docs_processed_chunks\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{dbName}.databricks_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bea2ac1-7656-4c07-9e4a-139da5a5c427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index workspace.default.databricks_index on endpoint databricks_doc_index...\nindex workspace.default.databricks_index on table workspace.default.databricks_docs_processed_chunks is ready\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "vsc.create_delta_sync_index(\n",
    "  endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "  index_name=vs_index_fullname,\n",
    "  source_table_name=source_table_fullname,\n",
    "  pipeline_type=\"TRIGGERED\",\n",
    "  primary_key=\"chunk_id\",\n",
    "  embedding_source_column='text', #The column containing our text\n",
    "  embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e7a5f01-dc4a-4bf1-8a28-0321cbf35716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Retrieval tool with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9445f84c-a15c-47a6-b26b-856097db49f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import VectorSearchRetrieverTool, ChatDatabricks\n",
    "\n",
    "\n",
    "# Retriever for API documentation\n",
    "generic_retriever = VectorSearchRetrieverTool(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\"],\n",
    "    tool_name=\"generic_doc_retriever\",\n",
    "    tool_description=\"Retrieves generic documentation for generic queries.\",\n",
    "    filters={\"doc_type\": \"general\"},\n",
    "    num_results=5,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "# Retriever for API documentation\n",
    "api_retriever = VectorSearchRetrieverTool(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\"],\n",
    "    tool_name=\"api_docs_retriever\",\n",
    "    tool_description=\"Retrieves API reference documentation.\",\n",
    "    filters={\"doc_type\": \"api_reference\"},\n",
    "    num_results=5,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "# Retriever for tutorials\n",
    "tutorial_retriever = VectorSearchRetrieverTool(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\"],\n",
    "    tool_name=\"tutorial_retriever\",\n",
    "    tool_description=\"Retrieves tutorial and how-to guides.\",\n",
    "    filters={\"doc_type\": \"tutorial\"},\n",
    "    num_results=5,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "# Retriever for code examples\n",
    "code_retriever = VectorSearchRetrieverTool(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\", \"has_code\"],\n",
    "    tool_name=\"code_examples_retriever\",\n",
    "    tool_description=\"Retrieves documentation with code examples.\",\n",
    "    filters={\"has_code\": \"true\"},\n",
    "    num_results=5,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "# Bind all tools to LLM\n",
    "llm = ChatDatabricks(endpoint=\"databricks-qwen3-next-80b-a3b-instruct\")\n",
    "llm_with_tools = llm.bind_tools([generic_retriever, api_retriever, tutorial_retriever, code_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74dfc86-e83c-4494-85a3-ee079d745570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Test Tool Calling with React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "857a3a47-c34a-4953-9e27-0798b9aa614e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-aa7cef4bcf11bab1d763aa4675fb848c\"",
      "text/plain": [
       "Trace(trace_id=tr-aa7cef4bcf11bab1d763aa4675fb848c)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = tutorial_retriever.invoke(\"how to use mlflow with databricks jobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be4ed60e-cbd7-4bcb-97d0-046d4d2f3665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use MLflow with Databricks jobs, you can integrate MLflow for experiment tracking and model management within scheduled Databricks workflows. Here's how you can do it:\n\n### Key Concepts:\n1. **Databricks Jobs**: These are non-interactive mechanisms to run notebooks or libraries on a scheduled or on-demand basis.\n2. **MLflow Experiments**: Used to track and organize machine learning training runs within Databricks.\n\n### Steps to Use MLflow with Databricks Jobs:\n\n1. **Set Up MLflow in Your Notebook**:\n   Ensure MLflow is properly configured in the notebook that will be part of your Databricks job. Include the necessary imports and tracking setup:\n   ```python\n   import mlflow\n   import mlflow.spark\n\n   # Set the tracking URI to use Databricks' managed MLflow\n   mlflow.set_tracking_uri(\"databricks\")\n\n   # Start an MLflow experiment\n   mlflow.set_experiment(\"/path/to/your/experiment\")\n   ```\n\n2. **Log Metrics and Parameters**:\n   Within your training or processing code, log parameters, metrics, and models using MLflow:\n   ```python\n   with mlflow.start_run():\n       mlflow.log_param(\"learning_rate\", 0.01)\n       mlflow.log_metric(\"accuracy\", 0.95)\n       mlflow.spark.log_model(spark_model, \"model\")\n   ```\n\n3. **Create a Databricks Job**:\n   - Navigate to the **Jobs** section in the Databricks UI.\n   - Create a new job and specify the notebook containing your MLflow-enabled code.\n   - Configure the schedule (e.g., daily, hourly) or trigger it manually.\n\n4. **Optional: Use MLflow with External Models**:\n   If you're using external models (e.g., OpenAI models), ensure you have MLflow with AI support installed:\n   ```bash\n   %pip install mlflow[genai]>=2.9.0\n   ```\n   Then, use the MLflow Deployments SDK to manage external model endpoints:\n   ```python\n   from mlflow.deployments import get_deploy_client\n\n   client = get_deploy_client(\"databricks\")\n   client.create_endpoint(name=\"my-external-model\", config={...})\n   ```\n\n5. **Monitor Results**:\n   After the job runs, you can view logged metrics, parameters, and models in the **MLflow Experiments** section of the Databricks UI.\n\n### Tips:\n- Use Databricks Secrets to securely handle credentials (e.g., API keys for external services).\n- Organize your MLflow experiments logically to track multiple versions or configurations of your models.\n- If your entire pipeline fits in a single notebook, you can schedule the notebook directly from the notebook UI.\n\nBy leveraging MLflow alongside Databricks jobs, you gain full traceability and centralization of your machine learning workflows.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-ea8e4378c59114defa0855be0a8b813c\"",
      "text/plain": [
       "Trace(trace_id=tr-ea8e4378c59114defa0855be0a8b813c)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create agent that automatically executes tools\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [generic_retriever, api_retriever, tutorial_retriever, code_retriever]\n",
    ")\n",
    "\n",
    "# Invoke - tools are automatically executed\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"how to use mlflow with databricks jobs\"}]\n",
    "})\n",
    "\n",
    "# Get the final answer\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b8e49f87-2771-45e1-8e51-d37f08ec2b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nQuery: how to use mlflow with databricks jobs\n\nTools Called: 1\n  1. tutorial_retriever\n     Query: how to use mlflow with databricks jobs\n\nFinal Answer:\nTo use MLflow with Databricks jobs, you can follow these steps based on the provided tutorial documentation:\n\n### 1. **Understand MLflow Integration with Databricks**\nMLflow is fully integrated into Databricks for experiment tracking, model training, and model management. Each MLflow experiment organizes training runs, logs metrics, parameters, and models, making it easy to track and manage machine learning workflows within Databricks.\n\n### 2. **Create a Databricks Job**\nDatabricks Jobs allow you to run notebooks or libraries on a scheduled or on-demand basis. You can use a notebook that includes MLflow code as a task in your job.\n\n- If your entire workflow is in a single notebook, you can schedule the notebook directly from the Databricks notebook UI.\n- If your workflow consists of multiple steps (e.g., data ingestion, preparation, and analysis), separate them into individual notebooks and create tasks in a single job.\n\n### 3. **Use MLflow in Your Notebook**\nWithin the notebook that will be executed as part of the Databricks job:\n- Import and initialize MLflow.\n- Start an MLflow run to track metrics, parameters, and models.\n- Log your model artifacts using `mlflow.log_model()` and other MLflow functions.\n\nExample:\n```python\nimport mlflow\nimport mlflow.sklearn\n\n# Set the experiment (optional if you want to organize runs under a specific experiment)\nmlflow.set_experiment(\"/Users/your-email@domain.com/mlflow-experiment\")\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Train your model (example)\n    model = SomeModel()\n    model.fit(X_train, y_train)\n    \n    # Log metrics and parameters\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_param(\"learning_rate\", 0.01)\n    \n    # Log the model\n    mlflow.sklearn.log_model(model, \"model\")\n```\n\n### 4. **Schedule the Job**\n- From the Databricks Jobs UI, create a new job and specify the notebook you created.\n- Define the schedule (e.g., daily, hourly) or trigger it manually.\n- Include any required cluster configurations and dependencies (e.g., installing MLflow libraries).\n\n### 5. **Ensure MLflow Libraries Are Installed**\nMake sure the required MLflow libraries are installed in your Databricks cluster. If using advanced features like external models, you may need to install specific MLflow extensions:\n```bash\n%pip install mlflow[genai]>=2.9.0\n```\n\n### 6. **Monitor and Manage Experiments**\nAfter scheduling your job, you can monitor the MLflow runs under the corresponding experiment in the MLflow UI (accessible from the Databricks workspace). Each run logs metrics, parameters, and models, allowing you to compare performance across job executions.\n\nBy combining MLflow's tracking capabilities with Databricks Jobs, you can build fully automated, reproducible, and traceable machine learning workflows.\n\nQuery: show me API docs for authentication\n\nTools Called: 1\n  1. api_docs_retriever\n     Query: authentication\n\nFinal Answer:\nThe API documentation related to authentication includes the following key points:\n\n1. **Personal Access Tokens**: \n   - Authentication often requires a personal access token.\n   - Users can configure a `.netrc` file to store the token, enabling the use of `-n` flag in `curl` commands for authentication.\n\n2. **Credentials ID**:\n   - For workspace creation via the API, you must provide a `credentials_id`, which refers to your cross-account role credentials tied to a credentials configuration object.\n\n3. **Optional Settings**:\n   - Some APIs allow optional fields like `deployment_name` or `custom_tags` for organizational purposes, but these are not directly tied to authentication.\n   - Note that API endpoints may enforce specific authentication requirements for operations like creating workspaces and configuring private access.\n\nFor specific endpoints, you’ll need to ensure your requests include valid authentication tokens or credentials. Check the [Databricks Admin API documentation](https://docs.databricks.com/en/admin/workspace/create-workspace-api.html) for further details on implementation.\n\nQuery: give me code examples for creating clusters\n\nTools Called: 1\n  1. code_examples_retriever\n     Query: creating clusters\n\nFinal Answer:\nHere are some code examples for creating clusters:\n\n### Example 1: Creating a Cluster in Kubernetes\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n```\n\n### Example 2: Creating a Cluster with Docker Swarm\n\n```bash\n# Initialize a Swarm cluster\ndocker swarm init\n\n# Add a worker node to the cluster\ndocker swarm join --token <TOKEN> <MANAGER-IP>:<PORT>\n\n# Deploy a service across the cluster\ndocker service create --name my-service --replicas 3 nginx:latest\n```\n\n### Example 3: Creating a Cluster with Apache Spark\n\n```python\nfrom pyspark.sql import SparkSession\n\n# Create a Spark session (this will automatically create a cluster if running in cluster mode)\nspark = SparkSession.builder \\\n    .appName(\"MyClusterApp\") \\\n    .config(\"spark.master\", \"spark://<MASTER-IP>:7077\") \\\n    .getOrCreate()\n\n# Example: Load data and perform operations\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\ndf.show()\n\nspark.stop()\n```\n\n### Example 4: Creating a Cluster with Hadoop\n\n```xml\n<!-- core-site.xml -->\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://<NAMENODE-HOST>:9000</value>\n    </property>\n</configuration>\n\n<!-- hdfs-site.xml -->\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>3</value>\n    </property>\n    <property>\n        <name>dfs.namenode.name.dir</name>\n        <value>file:/data/hadoop/namenode</value>\n    </property>\n    <property>\n        <name>dfs.datanode.data.dir</name>\n        <value>file:/data/hadoop/datanode</value>\n    </property>\n</configuration>\n\n<!-- Start the Hadoop cluster -->\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n```\n\nLet me know if you'd like examples for a specific platform or framework!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='give me code examples for creating clusters', additional_kwargs={}, response_metadata={}, id='9fff0801-5337-4799-a60b-8513e92cde4d'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_e0c7db80-e3b7-4bf9-9e06-9725f474b2a3', 'function': {'arguments': '{\"query\": \"creating clusters\"}', 'name': 'code_examples_retriever'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 1805, 'completion_tokens': 24, 'total_tokens': 1829}, 'prompt_tokens': 1805, 'completion_tokens': 24, 'total_tokens': 1829, 'model': 'qwen3-next-instruct-091725', 'model_name': 'qwen3-next-instruct-091725', 'finish_reason': 'tool_calls'}, id='run--d2d79033-7d20-4d53-a177-280148325ea2-0', tool_calls=[{'name': 'code_examples_retriever', 'args': {'query': 'creating clusters'}, 'id': 'call_e0c7db80-e3b7-4bf9-9e06-9725f474b2a3', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content=[], name='code_examples_retriever', id='c5c2d065-e154-44b6-b06f-635c8b13ff67', tool_call_id='call_e0c7db80-e3b7-4bf9-9e06-9725f474b2a3'),\n",
       "  AIMessage(content='Here are some code examples for creating clusters:\\n\\n### Example 1: Creating a Cluster in Kubernetes\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: my-pod\\nspec:\\n  containers:\\n  - name: my-container\\n    image: nginx:latest\\n    ports:\\n    - containerPort: 80\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\nspec:\\n  selector:\\n    app: my-app\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      targetPort: 80\\n  type: LoadBalancer\\n```\\n\\n### Example 2: Creating a Cluster with Docker Swarm\\n\\n```bash\\n# Initialize a Swarm cluster\\ndocker swarm init\\n\\n# Add a worker node to the cluster\\ndocker swarm join --token <TOKEN> <MANAGER-IP>:<PORT>\\n\\n# Deploy a service across the cluster\\ndocker service create --name my-service --replicas 3 nginx:latest\\n```\\n\\n### Example 3: Creating a Cluster with Apache Spark\\n\\n```python\\nfrom pyspark.sql import SparkSession\\n\\n# Create a Spark session (this will automatically create a cluster if running in cluster mode)\\nspark = SparkSession.builder \\\\\\n    .appName(\"MyClusterApp\") \\\\\\n    .config(\"spark.master\", \"spark://<MASTER-IP>:7077\") \\\\\\n    .getOrCreate()\\n\\n# Example: Load data and perform operations\\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\\ndf.show()\\n\\nspark.stop()\\n```\\n\\n### Example 4: Creating a Cluster with Hadoop\\n\\n```xml\\n<!-- core-site.xml -->\\n<configuration>\\n    <property>\\n        <name>fs.defaultFS</name>\\n        <value>hdfs://<NAMENODE-HOST>:9000</value>\\n    </property>\\n</configuration>\\n\\n<!-- hdfs-site.xml -->\\n<configuration>\\n    <property>\\n        <name>dfs.replication</name>\\n        <value>3</value>\\n    </property>\\n    <property>\\n        <name>dfs.namenode.name.dir</name>\\n        <value>file:/data/hadoop/namenode</value>\\n    </property>\\n    <property>\\n        <name>dfs.datanode.data.dir</name>\\n        <value>file:/data/hadoop/datanode</value>\\n    </property>\\n</configuration>\\n\\n<!-- Start the Hadoop cluster -->\\n$HADOOP_HOME/sbin/start-dfs.sh\\n$HADOOP_HOME/sbin/start-yarn.sh\\n```\\n\\nLet me know if you\\'d like examples for a specific platform or framework!', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 1841, 'completion_tokens': 550, 'total_tokens': 2391}, 'prompt_tokens': 1841, 'completion_tokens': 550, 'total_tokens': 2391, 'model': 'qwen3-next-instruct-091725', 'model_name': 'qwen3-next-instruct-091725', 'finish_reason': 'stop'}, id='run--94c8970b-7f08-4521-bf56-bc9d562d4084-0')]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-19814dcc98825d4fb7c4de8fcfa79b4b\", \"tr-75f32a4f7d898a51f1e1292b71c32ebc\", \"tr-a41d1aff30abaa9dae852660217d7a94\"]",
      "text/plain": [
       "[Trace(trace_id=tr-19814dcc98825d4fb7c4de8fcfa79b4b), Trace(trace_id=tr-75f32a4f7d898a51f1e1292b71c32ebc), Trace(trace_id=tr-a41d1aff30abaa9dae852660217d7a94)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def trace_agent_execution(query: str):\n",
    "    \"\"\"Execute agent and trace all tool calls.\"\"\"\n",
    "    \n",
    "    agent = create_react_agent(llm, [generic_retriever, api_retriever, tutorial_retriever, code_retriever])\n",
    "    \n",
    "    result = agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    # Extract tool call information\n",
    "    tool_calls = []\n",
    "    for msg in result[\"messages\"]:\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                tool_calls.append({\n",
    "                    'tool': tc['name'],\n",
    "                    'query': tc['args'].get('query', str(tc['args']))\n",
    "                })\n",
    "    \n",
    "    # Print trace\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"\\nTools Called: {len(tool_calls)}\")\n",
    "    for i, tc in enumerate(tool_calls, 1):\n",
    "        print(f\"  {i}. {tc['tool']}\")\n",
    "        print(f\"     Query: {tc['query']}\")\n",
    "    \n",
    "    print(f\"\\nFinal Answer:\")\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Use it\n",
    "trace_agent_execution(\"how to use mlflow with databricks jobs\")\n",
    "trace_agent_execution(\"show me API docs for authentication\")\n",
    "trace_agent_execution(\"give me code examples for creating clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff90b73f-d115-494e-9086-f355f6840328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Response with Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65acebc0-7c8e-4f89-8f20-b9f53d5d4a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\nTo use MLflow with Databricks Jobs, you can integrate MLflow experiment tracking and model management into your scheduled or automated workflows. Below is a step-by-step guide based on official Databricks documentation:\n\n### 1. **Use MLflow with Job Tasks**\nDatabricks Jobs allow you to run notebooks, JARs, or Python scripts as tasks. When you run MLflow-enabled code inside these tasks, MLflow automatically logs metrics, parameters, and artifacts to the Databricks MLflow Tracking server.\n\n- Define your MLflow experiment in your notebook or script before training a model:\n  ```python\n  import mlflow\n\n  mlflow.set_experiment(\"/Users/your-email@databricks.com/your-experiment-name\")\n  with mlflow.start_run():\n      # Your model training code here\n      mlflow.log_param(\"learning_rate\", 0.01)\n      mlflow.log_metric(\"accuracy\", 0.95)\n      mlflow.sklearn.log_model(model, \"model\")\n  ```\n  \n- Place this code in a notebook and assign it as a task in your Databricks Job [2].\n\n### 2. **Enable Databricks Autologging (Recommended)**\nDatabricks Autologging automatically captures parameters, metrics, and models from popular ML libraries (e.g., scikit-learn, XGBoost, TensorFlow) without requiring manual `mlflow.log_*` calls.\n\nSimply enable it at the start of your notebook:\n```python\nimport mlflow\nmlflow.autolog()\n```\nThis will automatically sync training runs with MLflow, even when the notebook is executed as part of a Databricks Job [5].\n\n### 3. **Schedule the Job**\nYou can schedule your job to run periodically using the Databricks Jobs UI or API:\n\n- In the UI: Go to **Jobs > Create Job > Notebook Task**, select your MLflow-enabled notebook, and define a schedule (e.g., hourly, daily).\n- Using the API: Submit a job with `notebook_task` specifying the notebook path [2].\n\nExample API payload for a job with two MLflow-enabled tasks:\n```json\n{\n  \"name\": \"MLflow-Training-Job\",\n  \"tasks\": [\n    {\n      \"task_key\": \"train_model\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/Users/your-email@databricks.com/train-model\"\n      },\n      \"existing_cluster_id\": \"your-cluster-id\"\n    }\n  ]\n}\n```\n[2]\n\n### 4. **View Results in MLflow UI**\nAfter the job runs, navigate to the MLflow UI in Databricks:\n- Click **Experiment** in the left sidebar.\n- Select your experiment to view logged runs, metrics, and models from your job executions.\n\n### 5. **Register Models to MLflow Model Registry**\nYou can also register trained models from your job to the MLflow Model Registry for production deployment:\n```python\nmlflow.register_model(\"runs:/<run_id>/model\", \"MyRegisteredModel\")\n```\n\nThis allows you to manage model versions, transitions (e.g., Staging → Production), and deploy via MLflow Model Serving [5].\n\n### Best Practices\n- Use shared or job-specific clusters (avoid all-purpose clusters for production jobs).\n- Set a proper experiment name path (e.g., `/Users/user@domain.com/experiments`) to group jobs logically.\n- Enable MLflow artifact logging to DBFS or cloud storage for persistence.\n\n### References\n- [Databricks Jobs API for creating tasks](https://docs.databricks.com/en/workflows/jobs/jobs-api-updates.html) [2]\n- [MLflow Autologging on Databricks](https://docs.databricks.com/en/archive/runtime-release-notes/10.2ml.html) [5]\n- [MLflow Experiment Tracking](https://docs.databricks.com/en/getting-started/concepts.html) [1]\n\nBy integrating MLflow into Databricks Jobs, you achieve end-to-end automation of model training, tracking, and deployment in a scalable, reproducible manner.\n**********************************************************************\nDatabricks supports several authentication methods to secure access to its platform and APIs. The primary methods include:\n\n1. **Databricks OAuth for Service Principals**: This method is recommended for automated tools, jobs, and applications. A service principal is an identity created in Databricks that can be assigned permissions without relying on individual user credentials [1].\n\n2. **Databricks OAuth for Users**: This method allows individual users to authenticate using OAuth, which is more secure than using usernames and passwords. It enables seamless single sign-on (SSO) integration with identity providers [1].\n\n3. **Username and Password (Admin Only)**: Databricks account administrators can authenticate using their username and password, but this method is discouraged in favor of OAuth due to security best practices [1].\n\n4. **Federation with SAML and IAM**: Databricks supports federated authentication via SAML for identity providers such as Azure AD, Okta, and others. Additionally, for cloud environments like AWS, credential passthrough allows Databricks to assume AWS IAM roles for users via temporary security tokens, enabling secure access to cloud storage like S3 [2].\n\nDatabricks strongly recommends using **OAuth for users or service principals** over direct username/password authentication for enhanced security and scalability [1].\n\nSources:\n[1] https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html\n[2] https://docs.databricks.com/en/archive/credential-passthrough/iam-federation.html\n**********************************************************************\nTo create a cluster in Databricks, you can use either the UI or the REST API. Below are code examples and step-by-step instructions:\n\n### **Using the Databricks UI**\n1. Click **Compute** in the sidebar.\n2. Click **Create Cluster** (or **New Cluster** in some interfaces).\n3. Provide a unique cluster name.\n4. Configure cluster settings (e.g., Spark version, node types, auto-termination).\n5. Click **Create Cluster**.  \n   [1]\n\n### **Using the Databricks REST API (Python/HTTP Example)**\nYou can create a cluster programmatically using the Clusters API 2.0 with a POST request to `/api/2.0/clusters/create`.\n\n#### Example: Create a cluster using `curl`\n```bash\ncurl -X POST -H \"Authorization: Bearer <your-access-token>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cluster_name\": \"my-python-cluster\",\n    \"spark_version\": \"13.3.x-scala2.12\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"num_workers\": 2,\n    \"autotermination_minutes\": 120\n  }' \\\n  https://<your-workspace-domain>/api/2.0/clusters/create\n```\n\n#### Example: Create a cluster using Python (`requests`)\n```python\nimport requests\n\nurl = \"https://<your-workspace-domain>/api/2.0/clusters/create\"\nheaders = {\n    \"Authorization\": \"Bearer <your-access-token>\",\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"cluster_name\": \"my-python-cluster\",\n    \"spark_version\": \"13.3.x-scala2.12\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"num_workers\": 2,\n    \"autotermination_minutes\": 120\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.json())\n```\n\n> ✅ Replace `<your-workspace-domain>` with your Databricks workspace URL (e.g., `adb-1234567890123456.7.azuredatabricks.net`) and `<your-access-token>` with a valid personal access token.\n\nFor more details on cluster configuration options, see the [Databricks Clusters API documentation](https://docs.databricks.com/api/workspace/clusters/create) [2].\n\n### **Additional Notes**\n- If you're using **PrivateLink**, you must wait ~20 minutes after workspace setup before creating clusters [3].\n- For advanced use cases like mounting Amazon EFS, use `cluster_mount_infos` in the API request [4].\n\nSources:\n[1] https://docs.databricks.com/en/getting-started/etl-quick-start.html\n[2] https://docs.databricks.com/api/workspace/clusters/create\n[3] https://docs.databricks.com/en/security/network/classic/privatelink.html\n[4] https://docs.databricks.com/en/sparkr/rstudio.html\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-a4f5a58f3adcb6d003765f472a0d91c6\", \"tr-ac058d90647dcc2b74f1c8775af60e98\", \"tr-e4fa974c601cea428625d0137ba29567\", \"tr-e842f0eb63a5979962fb580b37cc009b\", \"tr-7e33dea57ec6ce177d569dd27d3d15dc\", \"tr-34b026f77c07d48c2a4b895337ff55b4\", \"tr-51e977ca1b9f22eea678900014b40515\", \"tr-feecd23753d11727db67681a702db470\", \"tr-7c6197949ac362687c65220afed1c460\", \"tr-d546ff3c0ebff95bdb38b4528b5f29eb\"]",
      "text/plain": [
       "[Trace(trace_id=tr-a4f5a58f3adcb6d003765f472a0d91c6), Trace(trace_id=tr-ac058d90647dcc2b74f1c8775af60e98), Trace(trace_id=tr-e4fa974c601cea428625d0137ba29567), Trace(trace_id=tr-e842f0eb63a5979962fb580b37cc009b), Trace(trace_id=tr-7e33dea57ec6ce177d569dd27d3d15dc), Trace(trace_id=tr-34b026f77c07d48c2a4b895337ff55b4), Trace(trace_id=tr-51e977ca1b9f22eea678900014b40515), Trace(trace_id=tr-feecd23753d11727db67681a702db470), Trace(trace_id=tr-7c6197949ac362687c65220afed1c460), Trace(trace_id=tr-d546ff3c0ebff95bdb38b4528b5f29eb)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# System prompt with citation instructions\n",
    "system_message = SystemMessage(content=\"\"\"You are a Databricks documentation expert assistant.\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Use the retrieval tools to find accurate information\n",
    "2. Answer questions clearly and concisely\n",
    "3. ALWAYS include citations with URLs for every piece of information you provide\n",
    "4. Format citations as numbered references [1], [2], etc.\n",
    "5. List all source URLs at the end of your response\n",
    "Response format:\n",
    "<Your detailed answer with inline citations [1], [2]>\n",
    "Sources:\n",
    "[1] <URL from first source>\n",
    "[2] <URL from second source>.\"\"\")\n",
    "\n",
    "# Create agent\n",
    "llm = ChatDatabricks(endpoint=\"databricks-qwen3-next-80b-a3b-instruct\")\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [generic_retriever, api_retriever, tutorial_retriever, code_retriever]\n",
    ")\n",
    "\n",
    "# Function to query with system prompt\n",
    "def query_agent(user_query: str):\n",
    "    \"\"\"Query the agent with a user question.\"\"\"\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [\n",
    "            system_message,  # System prompt (stays the same)\n",
    "            {\"role\": \"user\", \"content\": user_query}  # User query (dynamic)\n",
    "        ]\n",
    "    })\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "# Use it with different queries\n",
    "answer1 = query_agent(\"how to use mlflow with databricks jobs\")\n",
    "print(\"*\"*70)\n",
    "print(answer1)\n",
    "\n",
    "answer2 = query_agent(\"explain authentication methods\")\n",
    "print(\"*\"*70)\n",
    "print(answer2)\n",
    "\n",
    "answer3 = query_agent(\"show me code examples for creating clusters\")\n",
    "print(\"*\"*70)\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c4ae6e-1f2e-4728-be52-82515d489b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test with tracing to see if URLs are being retrieved\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"how to use mlflow with databricks jobs\"}]\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RETRIEVED SOURCES:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check what URLs were retrieved\n",
    "for msg in result[\"messages\"]:\n",
    "    if hasattr(msg, 'name') and 'retriever' in str(msg.name):\n",
    "        print(f\"\\nTool: {msg.name}\")\n",
    "        # Print first 200 chars of response to see if URLs are there\n",
    "        print(str(msg.content)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a72479-147f-4538-998f-f775e8d0f081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "328a76b4-602b-47dc-a372-b84e894525c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Log MLflow experiments and analyse tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c5ef93-2b03-406a-98b8-ba9dbfde9ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/03 18:54:56 INFO mlflow.tracking.fluent: Experiment with name '/Users/shrashti.90@gmail.com/rag-agent-testing' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1 tool calls\nQuery 2: 1 tool calls\nQuery 3: 1 tool calls\nQuery 4: 1 tool calls\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-14d29a84cba7c284edf3e0348ca9b538\", \"tr-d9e94880734dd4a8829ccd3d6e0b8f7b\", \"tr-67553bc800b2664b32d5970c7a14b53d\", \"tr-52f4dd276247d4e746b176444df8c572\"]",
      "text/plain": [
       "[Trace(trace_id=tr-14d29a84cba7c284edf3e0348ca9b538), Trace(trace_id=tr-d9e94880734dd4a8829ccd3d6e0b8f7b), Trace(trace_id=tr-67553bc800b2664b32d5970c7a14b53d), Trace(trace_id=tr-52f4dd276247d4e746b176444df8c572)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/Users/shrashti.90@gmail.com/rag-agent-testing\")\n",
    "\n",
    "test_queries = [\n",
    "    \"how to use mlflow with databricks jobs\",\n",
    "    \"show me API docs for authentication\",\n",
    "    \"give me code examples for creating clusters\",\n",
    "    \"How to use Databricks platform for building end-to-end agentic application\"\n",
    "]\n",
    "\n",
    "agent = create_react_agent(llm, [generic_retriever, api_retriever, tutorial_retriever, code_retriever])\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    with mlflow.start_run(run_name=f\"query_{i}\"):\n",
    "        mlflow.log_param(\"query\", query)\n",
    "        \n",
    "        result = agent.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        # Count tool calls\n",
    "        tool_calls = []\n",
    "        for msg in result[\"messages\"]:\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                tool_calls.extend([tc['name'] for tc in msg.tool_calls])\n",
    "        \n",
    "        mlflow.log_metric(\"num_tool_calls\", len(tool_calls))\n",
    "        mlflow.log_param(\"tools_used\", \", \".join(set(tool_calls)))\n",
    "        mlflow.log_text(result[\"messages\"][-1].content, \"answer.txt\")\n",
    "        \n",
    "        print(f\"Query {i}: {len(tool_calls)} tool calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb8def1-8e1c-426f-a62a-c0d430403c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean up Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7918dc24-7d7d-4c7f-bbab-0f3a2904b7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing indexes:\n  - workspace.default.db_docs_index\n  - workspace.default.databricks_index\n"
     ]
    }
   ],
   "source": [
    "# List all indexes on an endpoint\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"databricks_doc_index\"\n",
    "\n",
    "indexes = vsc.list_indexes(VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "\n",
    "print(\"Existing indexes:\")\n",
    "for index in indexes.get('vector_indexes', []):\n",
    "    print(f\"  - {index['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904c4b54-9b67-47a6-aa94-70eb18e634df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deleted index: workspace.default.databricks_docs_processed_index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Delete a specific index\n",
    "index_name = \"workspace.default.databricks_docs_processed_index\"\n",
    "\n",
    "try:\n",
    "    vsc.delete_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    print(f\"✓ Deleted index: {index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393f2507-0988-422f-b946-e59464f087cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Knowledge-Base",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}