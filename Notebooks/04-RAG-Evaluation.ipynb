{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG System Evaluation with RAGAS\n",
                "\n",
                "This notebook provides comprehensive evaluation of the Databricks documentation RAG system using RAGAS metrics.\n",
                "\n",
                "## Evaluation Components:\n",
                "1. Ground truth data preparation\n",
                "2. RAG system response generation\n",
                "3. RAGAS metrics evaluation\n",
                "4. Tool calling evaluation\n",
                "5. Results analysis and visualization\n",
                "\n",
                "## RAGAS Metrics:\n",
                "- **Faithfulness**: Response grounded in retrieved context\n",
                "- **Answer Relevancy**: Answer relevance to question\n",
                "- **Context Precision**: Relevant contexts ranked higher\n",
                "- **Context Recall**: All relevant contexts retrieved\n",
                "- **Answer Correctness**: Match with reference answer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 1: Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install RAGAS if not already installed\n",
                "%pip install ragas pandas matplotlib seaborn --quiet\n",
                "\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import mlflow\n",
                "\n",
                "# Import custom modules\n",
                "from ground_truth_generator import GroundTruthGenerator\n",
                "from ragas_evaluator import RAGASEvaluator\n",
                "import sys
sys.path.insert(0, "./helpers")
from conversational_agent import create_databricks_agent\n",
                "\n",
                "print(\"‚úÖ All imports successful\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "CONFIG = {\n",
                "    # Data paths\n",
                "    \"docs_csv_path\": \"ground-truth data/databricks_docs.csv\",\n",
                "    \"output_dir\": \"ground-truth data\",\n",
                "    \n",
                "    # LLM configuration\n",
                "    \"llm_endpoint\": \"databricks-qwen3-next-80b-a3b-instruct\",\n",
                "    \n",
                "    # Vector search configuration (UPDATE THESE)\n",
                "    \"vector_search_endpoint\": \"your_vector_search_endpoint\",\n",
                "    \"vector_search_index\": \"your_vector_search_index\",\n",
                "    \n",
                "    # Evaluation parameters\n",
                "    \"num_samples\": 50,\n",
                "    \"questions_per_doc\": 1,\n",
                "    \"sampling_strategy\": \"diverse\",\n",
                "    \n",
                "    # MLflow\n",
                "    \"log_to_mlflow\": True,\n",
                "    \"experiment_name\": \"/Users/your_username/rag_evaluation\"\n",
                "}\n",
                "\n",
                "# Set MLflow experiment\n",
                "if CONFIG[\"log_to_mlflow\"]:\n",
                "    mlflow.set_experiment(CONFIG[\"experiment_name\"])\n",
                "\n",
                "print(\"üìã Configuration:\")\n",
                "for key, value in CONFIG.items():\n",
                "    print(f\"   {key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 2: Ground Truth Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize ground truth generator\n",
                "generator = GroundTruthGenerator(\n",
                "    csv_path=CONFIG[\"docs_csv_path\"],\n",
                "    llm_endpoint=CONFIG[\"llm_endpoint\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate evaluation dataset\n",
                "print(f\"ü§ñ Generating {CONFIG['num_samples']} evaluation samples...\")\n",
                "print(f\"   This will take several minutes...\\n\")\n",
                "\n",
                "evaluation_samples = generator.generate_dataset(\n",
                "    num_samples=CONFIG[\"num_samples\"],\n",
                "    questions_per_doc=CONFIG[\"questions_per_doc\"],\n",
                "    sampling_strategy=CONFIG[\"sampling_strategy\"]\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Generated {len(evaluation_samples)} evaluation samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export ground truth data\n",
                "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "\n",
                "json_path = f\"{CONFIG['output_dir']}/evaluation_dataset_{timestamp}.json\"\n",
                "csv_path = f\"{CONFIG['output_dir']}/evaluation_dataset_{timestamp}.csv\"\n",
                "ragas_path = f\"{CONFIG['output_dir']}/ragas_dataset_{timestamp}.json\"\n",
                "\n",
                "generator.export_to_json(evaluation_samples, json_path)\n",
                "generator.export_to_csv(evaluation_samples, csv_path)\n",
                "generator.export_ragas_format(evaluation_samples, ragas_path)\n",
                "\n",
                "print(f\"\\nüíæ Exported datasets:\")\n",
                "print(f\"   JSON: {json_path}\")\n",
                "print(f\"   CSV: {csv_path}\")\n",
                "print(f\"   RAGAS: {ragas_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the generated dataset\n",
                "df = pd.DataFrame([vars(s) for s in evaluation_samples])\n",
                "\n",
                "print(\"üìä Dataset Overview:\")\n",
                "print(f\"   Total samples: {len(df)}\")\n",
                "print(f\"\\n   Question Types:\")\n",
                "print(df['question_type'].value_counts())\n",
                "print(f\"\\n   Difficulty Levels:\")\n",
                "print(df['difficulty'].value_counts())\n",
                "\n",
                "# Display sample questions\n",
                "print(f\"\\nüìù Sample Questions:\")\n",
                "for i, row in df.head(3).iterrows():\n",
                "    print(f\"\\n{i+1}. {row['user_input']}\")\n",
                "    print(f\"   Type: {row['question_type']}, Difficulty: {row['difficulty']}\")\n",
                "    print(f\"   Reference: {row['reference'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 3: RAG System Response Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the RAG agent\n",
                "print(\"ü§ñ Initializing RAG agent...\")\n",
                "\n",
                "agent = create_databricks_agent(\n",
                "    vector_search_endpoint=CONFIG[\"vector_search_endpoint\"],\n",
                "    vector_search_index=CONFIG[\"vector_search_index\"],\n",
                "    llm_endpoint=CONFIG[\"llm_endpoint\"],\n",
                "    guardrail_strictness=\"moderate\",\n",
                "    enable_input_guardrail=True,\n",
                "    enable_output_guardrail=True,\n",
                "    log_to_mlflow=False  # We'll log evaluation results separately\n",
                ")\n",
                "\n",
                "print(\"‚úÖ RAG agent initialized\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate responses for all evaluation samples\n",
                "print(f\"üîç Generating RAG responses for {len(evaluation_samples)} questions...\")\n",
                "print(f\"   This will take several minutes...\\n\")\n",
                "\n",
                "rag_results = []\n",
                "\n",
                "for idx, sample in enumerate(evaluation_samples):\n",
                "    print(f\"\\nProcessing {idx+1}/{len(evaluation_samples)}: {sample.user_input[:60]}...\")\n",
                "    \n",
                "    try:\n",
                "        # Query the agent\n",
                "        result = agent.query(\n",
                "            user_query=sample.user_input,\n",
                "            conversation_id=f\"eval_{idx}\",\n",
                "            include_history=False\n",
                "        )\n",
                "        \n",
                "        # Store result with metadata\n",
                "        rag_results.append({\n",
                "            \"user_input\": sample.user_input,\n",
                "            \"response\": result[\"response\"],\n",
                "            \"reference\": sample.reference,\n",
                "            \"retrieved_contexts\": sample.retrieved_contexts,  # Ground truth contexts\n",
                "            \"tool_calls\": result[\"metadata\"][\"tool_calls\"],\n",
                "            \"tools_used\": result[\"metadata\"][\"tools_used\"],\n",
                "            \"latency_ms\": result[\"metadata\"][\"latency_ms\"],\n",
                "            \"was_rejected\": result[\"was_rejected\"],\n",
                "            \"question_type\": sample.question_type,\n",
                "            \"difficulty\": sample.difficulty,\n",
                "            \"doc_url\": sample.doc_url\n",
                "        })\n",
                "        \n",
                "        print(f\"   ‚úì Response generated ({result['metadata']['tool_calls']} tool calls)\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Error: {str(e)}\")\n",
                "        # Add placeholder for failed queries\n",
                "        rag_results.append({\n",
                "            \"user_input\": sample.user_input,\n",
                "            \"response\": \"ERROR: Failed to generate response\",\n",
                "            \"reference\": sample.reference,\n",
                "            \"retrieved_contexts\": sample.retrieved_contexts,\n",
                "            \"tool_calls\": 0,\n",
                "            \"tools_used\": [],\n",
                "            \"latency_ms\": 0,\n",
                "            \"was_rejected\": False,\n",
                "            \"question_type\": sample.question_type,\n",
                "            \"difficulty\": sample.difficulty,\n",
                "            \"doc_url\": sample.doc_url\n",
                "        })\n",
                "\n",
                "print(f\"\\n‚úÖ Generated {len(rag_results)} RAG responses\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save RAG results\n",
                "rag_results_path = f\"{CONFIG['output_dir']}/rag_results_{timestamp}.json\"\n",
                "with open(rag_results_path, 'w') as f:\n",
                "    json.dump(rag_results, f, indent=2)\n",
                "\n",
                "print(f\"üíæ Saved RAG results to: {rag_results_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 4: RAGAS Metrics Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize RAGAS evaluator\n",
                "evaluator = RAGASEvaluator(\n",
                "    llm_endpoint=CONFIG[\"llm_endpoint\"],\n",
                "    log_to_mlflow=CONFIG[\"log_to_mlflow\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run RAGAS evaluation\n",
                "print(\"üîç Running RAGAS evaluation...\")\n",
                "print(\"   This will take several minutes...\\n\")\n",
                "\n",
                "results = evaluator.evaluate(\n",
                "    evaluation_data=rag_results,\n",
                "    run_name=f\"ragas_eval_{timestamp}\"\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ RAGAS evaluation complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Bar chart of metric scores\n",
                "metrics = ['Faithfulness', 'Answer\\nRelevancy', 'Context\\nPrecision', 'Context\\nRecall', 'Answer\\nCorrectness']\n",
                "scores = [\n",
                "    results.faithfulness_score,\n",
                "    results.answer_relevancy_score,\n",
                "    results.context_precision_score,\n",
                "    results.context_recall_score,\n",
                "    results.answer_correctness_score\n",
                "]\n",
                "\n",
                "axes[0].bar(metrics, scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
                "axes[0].set_ylabel('Score')\n",
                "axes[0].set_title('RAGAS Metric Scores')\n",
                "axes[0].set_ylim([0, 1])\n",
                "axes[0].axhline(y=results.average_score, color='r', linestyle='--', label=f'Average: {results.average_score:.3f}')\n",
                "axes[0].legend()\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Distribution of scores\n",
                "detailed_df = results.detailed_results\n",
                "metric_cols = [col for col in detailed_df.columns if col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'answer_correctness']]\n",
                "detailed_df[metric_cols].boxplot(ax=axes[1])\n",
                "axes[1].set_ylabel('Score')\n",
                "axes[1].set_title('Score Distributions')\n",
                "axes[1].set_ylim([0, 1])\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CONFIG['output_dir']}/ragas_scores_{timestamp}.png\", dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüìä Visualization saved to: {CONFIG['output_dir']}/ragas_scores_{timestamp}.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 5: Tool Calling Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze tool calling patterns\n",
                "rag_df = pd.DataFrame(rag_results)\n",
                "\n",
                "print(\"üîß Tool Calling Analysis:\")\n",
                "print(f\"\\n   Total queries: {len(rag_df)}\")\n",
                "print(f\"   Queries with tool calls: {(rag_df['tool_calls'] > 0).sum()}\")\n",
                "print(f\"   Queries without tool calls: {(rag_df['tool_calls'] == 0).sum()}\")\n",
                "print(f\"   Average tool calls per query: {rag_df['tool_calls'].mean():.2f}\")\n",
                "\n",
                "# Tool usage by question type\n",
                "print(f\"\\n   Tool calls by question type:\")\n",
                "tool_by_type = rag_df.groupby('question_type')['tool_calls'].agg(['mean', 'count'])\n",
                "print(tool_by_type)\n",
                "\n",
                "# Most used tools\n",
                "all_tools = []\n",
                "for tools in rag_df['tools_used']:\n",
                "    all_tools.extend(tools)\n",
                "\n",
                "if all_tools:\n",
                "    tool_counts = pd.Series(all_tools).value_counts()\n",
                "    print(f\"\\n   Most used tools:\")\n",
                "    print(tool_counts)\n",
                "else:\n",
                "    print(f\"\\n   ‚ö†Ô∏è  No tools were called!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize tool usage\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Tool calls distribution\n",
                "axes[0].hist(rag_df['tool_calls'], bins=range(0, rag_df['tool_calls'].max() + 2), edgecolor='black')\n",
                "axes[0].set_xlabel('Number of Tool Calls')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('Distribution of Tool Calls per Query')\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Tool calls by question type\n",
                "tool_by_type['mean'].plot(kind='bar', ax=axes[1], color='steelblue')\n",
                "axes[1].set_xlabel('Question Type')\n",
                "axes[1].set_ylabel('Average Tool Calls')\n",
                "axes[1].set_title('Average Tool Calls by Question Type')\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CONFIG['output_dir']}/tool_usage_{timestamp}.png\", dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüìä Tool usage visualization saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 6: Results Analysis and Reporting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate by question type\n",
                "print(\"üìä Evaluating by question type...\\n\")\n",
                "\n",
                "results_by_type = evaluator.evaluate_by_category(\n",
                "    evaluation_data=rag_results,\n",
                "    category_key='question_type'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = []\n",
                "for q_type, type_results in results_by_type.items():\n",
                "    comparison_data.append({\n",
                "        'Question Type': q_type,\n",
                "        'Samples': type_results.num_samples,\n",
                "        'Faithfulness': f\"{type_results.faithfulness_score:.3f}\",\n",
                "        'Answer Relevancy': f\"{type_results.answer_relevancy_score:.3f}\",\n",
                "        'Context Precision': f\"{type_results.context_precision_score:.3f}\",\n",
                "        'Context Recall': f\"{type_results.context_recall_score:.3f}\",\n",
                "        'Answer Correctness': f\"{type_results.answer_correctness_score:.3f}\",\n",
                "        'Average': f\"{type_results.average_score:.3f}\"\n",
                "    })\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "print(\"\\nüìä Results by Question Type:\")\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate final evaluation report\n",
                "report = f\"\"\"# RAG System Evaluation Report\n",
                "\n",
                "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
                "\n",
                "## Overall Performance\n",
                "\n",
                "- **Total Samples Evaluated:** {results.num_samples}\n",
                "- **Average Score:** {results.average_score:.3f}\n",
                "\n",
                "### RAGAS Metrics\n",
                "\n",
                "| Metric | Score |\n",
                "|--------|-------|\n",
                "| Faithfulness | {results.faithfulness_score:.3f} |\n",
                "| Answer Relevancy | {results.answer_relevancy_score:.3f} |\n",
                "| Context Precision | {results.context_precision_score:.3f} |\n",
                "| Context Recall | {results.context_recall_score:.3f} |\n",
                "| Answer Correctness | {results.answer_correctness_score:.3f} |\n",
                "\n",
                "### Tool Usage\n",
                "\n",
                "- **Queries with Tool Calls:** {(rag_df['tool_calls'] > 0).sum()} / {len(rag_df)}\n",
                "- **Average Tool Calls:** {rag_df['tool_calls'].mean():.2f}\n",
                "- **Average Latency:** {rag_df['latency_ms'].mean():.0f}ms\n",
                "\n",
                "### Performance by Question Type\n",
                "\n",
                "{comparison_df.to_markdown(index=False)}\n",
                "\n",
                "## Recommendations\n",
                "\n",
                "1. **Tool Calling:** {'‚úÖ Good tool usage' if rag_df['tool_calls'].mean() > 0.8 else '‚ö†Ô∏è Low tool usage - review agent prompts'}\n",
                "2. **Faithfulness:** {'‚úÖ High faithfulness' if results.faithfulness_score > 0.8 else '‚ö†Ô∏è Improve context grounding'}\n",
                "3. **Context Recall:** {'‚úÖ Good retrieval coverage' if results.context_recall_score > 0.7 else '‚ö†Ô∏è Improve retrieval coverage'}\n",
                "\"\"\"\n",
                "\n",
                "# Save report\n",
                "report_path = f\"{CONFIG['output_dir']}/evaluation_report_{timestamp}.md\"\n",
                "with open(report_path, 'w') as f:\n",
                "    f.write(report)\n",
                "\n",
                "print(f\"\\nüìÑ Evaluation report saved to: {report_path}\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(report)\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"\\n‚úÖ EVALUATION COMPLETE!\")\n",
                "print(f\"\\nüìÅ Generated Files:\")\n",
                "print(f\"   - Ground truth dataset: {json_path}\")\n",
                "print(f\"   - RAG results: {rag_results_path}\")\n",
                "print(f\"   - Evaluation report: {report_path}\")\n",
                "print(f\"   - Visualizations: {CONFIG['output_dir']}/ragas_scores_{timestamp}.png\")\n",
                "print(f\"   - Visualizations: {CONFIG['output_dir']}/tool_usage_{timestamp}.png\")\n",
                "\n",
                "if CONFIG[\"log_to_mlflow\"]:\n",
                "    print(f\"\\nüìä Results logged to MLflow experiment: {CONFIG['experiment_name']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}