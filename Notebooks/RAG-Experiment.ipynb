{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e61f6e7-41f4-4dec-8773-ad309c31ba2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imports and testing code for index access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e064a117-5739-45ea-82fc-a4505f8c3452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (0.12.1)\nRequirement already satisfied: langchain-community in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (0.4.1)\nRequirement already satisfied: langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (1.2.0)\nRequirement already satisfied: databricks-sql-connector in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (4.2.3)\nRequirement already satisfied: databricks-ai-bridge>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (0.11.0)\nRequirement already satisfied: databricks-mcp>=0.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (0.5.1)\nRequirement already satisfied: databricks-sdk>=0.65.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (0.76.0)\nRequirement already satisfied: databricks-vectorsearch>=0.50 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (0.63)\nRequirement already satisfied: langchain-mcp-adapters>=0.1.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (0.2.1)\nRequirement already satisfied: mlflow>=2.20.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (3.8.1)\nRequirement already satisfied: openai>=1.99.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (2.14.0)\nRequirement already satisfied: pydantic>2.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-langchain) (2.12.5)\nRequirement already satisfied: unitycatalog-langchain>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.3.0)\nRequirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (1.2.5)\nRequirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (1.0.1)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (2.0.45)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (2.12.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (0.5.2)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-community) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\nRequirement already satisfied: langgraph<1.1.0,>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain) (1.0.5)\nRequirement already satisfied: lz4<5.0.0,>=4.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-sql-connector) (4.4.5)\nRequirement already satisfied: oauthlib<4.0.0,>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-sql-connector) (3.2.2)\nRequirement already satisfied: openpyxl<4.0.0,>=3.0.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-sql-connector) (3.1.5)\nRequirement already satisfied: pandas<2.4.0,>=1.2.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.2.3)\nRequirement already satisfied: pybreaker<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-sql-connector) (1.4.1)\nRequirement already satisfied: pyjwt<3.0.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.10.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.9.0.post0)\nRequirement already satisfied: thrift<0.21.0,>=0.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-sql-connector) (0.20.0)\nRequirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\nRequirement already satisfied: mlflow-skinny>=2.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (3.8.1)\nRequirement already satisfied: tabulate>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (0.9.0)\nRequirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (0.12.0)\nRequirement already satisfied: typing-extensions in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (4.15.0)\nRequirement already satisfied: mcp>=1.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-mcp>=0.5.1->databricks-langchain) (1.25.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.65.0->databricks-langchain) (2.40.0)\nRequirement already satisfied: protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-sdk>=0.65.0->databricks-langchain) (5.29.5)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from databricks-vectorsearch>=0.50->databricks-langchain) (2.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (24.1)\nRequirement already satisfied: uuid-utils<1.0,>=0.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.12.0)\nRequirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\nRequirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\nRequirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\nRequirement already satisfied: xxhash>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mlflow-tracing==3.8.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (3.8.1)\nRequirement already satisfied: Flask-CORS<7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (6.0.2)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (3.1.2)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (1.17.2)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (43.0.3)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (23.0.0)\nRequirement already satisfied: huey<3,>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (2.5.5)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (3.10.0)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (1.15.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.32.1)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.39.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.32.1)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.2.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.34.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (0.12.0)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (1.3.0)\nRequirement already satisfied: tqdm>4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (4.67.1)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from openpyxl<4.0.0,>=3.0.10->databricks-sql-connector) (2.0.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<2.4.0,>=1.2.5->databricks-sql-connector) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<2.4.0,>=1.2.5->databricks-sql-connector) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>2.10.0->databricks-langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from pydantic>2.10.0->databricks-langchain) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from pydantic>2.10.0->databricks-langchain) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.8.0->databricks-sql-connector) (1.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\nRequirement already satisfied: unitycatalog-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.3.2)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow>=2.20.1->databricks-langchain) (1.3.10)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow>=2.20.1->databricks-langchain) (1.17.1)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain) (3.1.5)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain) (3.1.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (4.9.1)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain) (3.2.7)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain) (3.2.0)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\nRequirement already satisfied: ormsgpack>=1.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (3.2.0)\nRequirement already satisfied: jsonschema>=4.20.0 in /databricks/python3/lib/python3.12/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (4.23.0)\nRequirement already satisfied: python-multipart>=0.0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.0.21)\nRequirement already satisfied: sse-starlette>=1.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (3.0.3)\nRequirement already satisfied: starlette>=0.27 in /databricks/python3/lib/python3.12/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.46.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.20.1->databricks-langchain) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.20.1->databricks-langchain) (3.5.0)\nRequirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from tiktoken>=0.8.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (2025.11.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.6.0)\nRequirement already satisfied: unitycatalog-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.3.1)\nRequirement already satisfied: databricks-connect<17.1,>=15.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (17.0.10)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow>=2.20.1->databricks-langchain) (2.21)\nRequirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: grpcio>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.10.9.9)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (74.0.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.21.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.22.3)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.53b1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.4.8)\nRequirement already satisfied: aiohttp-retry>=2.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2b0b0dc9-d91b-4ef1-974e-ae28640cdb69/lib/python3.12/site-packages (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (2.9.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (5.0.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade databricks-langchain langchain-community langchain databricks-sql-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "706ebabf-cd49-4c71-b971-38b8ed81021f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing-extensions>=4.7\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nInstalling collected packages: typing-extensions\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.15.0\n    Uninstalling typing_extensions-4.15.0:\n      Successfully uninstalled typing_extensions-4.15.0\nSuccessfully installed typing-extensions-4.15.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --force-reinstall typing-extensions>=4.7\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a7c6e9e-f602-4512-8f44-478d91e50c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='Using **Databricks** involves several key steps, from setting up an account to running data engineering, data science, and machine learning workflows. Here’s a comprehensive, step-by-step guide to help you get started:\\n\\n---\\n\\n## ✅ **1. Understand What Databricks Is**\\nDatabricks is a cloud-based **unified analytics platform** built on Apache Spark. It enables:\\n- **Data Engineering** (ETL, data pipelines)\\n- **Data Science & Machine Learning** (notebooks, MLflow, model registry)\\n- **Business Intelligence** (dashboards via Databricks SQL)\\n- **Real-time analytics** (Delta Lake, Structured Streaming)\\n\\nIt runs on **AWS, Azure, or GCP**.\\n\\n---\\n\\n## ✅ **2. Sign Up for Databricks**\\n\\n### Option A: Free Trial (Recommended for beginners)\\n1. Go to: [https://databricks.com/try-databricks](https://databricks.com/try-databricks)\\n2. Fill in your details (name, email, company).\\n3. Choose a cloud provider (AWS, Azure, or GCP).\\n4. Databricks will provision a **free trial workspace** (usually 1', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 15, 'completion_tokens': 250, 'total_tokens': 265}, 'prompt_tokens': 15, 'completion_tokens': 250, 'total_tokens': 265, 'model': 'qwen3-next-instruct-091725', 'model_name': 'qwen3-next-instruct-091725', 'finish_reason': 'length'}, id='lc_run--019b7e23-7660-7ab0-8525-4b49b12bd376-0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    # endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    endpoint=\"databricks-qwen3-next-80b-a3b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=250,\n",
    ")\n",
    "chat_model.invoke(\"How to use Databricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5dae8325-986b-46bd-b49c-13af9eb9e817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.vector_search.client import VectorSearchClient\n",
    "# vsc = VectorSearchClient(disable_notice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e16fdcad-43e4-4310-bdea-407745713d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[22694.0,\n",
       "  'Monitor and manage personal access tokens  \\nTo authenticate to the Databricks REST API, a user can create a personal access token and use it in their REST API request. This article explains how workspace admins can manage personal access tokens in their workspace.  \\nTo create a personal access token, see Databricks personal access token authentication.  \\nTo create a personal access token on behalf of a service principal, see Manage tokens for a service principal.  \\nOverview of personal access token management\\nOverview of personal access token management\\nPersonal access tokens are enabled by default for all Databricks workspaces that were created in 2018 or later.  \\nWhen personal access tokens are enabled on a workspace, users with the CAN USE permission can generate personal access tokens to access Databricks REST APIs, and they can generate these tokens with any expiration date they like, including an indefinite lifetime. By default, no non-admin workspace users have the CAN USE permission, meaning that they cannot create or use personal access tokens.  \\nAs a Databricks workspace admin, you can disable personal access tokens for a workspace, monitor and revoke tokens, control which non-admin users can create tokens and use tokens, and set a maximum lifetime for new tokens.  \\nManaging personal access tokens in your workspace requires the Premium plan or above. To create a personal access token, see Databricks personal access token authentication.\\n\\nEnable or disable personal access token authentication for the workspace\\nEnable or disable personal access token authentication for the workspace\\nPersonal access token authentication is enabled by default for all Databricks workspaces that were created in 2018 or later. You can change this setting in the workspace settings page.  \\nWhen personal access tokens are disabled for a workspace, personal access tokens cannot be used to authenticate to Databricks and workspace users and service principals cannot create new tokens. No tokens are deleted when you disable personal access token authentication for a workspace. If tokens are re-enabled later, any non-expired tokens are available for use.  \\nIf you want to disable token access for a subset of users, you can keep personal access token authentication enabled for the workspace and set fine-grained permissions for users and groups. See Control who can create and use tokens.  \\nWarning  \\nPartner Connect, partner integrations, and service principals require personal access tokens to be enabled on a workspace.  \\nTo disable the ability to create and use personal access tokens for the workspace:  \\nGo to the settings page.  \\nClick the Advanced tab.  \\nClick the Personal Access Tokens toggle.  \\nClick Confirm.  \\nThis change may take a few seconds to take effect.  \\nYou can also use the Workspace configuration API to disable personal access tokens for the workspace.\\n\\nControl who can create and use tokens\\nControl who can create and use tokens\\nWorkspace admins can set permissions on personal access tokens to control which users, service principals, and groups can create and use tokens. For details on how to configure personal access token permissions, see Manage access to Databricks automation.\\n\\nSet maximum lifetime of new tokens\\nSet maximum lifetime of new tokens\\nYou can manage the maximum lifetime of new tokens in your workspace using the Databricks CLI. This limit applies only to new tokens.  \\nSet maxTokenLifetimeDays to the maximum token lifetime of new tokens in days, as an integer. If you set it to zero, new tokens are permitted to have no lifetime limit. For example:  \\ndatabricks workspace-conf set-status --json \\'{ \"maxTokenLifetimeDays\": \"90\" }\\'  \\nYou can also use the Workspace configuration API to manage the maximum lifetime for new tokens in a workspace.\\n\\nMonitor and revoke tokens\\nMonitor and revoke tokens\\nThis section describes how to use the Databricks CLI to manage existing tokens in the workspace. You can also use the Token Management API.  \\nGet tokens for the workspace  \\nTo get the workspace’s tokens:  \\ndatabricks token-management list  \\nYou can filter results by a user by using the flags created-by-id (to filter by the user ID) or created-by-username (to filter by the username).  \\nFor example:  \\ndatabricks token-management list --created-by-username user@company.com  \\nExample response:  \\nID Created By Comment token-id user@company.com dev  \\nDelete (revoke) a token  \\nTo delete a token, replace TOKEN_ID with the id of the token to delete:  \\ndatabricks token-management delete TOKEN_ID',\n",
       "  0.0031566645]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how to monitor and manage personal access token?\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"databricks_doc_index\" \n",
    "vs_index_fullname = \"workspace.default.db_docs_index\"\n",
    "\n",
    "# results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "#   query_text=question,\n",
    "#   columns=[\"id\", \"content\"],\n",
    "#   num_results=1)\n",
    "# docs = results.get('result', {}).get('data_array', [])\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ac6bc6fb-c7f2-46e7-963c-c852d0e83b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_88256515-c5ed-4304-8ee2-90513524015a', 'function': {'arguments': '{\"query\": \"Personal Access Token in Databricks\"}', 'name': 'databricks_docs_retriever'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 200, 'completion_tokens': 31, 'total_tokens': 231}, 'prompt_tokens': 200, 'completion_tokens': 31, 'total_tokens': 231, 'model': 'qwen3-next-instruct-091725', 'model_name': 'qwen3-next-instruct-091725', 'finish_reason': 'tool_calls'}, id='lc_run--019b7e30-0a13-7f31-a85f-bb24747f293f-0', tool_calls=[{'name': 'databricks_docs_retriever', 'args': {'query': 'Personal Access Token in Databricks'}, 'id': 'call_88256515-c5ed-4304-8ee2-90513524015a', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-22e6dc2c37b514cd2f8562470f791575\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-22e6dc2c37b514cd2f8562470f791575)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks_langchain import VectorSearchRetrieverTool, ChatDatabricks\n",
    "\n",
    "# Initialize the retriever tool.\n",
    "vs_tool = VectorSearchRetrieverTool(\n",
    "  endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "  index_name=vs_index_fullname,\n",
    "  tool_name=\"databricks_docs_retriever\",\n",
    "  tool_description=\"Retrieves information about Databricks products from official Databricks documentation.\",\n",
    "  disable_notice=True\n",
    ")\n",
    "\n",
    "# Run a query against the vector search index locally for testing\n",
    "vs_tool.invoke(\"Explain how Personal Access Token works in databricks\")\n",
    "\n",
    "# Bind the retriever tool to your Langchain LLM of choice\n",
    "llm = ChatDatabricks(endpoint=\"databricks-qwen3-next-80b-a3b-instruct\")\n",
    "llm_with_tools = llm.bind_tools([vs_tool])\n",
    "\n",
    "# Chat with your LLM to test the tool calling functionality\n",
    "llm_with_tools.invoke(\"Explain how Personal Access Token works in databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fea594fa-5702-424f-a25c-72dcf779c8e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 67
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.databricks_docs_vector_search (\n",
    "  -- The agent uses this comment to determine how to generate the query string parameter.\n",
    "  query STRING\n",
    "  COMMENT 'The query string for searching Databricks documentation.'\n",
    ") RETURNS TABLE\n",
    "-- The agent uses this comment to determine when to call this tool. It describes the types of documents and information contained within the index.\n",
    "COMMENT 'Executes a search on Databricks documentation to retrieve text documents most relevant to the input query.' RETURN\n",
    "SELECT\n",
    "  content as content,\n",
    "  map('uri', 'id') as metadata\n",
    "FROM\n",
    "  vector_search(\n",
    "    -- Specify your Vector Search index name here\n",
    "    index => 'workspace.default.db_docs_index',\n",
    "    -- workspace.default.db_docs_index\n",
    "    query => query,\n",
    "    num_results => 5\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37e29736-58d0-4880-a4c1-8711c6e68818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n",
    "\n",
    "toolkit = UCFunctionToolkit(\n",
    "    function_names=[\n",
    "        \"main.default.databricks_docs_vector_search\"\n",
    "    ]\n",
    ")\n",
    "tools = toolkit.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1c6fcf7e-cdb2-4dfc-947c-8fdc0161a0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='main__default__databricks_docs_vector_search' description='Executes a search on Databricks documentation to retrieve text documents most relevant to the input query.' args_schema=<class 'unitycatalog.ai.core.utils.function_processing_utils.main__default__databricks_docs_vector_search__params'> func=<function UCFunctionToolkit.uc_function_to_langchain_tool.<locals>.func at 0xffeb1f06d260> uc_function_name='main.default.databricks_docs_vector_search' client_config={'profile': None}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"format\": \"CSV\", \"value\": \"content,metadata\\\\n\\\\\"Databricks personal access token authentication  \\\\nDatabricks personal access tokens are one of the most well-supported types of credentials for resources and operations at the Databricks workspace level. Many storage mechanisms for credentials and related information, such as environment variables and Databricks configuration profiles, provide support for Databricks personal access tokens. Although users can have multiple personal access tokens in a Databricks workspace, each personal access token works for only a single Databricks workspace. The number of personal access tokens per user is limited to 600 per workspace.  \\\\nNote  \\\\nTo automate Databricks account-level functionality, you cannot use Databricks personal access tokens. Instead, you must use either OAuth tokens for Databricks account admin users or service principals, or the username and password of Databricks account-level admins. For more information, see:  \\\\nBasic authentication (legacy)  \\\\nOAuth machine-to-machine (M2M) authentication  \\\\nOAuth user-to-machine (U2M) authentication  \\\\nDatabricks personal access tokens for workspace users\\\\nDatabricks personal access tokens for workspace users\\\\nTo create a Databricks personal access token for your Databricks workspace user, do the following:  \\\\nIn your Databricks workspace, click your Databricks username in the top bar, and then select Settings from the drop down.  \\\\nClick Developer.  \\\\nNext to Access tokens, click Manage.  \\\\nClick Generate new token.  \\\\n(Optional) Enter a comment that helps you to identify this token in the future, and change the token\\\\u2019s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank).  \\\\nClick Generate.  \\\\nCopy the displayed token to a secure location, and then click Done.  \\\\nNote  \\\\nBe sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (Revoke) icon next to the token on the Access tokens page.  \\\\nIf you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:  \\\\nEnable or disable personal access token authentication for the workspace  \\\\nPersonal access token permissions\\\\n\\\\nDatabricks personal access tokens for service principals\\\\\",{\\'uri\\': \\'id\\'}\\\\n\\\\\"Monitor and manage personal access tokens  \\\\nTo authenticate to the Databricks REST API, a user can create a personal access token and use it in their REST API request. This article explains how workspace admins can manage personal access tokens in their workspace.  \\\\nTo create a personal access token, see Databricks personal access token authentication.  \\\\nTo create a personal access token on behalf of a service principal, see Manage tokens for a service principal.  \\\\nOverview of personal access token management\\\\nOverview of personal access token management\\\\nPersonal access tokens are enabled by default for all Databricks workspaces that were created in 2018 or later.  \\\\nWhen personal access tokens are enabled on a workspace, users with the CAN USE permission can generate personal access tokens to access Databricks REST APIs, and they can generate these tokens with any expiration date they like, including an indefinite lifetime. By default, no non-admin workspace users have the CAN USE permission, meaning that they cannot create or use personal access tokens.  \\\\nAs a Databricks workspace admin, you can disable personal access tokens for a workspace, monitor and revoke tokens, control which non-admin users can create tokens and use tokens, and set a maximum lifetime for new tokens.  \\\\nManaging personal access tokens in your workspace requires the Premium plan or above. To create a personal access token, see Databricks personal access token authentication.\\\\n\\\\nEnable or disable personal access token authentication for the workspace\\\\nEnable or disable personal access token authentication for the workspace\\\\nPersonal access token authentication is enabled by default for all Databricks workspaces that were created in 2018 or later. You can change this setting in the workspace settings page.  \\\\nWhen personal access tokens are disabled for a workspace, personal access tokens cannot be used to authenticate to Databricks and workspace users and service principals cannot create new tokens. No tokens are deleted when you disable personal access token authentication for a workspace. If tokens are re-enabled later, any non-expired tokens are available for use.  \\\\nIf you want to disable token access for a subset of users, you can keep personal access token authentication enabled for the workspace and set fine-grained permissions for users and groups. See Control who can create and use tokens.  \\\\nWarning  \\\\nPartner Connect, partner integrations, and service principals require personal access tokens to be enabled on a workspace.  \\\\nTo disable the ability to create and use personal access tokens for the workspace:  \\\\nGo to the settings page.  \\\\nClick the Advanced tab.  \\\\nClick the Personal Access Tokens toggle.  \\\\nClick Confirm.  \\\\nThis change may take a few seconds to take effect.  \\\\nYou can also use the Workspace configuration API to disable personal access tokens for the workspace.\\\\n\\\\nControl who can create and use tokens\\\\nControl who can create and use tokens\\\\nWorkspace admins can set permissions on personal access tokens to control which users, service principals, and groups can create and use tokens. For details on how to configure personal access token permissions, see Manage access to Databricks automation.\\\\n\\\\nSet maximum lifetime of new tokens\\\\nSet maximum lifetime of new tokens\\\\nYou can manage the maximum lifetime of new tokens in your workspace using the Databricks CLI. This limit applies only to new tokens.  \\\\nSet maxTokenLifetimeDays to the maximum token lifetime of new tokens in days, as an integer. If you set it to zero, new tokens are permitted to have no lifetime limit. For example:  \\\\ndatabricks workspace-conf set-status --json \\'{ \\\\\"\\\\\"maxTokenLifetimeDays\\\\\"\\\\\": \\\\\"\\\\\"90\\\\\"\\\\\" }\\'  \\\\nYou can also use the Workspace configuration API to manage the maximum lifetime for new tokens in a workspace.\\\\n\\\\nMonitor and revoke tokens\\\\nMonitor and revoke tokens\\\\nThis section describes how to use the Databricks CLI to manage existing tokens in the workspace. You can also use the Token Management API.  \\\\nGet tokens for the workspace  \\\\nTo get the workspace\\\\u2019s tokens:  \\\\ndatabricks token-management list  \\\\nYou can filter results by a user by using the flags created-by-id (to filter by the user ID) or created-by-username (to filter by the username).  \\\\nFor example:  \\\\ndatabricks token-management list --created-by-username user@company.com  \\\\nExample response:  \\\\nID Created By Comment token-id user@company.com dev  \\\\nDelete (revoke) a token  \\\\nTo delete a token, replace TOKEN_ID with the id of the token to delete:  \\\\ndatabricks token-management delete TOKEN_ID\\\\\",{\\'uri\\': \\'id\\'}\\\\n\\\\\"Databricks personal access token authentication\\\\nDatabricks personal access token authentication uses a Databricks personal access token to authenticate the target Databricks entity, such as a Databricks user account or a Databricks service principal. See also Databricks personal access token authentication.  \\\\nNote  \\\\nYou cannot use Databricks personal access token authentication for authenticating with a Databricks account, as Databricks account-level commands do not use Databricks personal access tokens for authentication. To authenticate with a Databricks account, consider using one of the following authentication types instead:  \\\\nBasic authentication (legacy)  \\\\nOAuth machine-to-machine (M2M) authentication  \\\\nOAuth user-to-machine (U2M) authentication  \\\\nTo create a personal access token, do the following:  \\\\nIn your Databricks workspace, click your Databricks username in the top bar, and then select Settings from the drop down.  \\\\nClick Developer.  \\\\nNext to Access tokens, click Manage.  \\\\nClick Generate new token.  \\\\n(Optional) Enter a comment that helps you to identify this token in the future, and change the token\\\\u2019s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank).  \\\\nClick Generate.  \\\\nCopy the displayed token to a secure location, and then click Done.  \\\\nNote  \\\\nBe sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (Revoke) icon next to the token on the Access tokens page.  \\\\nIf you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:  \\\\nEnable or disable personal access token authentication for the workspace  \\\\nPersonal access token permissions  \\\\nTo configure and use Databricks personal access token authentication, do the following:  \\\\nNote  \\\\nThe following procedure creates a Databricks configuration profile with the name DEFAULT. If you already have a DEFAULT configuration profile that you want to use, then skip this procedure. Otherwise, this procedure overwrites your existing DEFAULT configuration profile. To view the names and hosts of any existing configuration profiles, run the command databricks auth profiles.  \\\\nTo create a configuration profile with a name other than DEFAULT, add --profile <configuration-profile-name> or -p <configuration-profile-name> to the end of the following databricks configure command, replacing <configuration-profile-name> with the new configuration profile\\\\u2019s name.  \\\\nUse the Databricks CLI to run the following command:  \\\\ndatabricks configure  \\\\nFor the prompt Databricks Host, enter your Databricks workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com.  \\\\nFor the prompt Personal Access Token, enter the Databricks personal access token for your workspace.  \\\\nAfter you enter your Databricks personal access token, a corresponding configuration profile is added to your .databrickscfg file. If the Databricks CLI cannot find this file in its default location, it creates this file for you first and then adds this configuration profile to the new file. The default location for this file is in your ~ (your user home) folder on Unix, Linux, or macOS, or your %USERPROFILE% (your user home) folder on Windows.  \\\\nYou can now use the Databricks CLI\\\\u2019s --profile or -p option followed by the name of your configuration profile, as part of the Databricks CLI command call, for example databricks clusters list -p <configuration-profile-name>.\\\\n\\\\nBasic authentication (legacy)\\\\\",{\\'uri\\': \\'id\\'}\\\\n\\\\\"Manage access to Databricks automation  \\\\nThis article describes the how to configure permissions for Databricks credentials. To learn how to use credentials to authenticate to Databricks, see Authentication for Databricks automation - overview.  \\\\nNote  \\\\nDatabricks automation authentication permissions are available only in the Premium plan or above.  \\\\nPersonal access token permissions\\\\nPersonal access token permissions\\\\nWorkspace admins can set permissions on personal access tokens to control which users, service principals, and groups can create and use tokens. Before you can use token access control, a Databricks workspace admin must enable personal access tokens for the workspace. See Enable or disable personal access token authentication for the workspace.  \\\\nA workspace user can have one of the following token permissions:  \\\\nNO PERMISSIONS: User cannot create or use personal access tokens to authenticate to the Databricks workspace.  \\\\nCAN USE: User can create a personal access token and use it to authenticate to the workspace.  \\\\nCAN MANAGE (workspace admins only):** User can manage all workspace users\\\\u2019 personal access tokens and permission to use them. Users in the workspace admins group have this permission by default and you cannot revoke it. No other users, service principals, or groups can be granted this permission.  \\\\nThis table lists the permissions required for each token-related task:  \\\\nTask  \\\\nNO PERMISSIONS  \\\\nCAN USE  \\\\nCAN MANAGE  \\\\nCreate a token  \\\\nx  \\\\nx  \\\\nUse a token for authentication  \\\\nx  \\\\nx  \\\\nRevoke your own token  \\\\nx  \\\\nx  \\\\nRevoke any user\\\\u2019s or service principal\\\\u2019s token  \\\\nx  \\\\nList all tokens  \\\\nx  \\\\nModify token permissions  \\\\nx  \\\\nManage token permissions using the admin settings page  \\\\nThis section describes how to manage permissions using the workspace UI. You can also use the Permissions API or Databricks Terraform provider.  \\\\nGo to the settings page.  \\\\nClick the Advanced tab.  \\\\nNext to Personal Access Tokens, click the Permissions button to open the token permissions editor.  \\\\nSearch for and select the user, service principal, or group and choose the permission to assign.  \\\\nIf the users group has the CAN USE permission and you want to apply more fine-grained access for non-admin users, remove the CAN USE permission from the users group by clicking the X next to the permission drop-down menu in the users row.  \\\\nClick + Add.  \\\\nClick Save.  \\\\nWarning  \\\\nAfter you save your changes, any users who previously had either the CAN USE or CAN MANAGE permission and no longer have either permission are denied access to personal access token authentication and their active tokens are immediately deleted (revoked). Deleted tokens cannot be retrieved.\\\\n\\\\nPassword permissions\\\\nPassword permissions\\\\nWhen unified login is disabled, by default all workspace admin users can sign in to Databricks using either workspace-level SSO or their username and password, and all API users can authenticate to the Databricks REST APIs using their username and password.  \\\\nAs a workspace admin, when workspace-level SSO is enabled you can configure password access control to limit workspace admin users\\\\u2019 and API users\\\\u2019 ability to authenticate with their username and password.  \\\\nNote  \\\\nPassword access control can only be configured when unified login is disabled. Unified login is enabled for all accounts created after June 21, 2023. If unified login is enabled on your account and you require password access control, contact your Databricks account team.  \\\\nFor more information on the sign-in process when unified login is enabled, see Workspace sign-in process.  \\\\nThere are two permission levels for passwords: NO PERMISSIONS and CAN USE. CAN USE grants more abilities to workspace admins than to non-admin users. This table lists the abilities for each permission.  \\\\nTask  \\\\nNO PERMISSIONS  \\\\nCAN USE  \\\\nCan authenticate to the API using password  \\\\nx  \\\\nCan authenticate to the Databricks UI using password  \\\\nx (Workspace admins only)  \\\\nIf a non-admin user with no permissions attempts to make a REST API call using a password, authentication will fail. Databricks recommends personal access token REST authentication instead of username and password.  \\\\nWorkspace admin users with CAN USE permission see the Admin Log In tab on the sign-in page. They can choose to use that tab to log in to Databricks with username and password.  \\\\nWorkspace admins with no permissions do not see this page and must log in using SSO. When workspace-level SSO is enabled, all non-admin users do not see this page and must log in using SSO.  \\\\nConfigure password permissions  \\\\nThis section describes how to manage permissions using the workspace admin settings page.  \\\\nAs a workspace admin, log in to the Databricks workspace.  \\\\nClick your username in the top bar of the Databricks workspace and select Settings.  \\\\nClick the Advanced tab.  \\\\nNext to Password Usage, click Permission Settings.  \\\\nIn the Permissions Settings dialog, assign password permission to users and groups using the drop-down menu next to the user or group. You can also configure permissions for the Admins group.  \\\\nClick Save.  \\\\nYou can also configure password permissions using the Permissions API.\\\\\",{\\'uri\\': \\'id\\'}\\\\n\\\\\"Databricks personal access token\\\\nTo create a Databricks personal access token, do the following:  \\\\nIn your Databricks workspace, click your Databricks username in the top bar, and then select Settings from the drop down.  \\\\nClick Developer.  \\\\nNext to Access tokens, click Manage.  \\\\nClick Generate new token.  \\\\n(Optional) Enter a comment that helps you to identify this token in the future, and change the token\\\\u2019s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank).  \\\\nClick Generate.  \\\\nCopy the displayed token to a secure location, and then click Done.  \\\\nNote  \\\\nBe sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (Revoke) icon next to the token on the Access tokens page.  \\\\nIf you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:  \\\\nEnable or disable personal access token authentication for the workspace  \\\\nPersonal access token permissions  \\\\nTo authenticate using a Databricks personal access token, set the following configuration.  \\\\nFor a JDBC connection URL with embedded general configuration properties and sensitive credential properties:  \\\\njdbc:databricks://<server-hostname>:443;httpPath=<http-path>;AuthMech=3;UID=token;PWD=<personal-access-token>  \\\\nFor Java code with general configuration properties and sensitive credential properties set outside of the JDBC connection URL:  \\\\n// ... String url = \\\\\"\\\\\"jdbc:databricks://<server-hostname>:443\\\\\"\\\\\"; Properties p = new java.util.Properties(); p.put(\\\\\"\\\\\"httpPath\\\\\"\\\\\", \\\\\"\\\\\"<http-path>\\\\\"\\\\\"); p.put(\\\\\"\\\\\"AuthMech\\\\\"\\\\\", \\\\\"\\\\\"3\\\\\"\\\\\"); p.put(\\\\\"\\\\\"UID\\\\\"\\\\\", \\\\\"\\\\\"token\\\\\"\\\\\"); p.put(\\\\\"\\\\\"PWD\\\\\"\\\\\", \\\\\"\\\\\"<personal-access-token>\\\\\"\\\\\"); // ... Connection conn = DriverManager.getConnection(url, p); // ...  \\\\nFor a complete Java code example that you can adapt the preceding code snippet to you own needs, see the code example at the beginning of this article.  \\\\nIn the preceding URL or Java code, replace <personal-access-token> with the Databricks personal access token for your workspace user.  \\\\nTo get the values for <server-hostname> and <http-path>, see Compute settings for the Databricks JDBC Driver.\\\\n\\\\nDatabricks username and password\\\\nDatabricks username and password\\\\nDatabricks username and password authentication is also known as Databricks basic authentication.  \\\\nUsername and password authentication is possible only if single sign-on is disabled.  \\\\nTo authenticate using a Databricks username and password, set the following configuration.  \\\\nFor a JDBC connection URL with embedded general configuration properties and sensitive credential properties:  \\\\njdbc:databricks://<server-hostname>:443;httpPath=<http-path>;AuthMech=3;UID=<username>;PWD=<password>  \\\\nFor Java code with general configuration properties and sensitive credential properties set outside of the JDBC connection URL:  \\\\n// ... String url = \\\\\"\\\\\"jdbc:databricks://<server-hostname>:443\\\\\"\\\\\"; Properties p = new java.util.Properties(); p.put(\\\\\"\\\\\"httpPath\\\\\"\\\\\", \\\\\"\\\\\"<http-path>\\\\\"\\\\\"); p.put(\\\\\"\\\\\"AuthMech\\\\\"\\\\\", \\\\\"\\\\\"3\\\\\"\\\\\"); p.put(\\\\\"\\\\\"UID\\\\\"\\\\\", \\\\\"\\\\\"<username>\\\\\"\\\\\"); p.put(\\\\\"\\\\\"PWD\\\\\"\\\\\", \\\\\"\\\\\"<password>\\\\\"\\\\\"); // ... Connection conn = DriverManager.getConnection(url, p); // ...  \\\\nFor a complete Java code example that you can adapt the preceding code snippet to you own needs, see the code example at the beginning of this article.  \\\\nIn the preceding URL or Java code, replace <username> and <password> with the username and password.  \\\\nTo get the values for <server-hostname> and <http-path>, see Compute settings for the Databricks JDBC Driver.  \\\\nFor more information, see the Using User Name and Password section in the Databricks JDBC Driver Guide.\\\\n\\\\nOAuth 2.0 tokens\\\\\",{\\'uri\\': \\'id\\'}\\\\n\", \"truncated\": false}'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test unity catalog tool\n",
    "\n",
    "print(tools[0])\n",
    "tools[0].invoke({\"query\":\"how to monitor personal access token in databricks?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dae177c-9674-4510-bac7-469224e0234d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d6bd939-3a07-4434-bff4-681f4f336a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff74dab8-e0f2-421a-92d7-830f36afab7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dfe04b1-ee40-41d3-adf5-e6f369f62ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ede34018-0738-4187-9d59-190f40ac2a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:192)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:192)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)",
        "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import mlflow\n",
    "import yaml, sys, os\n",
    "import mlflow.models\n",
    "# Add the ../agent_eval path relative to current working directory\n",
    "# agent_eval_path = os.path.abspath(os.path.join(os.getcwd(), \"../02-agent-eval\"))\n",
    "conf_path = os.path.abspath(os.path.join(os.getcwd(), \"agent_config.yaml\"))\n",
    "\n",
    "sys.path.append(agent_conf_path)\n",
    "# Let's also use the same experiment as in our previous notebook to keep all the trace in a single place\n",
    "# mlflow.set_experiment(agent_eval_path+\"/02.1_agent_evaluation\")\n",
    "# conf_path = os.path.join(agent_eval_path, 'agent_config.yaml')\n",
    "\n",
    "try:\n",
    "    config = yaml.safe_load(open(conf_path))\n",
    "    config[\"config_version_name\"] = \"model_with_retriever\"\n",
    "    config[\"retriever_config\"] =  {\n",
    "        \"index_name\": vs_index_fullname,\n",
    "        \"tool_name\": \"product_technical_docs_retriever\",\n",
    "        \"num_results\": 1,\n",
    "        \"description\": \"Retrieves internal documentation about our products, infrastructure, router and other, including features, usage, and troubleshooting. Use this tool for any questions about product documentation or product issues.\"\n",
    "    }\n",
    "    yaml.dump(config, open(conf_path, \"w\"))\n",
    "except Exception as e:\n",
    "    print(f\"Skipped update - ignore for job run - {e}\")\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51bd09d0-8c49-4437-a4c0-8a6d735ea945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4638448018082628,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RAG-Experiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}