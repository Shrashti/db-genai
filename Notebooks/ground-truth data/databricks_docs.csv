id	url	content
22693	https://docs.databricks.com/en/admin/access-control/auth-external.html	"Enable authentication to external Databricks services  
Databricks administrators can enable users to authenticate directly to external Databricks services like the Ideas Portal and the Help Center using their Databricks workspace credentials.  
Requirements
Requirements
This feature requires the Premium plan or above.

Ideas Portal
Ideas Portal
The Ideas Portal, powered by Aha!, lets you provide product feedback, vote up product ideas, and check on the progress of your favorite ideas. When you enable authentication to the Ideas Portal, all of your users will be able to access the Ideas Portal using delegated Databricks authentication: as long as they have an active Databricks workspace session, they will be signed into the Ideas Portal automatically. They can go to the workspace help menu (click the question mark in the upper right) and click Feedback, or they can use the Databricks SSO login option when they go to ideas.databricks.com. If they do not have an active Databricks workspace session, they will be prompted to log into Databricks and then taken to the Ideas Portal.  
For more information about the Ideas Portal, see Submit product feedback.  
Access to the Ideas Portal is enabled by default for all users in your workspace. To change access:  
Go to the settings page.  
Click the Advanced tab.  
Click the Authentication to Ideas Portal toggle.  
Click Confirm.

Help Center
Help Center
If your organization has a Databricks Support contract, your authorized support contacts can use the Help Center, powered by Salesforce, to submit and monitor support cases. Users who are not support contacts can use the Help Center to search for help across multiple Databricks content sites without having to authenticate.  
When you enable authentication to the Help Center, users will be able to access the case submission and monitoring interface in the Help Center using delegated Databricks authentication, as long as their Databricks username for this workspace is registered in Salesforce as an authorized support contact. Delegated Databricks authentication signs such users in to the Help Center automatically if they have an active workspace session in their browser. They can go to the workspace help menu (click the question mark in the upper right) and click Support, or they can use the Databricks Sign-in option when they go to the Help Center. If they do not have an active Databricks workspace session when they attempt to sign in from the Help Center, they will be prompted to log into Databricks and then taken to the Help Center.  
For more information about the support process, see Support.  
Access to the Help Center is enabled by default for all users who meet the criteria described above. To change access:  
Go to the settings page.  
Click the Advanced tab.  
Click the Authentication to Help Center toggle.  
Click Confirm."
22694	https://docs.databricks.com/en/admin/access-control/tokens.html	"Monitor and manage personal access tokens  
To authenticate to the Databricks REST API, a user can create a personal access token and use it in their REST API request. This article explains how workspace admins can manage personal access tokens in their workspace.  
To create a personal access token, see Databricks personal access token authentication.  
To create a personal access token on behalf of a service principal, see Manage tokens for a service principal.  
Overview of personal access token management
Overview of personal access token management
Personal access tokens are enabled by default for all Databricks workspaces that were created in 2018 or later.  
When personal access tokens are enabled on a workspace, users with the CAN USE permission can generate personal access tokens to access Databricks REST APIs, and they can generate these tokens with any expiration date they like, including an indefinite lifetime. By default, no non-admin workspace users have the CAN USE permission, meaning that they cannot create or use personal access tokens.  
As a Databricks workspace admin, you can disable personal access tokens for a workspace, monitor and revoke tokens, control which non-admin users can create tokens and use tokens, and set a maximum lifetime for new tokens.  
Managing personal access tokens in your workspace requires the Premium plan or above. To create a personal access token, see Databricks personal access token authentication.

Enable or disable personal access token authentication for the workspace
Enable or disable personal access token authentication for the workspace
Personal access token authentication is enabled by default for all Databricks workspaces that were created in 2018 or later. You can change this setting in the workspace settings page.  
When personal access tokens are disabled for a workspace, personal access tokens cannot be used to authenticate to Databricks and workspace users and service principals cannot create new tokens. No tokens are deleted when you disable personal access token authentication for a workspace. If tokens are re-enabled later, any non-expired tokens are available for use.  
If you want to disable token access for a subset of users, you can keep personal access token authentication enabled for the workspace and set fine-grained permissions for users and groups. See Control who can create and use tokens.  
Warning  
Partner Connect, partner integrations, and service principals require personal access tokens to be enabled on a workspace.  
To disable the ability to create and use personal access tokens for the workspace:  
Go to the settings page.  
Click the Advanced tab.  
Click the Personal Access Tokens toggle.  
Click Confirm.  
This change may take a few seconds to take effect.  
You can also use the Workspace configuration API to disable personal access tokens for the workspace.

Control who can create and use tokens
Control who can create and use tokens
Workspace admins can set permissions on personal access tokens to control which users, service principals, and groups can create and use tokens. For details on how to configure personal access token permissions, see Manage access to Databricks automation.

Set maximum lifetime of new tokens
Set maximum lifetime of new tokens
You can manage the maximum lifetime of new tokens in your workspace using the Databricks CLI. This limit applies only to new tokens.  
Set maxTokenLifetimeDays to the maximum token lifetime of new tokens in days, as an integer. If you set it to zero, new tokens are permitted to have no lifetime limit. For example:  
databricks workspace-conf set-status --json '{ ""maxTokenLifetimeDays"": ""90"" }'  
You can also use the Workspace configuration API to manage the maximum lifetime for new tokens in a workspace.

Monitor and revoke tokens
Monitor and revoke tokens
This section describes how to use the Databricks CLI to manage existing tokens in the workspace. You can also use the Token Management API.  
Get tokens for the workspace  
To get the workspace’s tokens:  
databricks token-management list  
You can filter results by a user by using the flags created-by-id (to filter by the user ID) or created-by-username (to filter by the username).  
For example:  
databricks token-management list --created-by-username user@company.com  
Example response:  
ID Created By Comment token-id user@company.com dev  
Delete (revoke) a token  
To delete a token, replace TOKEN_ID with the id of the token to delete:  
databricks token-management delete TOKEN_ID"
22695	https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-call-api.html	"Step 4: Call the log delivery API  
This article describes how to call the log delivery API. This is the last step in the audit log delivery configuration.  
To configure log delivery, you must call the log delivery API.  
Required values
Required values
In your API call, specify the following values that you copied in the previous steps:  
credentials_id: Your Databricks credential configuration ID, which represents your cross-account role credentials.  
storage_configuration_id: Your Databricks storage configuration ID, which represents your root S3 bucket.  
Also set the following fields:  
log_type: Set to AUDIT_LOGS.  
output_format: Set to JSON.  
delivery_path_prefix: (Optional) Set to the path prefix. This must match the path prefix that you used in your role policy. The delivery path is <bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs_<internal-id>.json. If you configure audit log delivery for the entire account, account-level audit events that are not associated with any single workspace are delivered to the workspaceId=0 partition.  
workspace_ids_filter: (Optional) To ensure delivery of account-level events, including Unity Catalog and Delta Sharing events, leave workspace_ids_filter empty. If you only want logs for select workspaces, set to an array of workspace IDs (each one is an int64). If you add specific workspace IDs in this field, you won’t receive account-level logs and or logs for workspaces created in the future.

API call example
API call example
Here is an example call to the log delivery API:  
curl -X POST 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/log-delivery' \ --header 'Authorization: Bearer $OAUTH_TOKEN' \ -d '{ ""log_delivery_configuration"": { ""log_type"": ""AUDIT_LOGS"", ""config_name"": ""audit log config"", ""output_format"": ""JSON"", ""credentials_id"": ""<databricks-credentials-id>"", ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""delivery_path_prefix"": ""auditlogs-data"", ""workspace_ids_filter"": [ 6383650456894062, 4102272838062927 ] } }'  
Example response:  
{ ""log_delivery_configuration"": { ""config_id"": ""<config-id>"", ""config_name"": ""audit log config"", ""log_type"": ""AUDIT_LOGS"", ""output_format"": ""JSON"", ""account_id"": ""<account-id>"", ""credentials_id"": ""<databricks-credentials-id>"", ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""workspace_ids_filter"": [ 6383650456894062, 4102272838062927 ], ""delivery_path_prefix"": ""auditlogs-data"", ""status"": ""ENABLED"", ""creation_time"": 1591638409000, ""update_time"": 1593108904000, ""log_delivery_status"": { ""status"": ""CREATED"", ""message"": ""Log Delivery Configuration is successfully created. Status will be updated after the first delivery attempt."" } } }  
Note  
After initial setup or other log delivery configuration changes, expect a delay of up to one hour until changes take effect. After logging delivery begins, auditable events are typically logged within 15 minutes.

Next steps
Next steps
Once you’ve configured your audit log delivery, learn more about the log schema and available logs by referencing the Audit log reference."
22696	https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html	"Step 2: Configure credentials for audit log delivery  
This article describes how to set up IAM services for audit log delivery. To use different credentials for different workspaces, repeat the procedures in this article for each workspace or group of workspaces.  
Note  
To use different S3 bucket names, you need to create separate IAM roles.  
Create the IAM role
Create the IAM role
Log into your AWS Console as a user with administrator privileges and go to the IAM service.  
Click the Roles tab in the sidebar.  
Click Create role.  
In Select type of trusted entity, click AWS service.  
Under Use Case, select EC2.  
Click the Next button.  
Click the Next button.  
In the Role name field, enter a role name.  
Click Create role. The list of roles displays.

Create the inline policy
Create the inline policy
In the list of roles, click the role you created.  
Add an inline policy.  
On the Permissions tab, click Add permissions then Create inline policy.  
In the policy editor, click the JSON tab.  
Copy this access policy and modify it. Replace the following values in the policy with your own configuration values:  
<s3-bucket-name>: The bucket name of your AWS S3 bucket.  
<s3-bucket-path-prefix>: (Optional) The path to the delivery location in the S3 bucket. If unspecified, the logs are delivered to the root of the bucket. This path must match the delivery_path_prefix argument when you call the log delivery API.  
{ ""Version"":""2012-10-17"", ""Statement"":[ { ""Effect"":""Allow"", ""Action"":[ ""s3:GetBucketLocation"" ], ""Resource"":[ ""arn:aws:s3:::<s3-bucket-name>"" ] }, { ""Effect"":""Allow"", ""Action"":[ ""s3:PutObject"", ""s3:GetObject"", ""s3:DeleteObject"", ""s3:PutObjectAcl"", ""s3:AbortMultipartUpload"" ], ""Resource"":[ ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/"", ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/*"" ] }, { ""Effect"":""Allow"", ""Action"":[ ""s3:ListBucket"", ""s3:ListMultipartUploadParts"", ""s3:ListBucketMultipartUploads"" ], ""Resource"":""arn:aws:s3:::<s3-bucket-name>"", ""Condition"":{ ""StringLike"":{ ""s3:prefix"":[ ""<s3-bucket-path-prefix>"", ""<s3-bucket-path-prefix>/*"" ] } } } ] }  
You can customize the policy usage of the path prefix in the following ways:  
If you do not want to use the bucket path prefix, remove <s3-bucket-path-prefix>/ (including the final slash) from the policy each time it appears.  
If you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes, you can include multiple path prefixes. There are two separate parts of the policy that reference <s3-bucket-path-prefix>. For each case, duplicate the two lines that reference the path prefix. For example:  
{ ""Resource"":[ ""arn:aws:s3:::<mybucketname>/field-team/"", ""arn:aws:s3:::<mybucketname>/field-team/*"", ""arn:aws:s3:::<mybucketname>/finance-team/"", ""arn:aws:s3:::<mybucketname>/finance-team/*"" ] }  
Click Review policy.  
In the Name field, enter a policy name.  
Click Create policy.  
If you use service control policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is whitelisted so Databricks can assume the cross-account role.

Create the trust policy"
22697	https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-credentials.html	"Create the trust policy
On the role summary page, click the Trust Relationships tab.  
Paste this access policy into the editor, replacing <databricks-account-id> with your Databricks account ID. The policy uses the Databricks AWS account ID 414351767826. If you are are using Databricks on AWS GovCloud use the Databricks account ID 044793339203.  
{ ""Version"":""2012-10-17"", ""Statement"":[ { ""Effect"":""Allow"", ""Principal"":{ ""AWS"":""arn:aws:iam::414351767826:role/SaasUsageDeliveryRole-prod-IAMRole-3PLHICCRR1TK"" }, ""Action"":""sts:AssumeRole"", ""Condition"":{ ""StringEquals"":{ ""sts:ExternalId"":[ ""<databricks-account-id>"" ] } } } ] }  
In the role summary, copy the Role ARN. You need this value to call the create credential configuration API in the next step.

Call the create credential configuration API
Call the create credential configuration API
To finish settings up your credentials, call the Create credential configuration API.  
This request establishes cross-account trust and returns a reference ID you can use when creating a new workspace.  
Replace <account-id> with your Databricks account ID.  
Set credentials_name to a name that is unique within your account.  
Set role_arn to the role ARN that you just created.  
The response body includes a credentials_id field. Copy this field so you can use it to create the log delivery configuration in Step 4.  
For example:  
curl -X POST -n \ 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/credentials' \ -d '{ ""credentials_name"": ""databricks-credentials-v1"", ""aws_credentials"": { ""sts_role"": { ""role_arn"": ""arn:aws:iam::<aws-account-id>:role/my-company-example-role"" } } }'  
Example response:  
{ ""credentials_id"": ""<databricks-credentials-id>"", ""account_id"": ""<databricks-account-id>"", ""aws_credentials"": { ""sts_role"": { ""role_arn"": ""arn:aws:iam::<aws-account-id>:role/my-company-example-role"", ""external_id"": ""<databricks-account-id>"" } }, ""credentials_name"": ""databricks-credentials-v1"", ""creation_time"": 1579753556257 }  
Again, copy the credentials_id field from the response for later use.

Next steps
Next steps
If you need to set up cross-account delivery (your S3 bucket is in a different AWS account than the IAM role used for log delivery), see Step 3: Configure cross-account support (Optional).  
If your S3 bucket is in the same AWS account as your IAM role used for log delivery, skip to the final step of calling the log delivery API. See Step 4: Call the log delivery API."
22698	https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-cross-account.html	"Step 3: Configure cross-account support (Optional)  
This article describes how to set up cross-account audit log delivery. If your S3 bucket is in the same AWS account as your IAM role used for log delivery, skip this step.  
To deliver logs to an AWS account other than the one used for your Databricks workspace, you must add the S3 bucket policy provided in this step. This policy references IDs for the cross-account IAM role that you created in Step 2: Configure credentials for audit log delivery.  
In the AWS Console, go to the S3 service.  
Click the bucket name.  
Click the Permissions tab.  
Click the Bucket Policy button.  
Click the Edit button.  
Copy and modify this bucket policy. Replace <s3-bucket-name> with the S3 bucket name, <customer-iam-role-id> with the role ID of your newly-created IAM role, and <s3-bucket-path-prefix> with the bucket path prefix you want.  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": [""arn:aws:iam::<customer-iam-role-id>""] }, ""Action"": ""s3:GetBucketLocation"", ""Resource"": ""arn:aws:s3:::<s3-bucket-name>"" }, { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""arn:aws:iam::<customer-iam-role-id>"" }, ""Action"": [ ""s3:PutObject"", ""s3:GetObject"", ""s3:DeleteObject"", ""s3:PutObjectAcl"", ""s3:AbortMultipartUpload"", ""s3:ListMultipartUploadParts"" ], ""Resource"": [ ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/"", ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/*"" ] }, { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""arn:aws:iam::<customer-iam-role-id>"" }, ""Action"": ""s3:ListBucket"", ""Resource"": ""arn:aws:s3:::<s3-bucket-name>"", ""Condition"": { ""StringLike"": { ""s3:prefix"": [ ""<s3-bucket-path-prefix>"", ""<s3-bucket-path-prefix>/*"" ] } } } ] }  
Customize path prefixes
Customize path prefixes
You can customize the policy use of the path prefix:  
If you do not want to use the bucket path prefix, remove <s3-bucket-path-prefix>/ (including the final slash) from the policy each time it appears.  
If you want log delivery configurations for multiple workspaces that share the S3 bucket but use different path prefixes, you can include multiple path prefixes. There are two separate parts of the policy that reference <s3-bucket-path-prefix>. For each case, duplicate the two lines that reference the path prefix. For example:  
{ ""Resource"":[ ""arn:aws:s3:::<mybucketname>/field-team/"", ""arn:aws:s3:::<mybucketname>/field-team/*"", ""arn:aws:s3:::<mybucketname>/finance-team/"", ""arn:aws:s3:::<mybucketname>/finance-team/*"" ] }

Next steps
Next steps
Finally, you’ll call the log delivery API to finish setting up delivery. See Step 4: Call the log delivery API."
22699	https://docs.databricks.com/en/admin/account-settings-e2/audit-aws-storage.html	"Step 1: Configure audit log storage  
This article explains how to set up an AWS S3 storage bucket for low-latency delivery of audit logs.  
Create the S3 bucket
Create the S3 bucket
Log into your AWS Console as a user with administrator privileges and go to the S3 service.  
Click the Create bucket button.  
In Bucket name, enter a name for your bucket. For more bucket naming guidance, see the AWS bucket naming rules.  
Click Create bucket.

Create a Databricks storage configuration record
Create a Databricks storage configuration record
Next, you need to create a Databricks storage configuration record that represents your new S3 bucket.  
Specify your S3 bucket by calling the create new storage configuration API.  
Pass the following values:  
storage_configuration_name: New unique storage configuration name.  
root_bucket_info: A JSON object that contains a bucket_name field that contains your S3 bucket name.  
For example:  
curl -X POST 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/storage-configurations' \ --header 'Authorization: Bearer $OAUTH_TOKEN' \ -d '{ ""storage_configuration_name"": ""databricks-workspace-storageconf-v1"", ""root_bucket_info"": { ""bucket_name"": ""my-company-example-bucket"" } }'  
Response:  
{ ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""account_id"": ""<databricks-account-id>"", ""root_bucket_info"": { ""bucket_name"": ""my-company-example-bucket"" }, ""storage_configuration_name"": ""databricks-workspace-storageconf-v1"", ""creation_time"": 1579754875555 }  
Copy the storage_configuration_id value returned in the response body. You’ll need it when you call the log delivery API.

Next steps
Next steps
Next, configure an IAM role and create a credential in Databricks. See Step 2: Configure credentials for audit log delivery."
22700	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"Create an IAM role for workspace deployment  
This article describes how to:  
Create and configure a cross-account IAM role for Databricks workspace deployment. This role gives Databricks limited access to your AWS account for the purposes of creating and managing compute and VPC resources.  
Use the Databricks account console to create a credential configuration that references the IAM role.  
Requirements
Requirements
You need to be a Databricks account admin.

Automate IAM role creation
Automate IAM role creation
You can automate the IAM role creation by using one of the following automation options:  
The AWS Quick Start (CloudFormation) to deploy your workspace. This is the recommended workspace deployment method.  
The Databricks Terraform provider. See Create Databricks workspaces using Terraform.

Manual IAM role creation
Manual IAM role creation
The following steps apply to a custom AWS workspace deployment. You only need to follow these steps if you are deploying a workspace using the Custom AWS configuration option.  
Step 1: Create a cross-account IAM role  
Step 2: Create an access policy  
Step 3: Create a credential configuration for the role in Databricks

Step 1: Create a cross-account IAM role
Step 1: Create a cross-account IAM role
Get your Databricks account ID. See Locate your account ID.  
Log into your AWS Console as a user with administrator privileges and go to the IAM console.  
Click the Roles tab in the sidebar.  
Click Create role.  
In Select type of trusted entity, click the AWS account tile.  
Select the Another AWS account checkbox.  
In the Account ID field, enter the Databricks account ID 414351767826. This is not the Account ID you copied from the Databricks account console. If you are are using Databricks on AWS GovCloud use the Databricks account ID 044793339203.  
Select the Require external ID checkbox.  
In the External ID field, enter your Databricks account ID, which you copied from the Databricks account console.  
Click the Next button.  
In the Add Permissions page, click the Next button. You should now be on the Name, review, and create page.  
In the Role name field, enter a role name.  
Click Create role. The list of roles appears.

Step 2: Create an access policy"
22701	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"Step 2: Create an access policy
The access policy you add to the role depends on your Amazon VPC (Virtual Private Cloud) deployment type. For information about how Databricks uses each permission, see IAM permissions for Databricks-managed VPCs. Use the policy instructions that describe your deployment:  
Option 1: Default. A single VPC that Databricks creates and configures in your AWS account. This is the default configuration.  
Option 2: Customer-managed VPC with default restrictions. Create your Databricks workspaces in your own VPC, using a feature known as customer-managed VPC.  
Option 3: Customer-managed VPC with custom restrictions. Create your Databricks workspaces in your own VPC with custom restrictions for account ID, VPC ID, AWS Region, and security group.  
Option 1: Default deployment policy  
In the Roles section of the IAM console, click the IAM role you created in Step 1.  
Click the Add permissions drop-down and select Create inline policy.  
In the policy editor, click the JSON tab.  
Copy and paste the following access policy:  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Sid"": ""Stmt1403287045000"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AllocateAddress"", ""ec2:AssignPrivateIpAddresses"", ""ec2:AssociateDhcpOptions"", ""ec2:AssociateIamInstanceProfile"", ""ec2:AssociateRouteTable"", ""ec2:AttachInternetGateway"", ""ec2:AttachVolume"", ""ec2:AuthorizeSecurityGroupEgress"", ""ec2:AuthorizeSecurityGroupIngress"", ""ec2:CancelSpotInstanceRequests"", ""ec2:CreateDhcpOptions"", ""ec2:CreateFleet"", ""ec2:CreateInternetGateway"", ""ec2:CreateLaunchTemplate"", ""ec2:CreateLaunchTemplateVersion"", ""ec2:CreateNatGateway"", ""ec2:CreateRoute"", ""ec2:CreateRouteTable"", ""ec2:CreateSecurityGroup"", ""ec2:CreateSubnet"", ""ec2:CreateTags"", ""ec2:CreateVolume"", ""ec2:CreateVpc"", ""ec2:CreateVpcEndpoint"", ""ec2:DeleteDhcpOptions"", ""ec2:DeleteFleets"", ""ec2:DeleteInternetGateway"", ""ec2:DeleteLaunchTemplate"", ""ec2:DeleteLaunchTemplateVersions"", ""ec2:DeleteNatGateway"", ""ec2:DeleteRoute"", ""ec2:DeleteRouteTable"", ""ec2:DeleteSecurityGroup"", ""ec2:DeleteSubnet"", ""ec2:DeleteTags"", ""ec2:DeleteVolume"", ""ec2:DeleteVpc"", ""ec2:DeleteVpcEndpoints"", ""ec2:DescribeAvailabilityZones"", ""ec2:DescribeFleetHistory"", ""ec2:DescribeFleetInstances"", ""ec2:DescribeFleets"", ""ec2:DescribeIamInstanceProfileAssociations"", ""ec2:DescribeInstanceStatus"", ""ec2:DescribeInstances"", ""ec2:DescribeInternetGateways"", ""ec2:DescribeLaunchTemplates"", ""ec2:DescribeLaunchTemplateVersions"", ""ec2:DescribeNatGateways"", ""ec2:DescribePrefixLists"", ""ec2:DescribeReservedInstancesOfferings"", ""ec2:DescribeRouteTables"", ""ec2:DescribeSecurityGroups"", ""ec2:DescribeSpotInstanceRequests"", ""ec2:DescribeSpotPriceHistory"", ""ec2:DescribeSubnets"", ""ec2:DescribeVolumes"", ""ec2:DescribeVpcs"", ""ec2:DetachInternetGateway"", ""ec2:DisassociateIamInstanceProfile"", ""ec2:DisassociateRouteTable"", ""ec2:GetLaunchTemplateData"", ""ec2:GetSpotPlacementScores"", ""ec2:ModifyFleet"", ""ec2:ModifyLaunchTemplate"", ""ec2:ModifyVpcAttribute"", ""ec2:ReleaseAddress"", ""ec2:ReplaceIamInstanceProfileAssociation"", ""ec2:RequestSpotInstances"", ""ec2:RevokeSecurityGroupEgress"", ""ec2:RevokeSecurityGroupIngress"", ""ec2:RunInstances"", ""ec2:TerminateInstances"" ], ""Resource"": [ ""*"" ] }, { ""Effect"": ""Allow"", ""Action"": [ ""iam:CreateServiceLinkedRole"", ""iam:PutRolePolicy"" ], ""Resource"": ""arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot"", ""Condition"": { ""StringLike"": { ""iam:AWSServiceName"": ""spot.amazonaws.com"" } } } ] }  
Click Review policy.  
In the Name field, enter a policy name.  
Click Create policy.  
(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  
In the role summary, copy the Role ARN to add to Databricks.  
Option 2: Customer-managed VPC with default restrictions policy  
Log into your AWS Console as a user with administrator privileges and go to the IAM console.  
Click the Roles tab in the sidebar.  
In the list of roles, click the cross-account IAM role that you created in Step 1.  
Click the Add permissions drop-down and select Create inline policy.  
In the policy editor, click the JSON tab.  
Copy and paste the following access policy."
22702	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"Click the Add permissions drop-down and select Create inline policy.  
In the policy editor, click the JSON tab.  
Copy and paste the following access policy.  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Sid"": ""Stmt1403287045000"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AssociateIamInstanceProfile"", ""ec2:AttachVolume"", ""ec2:AuthorizeSecurityGroupEgress"", ""ec2:AuthorizeSecurityGroupIngress"", ""ec2:CancelSpotInstanceRequests"", ""ec2:CreateTags"", ""ec2:CreateVolume"", ""ec2:DeleteTags"", ""ec2:DeleteVolume"", ""ec2:DescribeAvailabilityZones"", ""ec2:DescribeIamInstanceProfileAssociations"", ""ec2:DescribeInstanceStatus"", ""ec2:DescribeInstances"", ""ec2:DescribeInternetGateways"", ""ec2:DescribeNatGateways"", ""ec2:DescribeNetworkAcls"", ""ec2:DescribePrefixLists"", ""ec2:DescribeReservedInstancesOfferings"", ""ec2:DescribeRouteTables"", ""ec2:DescribeSecurityGroups"", ""ec2:DescribeSpotInstanceRequests"", ""ec2:DescribeSpotPriceHistory"", ""ec2:DescribeSubnets"", ""ec2:DescribeVolumes"", ""ec2:DescribeVpcAttribute"", ""ec2:DescribeVpcs"", ""ec2:DetachVolume"", ""ec2:DisassociateIamInstanceProfile"", ""ec2:ReplaceIamInstanceProfileAssociation"", ""ec2:RequestSpotInstances"", ""ec2:RevokeSecurityGroupEgress"", ""ec2:RevokeSecurityGroupIngress"", ""ec2:RunInstances"", ""ec2:TerminateInstances"", ""ec2:DescribeFleetHistory"", ""ec2:ModifyFleet"", ""ec2:DeleteFleets"", ""ec2:DescribeFleetInstances"", ""ec2:DescribeFleets"", ""ec2:CreateFleet"", ""ec2:DeleteLaunchTemplate"", ""ec2:GetLaunchTemplateData"", ""ec2:CreateLaunchTemplate"", ""ec2:DescribeLaunchTemplates"", ""ec2:DescribeLaunchTemplateVersions"", ""ec2:ModifyLaunchTemplate"", ""ec2:DeleteLaunchTemplateVersions"", ""ec2:CreateLaunchTemplateVersion"", ""ec2:AssignPrivateIpAddresses"", ""ec2:GetSpotPlacementScores"" ], ""Resource"": [ ""*"" ] }, { ""Effect"": ""Allow"", ""Action"": [ ""iam:CreateServiceLinkedRole"", ""iam:PutRolePolicy"" ], ""Resource"": ""arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot"", ""Condition"": { ""StringLike"": { ""iam:AWSServiceName"": ""spot.amazonaws.com"" } } } ] }  
Click Review policy.  
In the Name field, enter a policy name.  
Click Create policy.  
(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  
In the role summary, copy the Role ARN.  
Option 3: Customer-managed VPC with custom policy restrictions  
Note  
The Databricks production AWS account from which Amazon Machine Images (AMI) are sourced is 601306020600. You can use this account ID to create custom access policies that restrict which AMIs can be used within your AWS account. For more information, contact your Databricks account team.  
Log into your AWS Console as a user with administrator privileges and go to the IAM console.  
Click the Roles tab in the sidebar.  
In the list of roles, click the cross-account IAM role that you created for in Step 1.  
Click the Add permissions dropdown then Create inline policy.  
In the policy editor, click the JSON tab.  
Copy and paste the following access policy.  
Replace the following values in the policy with your own configuration values:  
ACCOUNTID — Your AWS account ID, which is a number.  
VPCID — ID of the AWS VPC where you want to launch workspaces.  
REGION — AWS Region name for your VPC deployment, for example us-west-2.  
SECURITYGROUPID — ID of your AWS security group. When you add a security group restriction, you cannot reuse the cross-account IAM role or reference a credentials ID (credentials_id) for any other workspaces. For those other workspaces, you must create separate roles, policies, and credentials objects.  
Note  
If you have custom requirements configured for security groups with your customer-managed vpc, contact your Databricks account team for assistance with IAM policy customizations."
22703	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Sid"": ""NonResourceBasedPermissions"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AssignPrivateIpAddresses"", ""ec2:CancelSpotInstanceRequests"", ""ec2:DescribeAvailabilityZones"", ""ec2:DescribeIamInstanceProfileAssociations"", ""ec2:DescribeInstanceStatus"", ""ec2:DescribeInstances"", ""ec2:DescribeInternetGateways"", ""ec2:DescribeNatGateways"", ""ec2:DescribeNetworkAcls"", ""ec2:DescribePrefixLists"", ""ec2:DescribeReservedInstancesOfferings"", ""ec2:DescribeRouteTables"", ""ec2:DescribeSecurityGroups"", ""ec2:DescribeSpotInstanceRequests"", ""ec2:DescribeSpotPriceHistory"", ""ec2:DescribeSubnets"", ""ec2:DescribeVolumes"", ""ec2:DescribeVpcAttribute"", ""ec2:DescribeVpcs"", ""ec2:CreateTags"", ""ec2:DeleteTags"", ""ec2:GetSpotPlacementScores"", ""ec2:RequestSpotInstances"", ""ec2:DescribeFleetHistory"", ""ec2:ModifyFleet"", ""ec2:DeleteFleets"", ""ec2:DescribeFleetInstances"", ""ec2:DescribeFleets"", ""ec2:CreateFleet"", ""ec2:DeleteLaunchTemplate"", ""ec2:GetLaunchTemplateData"", ""ec2:CreateLaunchTemplate"", ""ec2:DescribeLaunchTemplates"", ""ec2:DescribeLaunchTemplateVersions"", ""ec2:ModifyLaunchTemplate"", ""ec2:DeleteLaunchTemplateVersions"", ""ec2:CreateLaunchTemplateVersion"" ], ""Resource"": [ ""*"" ] }, { ""Sid"": ""InstancePoolsSupport"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AssociateIamInstanceProfile"", ""ec2:DisassociateIamInstanceProfile"", ""ec2:ReplaceIamInstanceProfileAssociation"" ], ""Resource"": ""arn:aws:ec2:REGION:ACCOUNTID:instance/*"", ""Condition"": { ""StringEquals"": { ""ec2:ResourceTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""AllowEc2RunInstancePerTag"", ""Effect"": ""Allow"", ""Action"": ""ec2:RunInstances"", ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:volume/*"", ""arn:aws:ec2:REGION:ACCOUNTID:instance/*"" ], ""Condition"": { ""StringEquals"": { ""aws:RequestTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""AllowEc2RunInstanceImagePerTag"", ""Effect"": ""Allow"", ""Action"": ""ec2:RunInstances"", ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:image/*"" ], ""Condition"": { ""StringEquals"": { ""aws:ResourceTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""AllowEc2RunInstancePerVPCid"", ""Effect"": ""Allow"", ""Action"": ""ec2:RunInstances"", ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:network-interface/*"", ""arn:aws:ec2:REGION:ACCOUNTID:subnet/*"", ""arn:aws:ec2:REGION:ACCOUNTID:security-group/*"" ], ""Condition"": { ""StringEquals"": { ""ec2:vpc"": ""arn:aws:ec2:REGION:ACCOUNTID:vpc/VPCID"" } } }, { ""Sid"": ""AllowEc2RunInstanceOtherResources"", ""Effect"": ""Allow"", ""Action"": ""ec2:RunInstances"", ""NotResource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:image/*"", ""arn:aws:ec2:REGION:ACCOUNTID:network-interface/*"", ""arn:aws:ec2:REGION:ACCOUNTID:subnet/*"", ""arn:aws:ec2:REGION:ACCOUNTID:security-group/*"", ""arn:aws:ec2:REGION:ACCOUNTID:volume/*"", ""arn:aws:ec2:REGION:ACCOUNTID:instance/*"" ] }, { ""Sid"": ""EC2TerminateInstancesTag"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:TerminateInstances"" ], ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:instance/*"" ], ""Condition"": { ""StringEquals"": { ""ec2:ResourceTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""EC2AttachDetachVolumeTag"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AttachVolume"", ""ec2:DetachVolume"" ], ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:instance/*"", ""arn:aws:ec2:REGION:ACCOUNTID:volume/*"" ], ""Condition"": { ""StringEquals"": { ""ec2:ResourceTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""EC2CreateVolumeByTag"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:CreateVolume"" ], ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:volume/*"" ], ""Condition"": { ""StringEquals"": { ""aws:RequestTag/Vendor"": ""Databricks"" } } }, { ""Sid"": ""EC2DeleteVolumeByTag"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:DeleteVolume"" ],"
22704	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"""Databricks"" } } }, { ""Sid"": ""EC2DeleteVolumeByTag"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:DeleteVolume"" ], ""Resource"": [ ""arn:aws:ec2:REGION:ACCOUNTID:volume/*"" ], ""Condition"": { ""StringEquals"": { ""ec2:ResourceTag/Vendor"": ""Databricks"" } } }, { ""Effect"": ""Allow"", ""Action"": [ ""iam:CreateServiceLinkedRole"", ""iam:PutRolePolicy"" ], ""Resource"": ""arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot"", ""Condition"": { ""StringLike"": { ""iam:AWSServiceName"": ""spot.amazonaws.com"" } } }, { ""Sid"": ""VpcNonresourceSpecificActions"", ""Effect"": ""Allow"", ""Action"": [ ""ec2:AuthorizeSecurityGroupEgress"", ""ec2:AuthorizeSecurityGroupIngress"", ""ec2:RevokeSecurityGroupEgress"", ""ec2:RevokeSecurityGroupIngress"" ], ""Resource"": ""arn:aws:ec2:REGION:ACCOUNTID:security-group/SECURITYGROUPID"", ""Condition"": { ""StringEquals"": { ""ec2:vpc"": ""arn:aws:ec2:REGION:ACCOUNTID:vpc/VPCID"" } } } ] }"
22705	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"Click Review policy.  
In the Name field, enter a policy name.  
Click Create policy.  
(Optional) If you use Service Control Policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is allowlisted so Databricks can assume the cross-account role.  
In the role summary, copy the Role ARN."
22706	https://docs.databricks.com/en/admin/account-settings-e2/credentials.html	"Step 3: Create a credential configuration in Databricks
Step 3: Create a credential configuration in Databricks
When you have created the IAM role, you can tell Databricks about it by creating a credential configuration that uses that role’s ID.  
To create a credential configuration:  
In the account console, click Cloud resources.  
Click Credential configuration.  
Click Add credential configuration.  
In the Credential configuration name field, enter a human-readable name for your new credential configuration.  
In the Role ARN field, enter your role’s ARN.  
Click Add.  
Validation is not run during credential configuration creation. Some errors are detected only when you use the configuration to create a new workspace. These errors can include an invalid ARN or incorrect permissions for the role, among others.

Delete a credential configuration
Delete a credential configuration
Credential configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the credential configuration:  
In the account console, click Cloud resources.  
Click Credential configuration.  
On the credential configuration row, click the Actions menu icon, and select Delete.  
You can also click the credential configuration name and click Delete on the pop-up dialog.  
On the confirmation dialog, click Confirm Delete."
22707	https://docs.databricks.com/en/admin/account-settings-e2/networks.html	"Create network configurations for custom VPC deployment  
By default, Databricks creates a VPC in your AWS account for each workspace and creates new Databricks Runtime clusters in those workspaces. You have the option to create workspaces in your own VPC, known as a customer-managed VPC. This article describes how to use the account console to create and manage network configurations for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see Create a workspace using the Account API.  
Using your own VPC enables you to configure it according to your organization’s enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.  
The following related sections discuss updating existing network and configuration objects:  
Update a running or failed workspace.  
Updates of existing PrivateLink configuration objects.  
Create a network configuration
Create a network configuration
To create a network configuration for a customer-managed VPC, you must create the VPC and subnets to meet Databricks requirements and then reference that VPC—including network objects such as VPCs, subnets, and security groups—in a network configuration for your Databricks account.  
Note  
These instructions show you how to create the network configuration from the Cloud resources page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See Manually create a workspace (existing Databricks accounts).  
Set up your VPC, subnets, and security groups, using the instructions in Configure a customer-managed VPC.  
Copy the IDs for each of these objects for use in the next step.  
Important  
If you plan to share a VPC and subnets across multiple workspaces, be sure to size your VPC and subnets to be large enough to scale with usage. You cannot reuse a network configuration object across workspaces.  
In the account console, click Cloud resources.  
Click Network.  
From the vertical navigation on the page, click Network configurations.  
Click Add network configuration.  
In the Network configuration name field, enter a human-readable name for your new network configuration.  
In the VPC ID field, enter the VPC ID.  
In the Subnet IDs field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see Configure a customer-managed VPC.  
In the Security Group IDs field, enter the ID for at least one AWS security group. For network configuration requirements, see Configure a customer-managed VPC.  
(Optional) To support AWS PrivateLink back-end connectivity, you must select two VPC endpoint registrations from the fields under the Back-end private connectivity heading.  
If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See Step 2: Create VPC endpoints. You can use the AWS Console or various automation tools.  
For each field, either choose existing VPC endpoint registrations, or choose Register a new VPC endpoint to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see Manage VPC endpoint registrations.  
Click Add.

View network configurations and any validation errors
View network configurations and any validation errors
In the account console, click Cloud resources.  
Click Network.  
All network configurations are listed, with VPC ID, VPC Status, and Created date displayed for each.  
Click the network configuration name to view more details, including subnet IDs, security group IDs.  
If there are network validation error messages, they will be shown here.  
Important  
Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.

Delete a network configuration
Delete a network configuration
Network configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:  
In the account console, click Cloud resources.  
Click Network.  
On the row for the configuration, click the kebab menu on the right, and select Delete.  
In the confirmation dialog, click Confirm Delete."
22708	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/azure-ad.html	"SSO to Databricks with Microsoft Entra ID(Azure Active Directory)  
This article shows how to configure Microsoft Entra ID (formerly Azure Active Directory) as the identity provider for single sign-on (SSO) in your Databricks account. Microsoft Entra ID supports both OpenID Connect (OIDC) and SAML 2.0. To sync users and groups from Microsoft Entra ID, see Sync users and groups from your identity provider.  
Warning  
To prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Configure emergency access.  
Enable account single sign-on authentication using OIDC
Enable account single sign-on authentication using OIDC
As an account owner or account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
From the drop-down at the top of this tab, select OpenID Connect.  
On the Single sign-on tab, make note of the Databricks Redirect URI value.  
In another browser tab, create a Microsoft Entra ID application:  
Log in to Azure portal as an administrator.  
In the Azure services pane, click Microsoft Entra ID, in the left plan, click App registrations.  
Click New registration.  
Enter a name.  
Under Supported account types choose: Accounts in this organizational directory only.  
Under Redirect URI, choose web and paste the Databricks Redirect URI value.  
Click Register.  
Gather the required information from the Microsoft Entra ID application:  
Under Essentials, copy the Application (client) ID.  
Click Endpoints.  
Copy the URL under OpenID Connect metadata document  
In the left pane, click Certificates & secrets.  
Click + New client secret.  
Enter a description and choose an expiration.  
Click Add.  
Copy the secret value.  
Return to the Databricks account console Single sign-on tab and enter values you copied from the identity provider application to the Client ID, Client secret, and OpenID issuer URL fields. Remove the /.well-known/openid-configuration ending from the URL.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO.

Enable account single sign-on authentication using SAML"
22709	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/azure-ad.html	"Enable account single sign-on authentication using SAML
Follow these steps to create a non-gallery Azure portal SAML application for use with Databricks account console.  
To get the Databricks SAML URL as an account owner or account admin, log in to the account console. Click Settings in the sidebar and click the Single sign-on tab. From the picker, select SAML 2.0. Copy the value in the Databricks Redirect URI field.  
In another browser tab, create a Microsoft Entra ID application:  
Log in to Azure portal as an administrator.  
In the Azure services pane, click Microsoft Entra ID, in the left plan, click Enterprise applications. The All applications pane opens and displays a random sample of the applications in your Microsoft Entra ID tenant.  
Click New application.  
Click Create your own application.  
Enter a name.  
Under What are you looking to do with your application? choose Integrate any other application you don’t find in the gallery.  
Configure the Microsoft Entra ID application:  
Click Properties.  
Set Assignment required to No. Databricks recommends this option, which allows all users to sign in to the Databricks account. Users must have access to this SAML application to log into your Databricks account using SSO.  
In the application’s properties pane, click Set up single sign on.  
Click SAML configure the application for SAML authentication. The SAML properties pane appears.  
Next to Basic SAML configuration, click Edit.  
Set Entity ID to the Databricks SAML URL you got from the Databricks SSO configuration page.  
Set Reply URL to the Databricks SAML URL you got from the Databricks SSO configuration page.  
Next to SAML Signing Certificate, click Edit.  
In the Signing Option drop-down list, select Sign SAML response and assertion.  
In Attributes & Claims, click Edit.  
Set the Unique User Identifier (Name ID) field to user.mail.  
Under SAML Certificates, next to Certificate (Base64), click Download. The certificate is downloaded locally as a file with the .cer extension.  
Open the .cer file in a text editor and copy the file contents. The file is the entire x.509 certificate for the Microsoft Entra ID SAML application.  
Important  
Do not open it using the macOS keychain, which is the default application for that file type in macOS.  
The certificate is sensitive data. Use caution about where to download it. Delete it from local storage as soon as possible.  
In the Azure portal, under Set up Microsoft Entra ID SAML Toolkit, copy and save the Login URL and Microsoft Entra ID Identifier.  
Configure Databricks in the Databricks account console SSO page. See Enable account single sign-on authentication using SAML for details on optional fields.  
Click Single sign-on.  
Set the SSO type drop-down to SAML 2.0.  
Set Single Sign-On URL to the Microsoft Entra ID field that was called Login URL.  
Set Identity Provider Entity ID to the Microsoft Entra ID field that was called Microsoft Entra ID Identifier.  
Set x.509 Certificate to the Microsoft Entra ID x.509 certificate, including the markers for the beginning and ending of the certificate.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO."
22710	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/index.html	"SSO in your Databricks account console  
This article shows how to configure single sign-on (SSO) to authenticate to the account console and Databricks workspaces using your organization’s identity provider. To sync users and groups from your identity provider, see Sync users and groups from your identity provider.  
Overview of SSO setup
Overview of SSO setup
SSO supports using either SAML 2.0 or OpenID Connect (OIDC). Your identity provider (IdP) must support at least one of these protocols.  
Once you have enabled SSO in the account console, you can choose to enable unified login. Unified login allows you to manage one SSO configuration in your account that is used for the account and Databricks workspaces. If your account was created after June 21, 2023, unified login is enabled on your account by default for all workspaces, new and existing, and it cannot be disabled. For more information, see Enable unified login.  
In workspaces where unified login is disabled, workspace-level SSO needs to be configured separately. For more information, see Set up SSO for your workspace.  
You can read the instructions on how to configure SSO to the following identity providers:  
Microsoft Entra ID (formerly Azure Active Directory)  
Okta  
One Login  
The process is similar for any identity provider that supports OIDC or SAML 2.0. If your identity provider is not listed above, follow the instructions for OIDC or SAML.  
Account sign-in process  
When account-level SSO is enabled, the sign-in behavior is as follows:  
All users, including admins, must sign in to the Databricks account and unified-login enabled workspaces using single sign-on. Users who have been selected for emergency access can use a username and password and a security key to log in.  
If your account was created before June 21, 2023, the account owner can also login to the Databricks account using their username and password.  
Account admins can use their username and password to make account-level REST API calls.  
All users can use their username and password to make workspace-level REST API calls. Basic authentication is legacy and not recommended in production.  
To learn about the workspace sign-in process when SSO is enabled, see Workspace sign-in process.

Enable unified login"
22711	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/index.html	"Enable unified login
Preview  
Unified login is currently in Public Preview for accounts created before June 21, 2023. Unified login is generally available for accounts created after June 21, 2023.  
Unified login allows you to manage one SSO configuration in your account that is used for the account and Databricks workspaces. When SSO is enabled on your account, you can choose to enabled unified login for all workspaces or for selected workspaces. Unified login workspaces use the account-level SSO configuration, and all users, including account and workspace admins, must sign in to Databricks using SSO. You cannot manage SSO separately on the workspace-level in a unified login enabled workspace. Databricks recommends that you configure unified login for all workspaces.  
If your account was created after June 21, 2023, unified login is enabled on your account by default for all workspaces, new and existing, and it cannot be disabled.  
You can enable unified login using the account console or the workspace admin settings page.  
Enable unified login using the account console  
SSO must be enabled in the account to enable unified login.  
As an account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
In Unified login, click Get started.  
Ensure that all workspaces users have access to the identity provider in your account-level SSO configuration. Click Confirm.  
Choose to apply unified login to All workspaces or Selected workspaces. Databricks recommends that you enable unified login for all workspaces. Additional steps are required to enable unified login on workspaces that configure user-to-workspace private connectivity. See Step 5: (Optional) Configure front-end PrivateLink with unified login.  
If you choose Selected workspaces, choose to apply settings to newly created workspaces and select the existing workspaces to apply the settings to.  
Click Save  
Enable unified login using the workspace admin settings page  
If unified login is enabled on selected workspaces by an account admin, a workspace admin can enable unified login on their workspace. If unified login is enabled on all workspaces, the single sign on configuration is not available on the workspace-level.  
To enable unified login on workspaces that configure user to workspace private connectivity you must configure additional steps. See Step 5: (Optional) Configure front-end PrivateLink with unified login.  
As a workspace admin, log in to the Databricks workspace.  
Click your username in the top bar of the Databricks workspace and select Settings.  
Click on the Identity and access tab.  
Next to SSO settings, click Manage.  
Next to Unified login click Enable.  
Upgrade to unified login  
If you are enabling unified login on an existing workspace with workspace-level SSO configured, do the following:  
Configure single sign on on your account.  
Ensure the users in your workspace have access to the account-level SSO application in your identity provider.  
Granting users access to the account-level SSO application does not grant them any additional access in Databricks. All Databricks workspace users are automatically users in the Databricks account. See How do admins assign users to the account?.  
Configure unified login on the workspace following Enable unified login.  
Test SSO on the workspace by having a workspace user sign in.  
Decommission the workspace-level SSO application in your identity provider.

Configure emergency access
Configure emergency access
Preview  
This feature is in Private Preview. To join this preview, contact your Databricks account team.  
To prevent lockouts, account admins can set up emergency access for up to ten users. These users can sign into Databricks using multi-factor authentication with FIDO 2 security keys, which may be hardware-based, like a physical security key, or software-based, like a mobile authenticator app.  
Databricks recommends configuring a strong password and at least one FIDO 2 security key for signing in with emergency access.  
To configure emergency access:  
As an account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
In Emergency access, choose up to ten users that can sign in using emergency access. These users must register security keys.  
Click Save.  
It might take up to two minutes for the users to see the security key management page.  
Register a security key for emergency access  
A security key can be hardware-based, like a physical security key, or software-based, like a mobile authenticator app. To register a security key:  
As a user with emergency access, log in to the account console.  
Click the down arrow next to your username in the upper right corner.  
Click User preferences.  
In Authentication, click Add key.  
You need to configure a password in order to add a key. If you have not configured a password already, you will see a prompt to reset your password first.  
Click Set up and follow the browser prompts to configure your key.  
After you configure your key, you will see a Databricks notification that the security key was added successfully.  
Login to Databricks using a security key  
To login using emergency access and a security key:  
As a user with emergency access, go to the account console.  
Click Sign in with Databricks credentials.  
Enter your username and and password. Click Continue.  
Follow the browser prompt to use your security key."
22712	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/oidc.html	"Configure account SSO using OIDC  
This article hows how to generally configure single sign-on (SSO) to authenticate to the account console and Databricks workspaces using OIDC. You can also read the specific instructions on how to configure SSO with OIDC to the following identity providers:  
Microsoft Entra ID (formerly Azure Active Directory)  
Okta  
One Login  
For an overview of single sign-on in the account, see SSO in your Databricks account console.  
Enable account single sign-on authentication using OIDC
Enable account single sign-on authentication using OIDC
Warning  
To prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Configure emergency access.  
As an account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
From the drop-down at the top of this tab, select OpenID Connect.  
Copy the value in the Databricks Redirect URI field.  
Go to your identity provider and create a new client application (web), entering the Databricks Redirect URI value in the appropriate field in the identity provider configuration interface.  
Your identity provider should have documentation to guide you through this process.  
Copy the client ID, client secret, and OpenID issuer URL generated by the identity provider for the application.  
Client ID is the unique identifier for the Databricks application you created in your identity provider. This is sometimes referred to as the Application ID.  
Client secret is a secret or password generated for the Databricks application that you created. It is used to authorize Databricks with your identity provider.  
OpenID issuer URL is the URL at which your identity-provider’s OpenID Configuration Document can be found. That OpenID Configuration Document must found be in {issuer-url}/.well-known/openid-configuration.  
Return to the Databricks account console Single sign-on tab and enter values you copied from the identity provider application to the Client ID, Client secret, and OpenID issuer URL fields.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO.  
Grant all account users access to the Databricks application in your identity provider. You might need to modify the access permissions for the application.

Configure unified login
Configure unified login
Once you have enabled SSO in the account console, Databricks recommends enabling unified login. Unified login allows you to use the account console SSO configuration in your Databricks workspaces. If your account was created after June 21, 2023, unified login is enabled on your account by default for all workspaces, new and existing, and it cannot be disabled. To configure unified login, see Enable unified login."
22713	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/okta.html	"SSO to Databricks with Okta  
This article shows how to configure Okta as the identity provider for single sign-on (SSO) in your Databricks account. Okta supports both OpenID Connect (OIDC) and SAML 2.0. To sync users and groups from Okta, see Sync users and groups from your identity provider.  
Warning  
To prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Configure emergency access.  
Enable account single sign-on authentication using OIDC
Enable account single sign-on authentication using OIDC
As an account owner or account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
From the drop-down at the top of this tab, select OpenID Connect.  
On the Single sign-on tab, make note of the Databricks Redirect URI value.  
In a new browser tab, log into Okta as an administrator.  
In the home page, click Applications > Applications.  
Click Create App Integration.  
Select OIDC - OpenID Connect and Web Application and click Next.  
In New Web App Integration, under Sign-in redirect URIs, enter the Databricks Redirect URI from step 4. You can choose to configure the other settings or you can leave them to their default values.  
Click Save  
Under the General tab, copy the client ID and client secret generated by Okta for the application.  
Client ID is the unique identifier for the Databricks application you created in your identity provider.  
Client secret is a secret or password generated for the Databricks application that you created. It is used to authorize Databricks with your identity provider.  
Under the Sign On tab, in OpenID Connect ID Token copy the Okta URL in the issuer field.  
If the issuer field says Dynamic, click Edit and choose Okta URL (url) in the drop down.  
This URL is the URL at which Okta’s OpenID Configuration Document can be found. That OpenID Configuration Document must found be in {issuer-url}/.well-known/openid-configuration.  
Click the Assignments tab. Databricks recommends adding the Okta group named Everyone to the application. This ensures all users in your organization can access the Databricks account.  
Return to the Databricks account console Single sign-on tab and enter values you copied from the identity provider application to the Client ID, Client secret, and OpenID issuer URL fields.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO.

Enable account single sign-on authentication using SAML
Enable account single sign-on authentication using SAML
Follow these steps to create an Okta SAML application for use with Databricks account console.  
To get the Databricks SAML URL, as an account owner or account admin, log in to the account console. Click Settings in the sidebar and click the Single sign-on tab. From the picker, select SAML 2.0. Copy the value in the Databricks Redirect URI field.  
In a new browser tab, log into Okta as an administrator.  
In the home page, click Applications > Applications.  
Click Create App Integration.  
Select SAML 2.0 and click Next.  
Set App name to Databricks SSO and click Next.  
Configure the application using the following settings:  
Single Sign On URL: the Databricks SAML URL from Gather required information  
Audience URI: the Databricks SAML URL from Gather required information  
Name ID Format: EmailAddress  
Application Username: Email  
Click Advanced settings. Ensure that Response is set to Signed (the default). Signing the assertion is optional.  
Important  
Do not modify other advanced settings. For example, assertion encryption must be set to Unencrypted.  
Click Hide advanced settings.  
Click Next.  
Select I’m an Okta customer adding an internal app.  
Click Finish. The Databricks SAML app is shown.  
Under SAML 2.0 is not configured until you complete the setup instructions, click View Setup Instructions.  
Copy the following values:  
Identity Provider Single Sign-On URL  
Identity Provider Issuer  
x.509 certificate  
Click the Assignments tab. Databricks recommends adding the Okta group named Everyone to the application. This ensures all users in your organization can access the Databricks account.  
Configure Databricks in the Databricks account console SSO page. See Enable account single sign-on authentication using SAML for details on optional fields.  
Click Single sign-on.  
Set the SSO type drop-down to SAML 2.0.  
Set Single Sign-On URL to the Okta field called Login URL.  
Set Identity Provider Entity ID to the Okta field that was called Identity Provider Issuer.  
Set x.509 Certificate to the Okta x.509 certificate, including the markers for the beginning and ending of the certificate.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO."
22714	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/one-login.html	"SSO to Databricks with OneLogin  
This article shows how to configure OneLogin as the identity provider for single sign-on (SSO) in your Databricks account. OneLogin supports both OpenID Connect (OIDC) and SAML 2.0. To sync users and groups from OneLogin, see Sync users and groups from your identity provider.  
Warning  
To prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Configure emergency access.  
Enable account single sign-on authentication using OIDC
Enable account single sign-on authentication using OIDC
As an account owner or account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
From the drop-down at the top of this tab, select OpenID Connect.  
On the Single sign-on tab, make note of the Databricks Redirect URI value.  
In a new browser tab, log in to OneLogin.  
Click Administration.  
Click Applications.  
Click Add App.  
Search for OpenId Connect and select the OpenId Connect (OIDC) app.  
Enter a name and click Save.  
In the Configuration tab, Databricks Redirect URI from step 4. You can choose to configure the other settings or you can leave them to their default values.  
In the SSO tab, copy the copy the client ID, client secret, and issuer URL values.  
Client ID is the unique identifier for the Databricks application you created in OneLogin.  
Client secret is a secret or password generated for the Databricks application that you created. It is used to authorize Databricks with your identity provider.  
OpenID issuer URL is the URL at which OneLogin’s OpenID Configuration Document can be found. That OpenID Configuration Document must found be in {issuer-url}/.well-known/openid-configuration.  
Return to the Databricks account console Single sign-on tab and enter values you copied from the identity provider application to the Client ID, Client secret, and OpenID issuer URL fields.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO.

Enable account single sign-on authentication using SAML
Enable account single sign-on authentication using SAML
Follow these steps to create a OneLogin SAML application for use with Databricks account console.  
To get the Databricks SAML URL, as an account owner or account admin, log in to the account console. Click Settings in the sidebar and click the Single sign-on tab. From the picker, select SAML 2.0. Copy the value in the Databricks Redirect URI field.  
In a new browser tab, log in to OneLogin.  
Click Administration.  
Click Applications.  
Click Add App.  
Search for SAML Custom Connector (Advanced) and click the result by OneLogin, Inc.  
Set Display Name to Databricks.  
Click Save. The application’s Info tab loads.  
Click Configuration.  
In Gather required information, set each of the following fields to the Databricks SAML URL:  
Audience  
Recipient  
ACS (Consumer) URL Validator  
ACS (Consumer) URL  
Single Logout URL  
Login URL  
Set SAML signature element to Both.  
Click Parameters.  
Set Credentials are to Configured by admins and shared by all users.  
Click Email. Set the value to email and enable Include in SAML Assertion.  
Click the SSO tab.  
Copy the following values:  
x.509 certificate  
Issuer URL  
SAML 2.0 endpoint (HTTP)  
Verify that SAML signature element is set to Response or Both.  
Verify that Encrypt assertion is disabled.  
Configure Databricks in the Databricks account console SSO page. See Enable account single sign-on authentication using SAML for details on optional fields.  
Click Single sign-on.  
Set the SSO type drop-down to SAML 2.0.  
Set Single Sign-On URL to the OneLogin SAML 2.0 endpoint.  
Set Identity Provider Entity ID to the OneLogin Issuer URL.  
Set x.509 Certificate to the OneLogin x.509 certificate, including the markers for the beginning and ending of the certificate.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO."
22715	https://docs.databricks.com/en/admin/account-settings-e2/single-sign-on/saml.html	"Configure account SSO using SAML  
This article hows how to generally configure single sign-on (SSO) to authenticate to the account console and Databricks workspaces using SAML. You can also read the specific instructions on how to configure SSO with OIDC to the following identity providers:  
Microsoft Entra ID (formerly Azure Active Directory)  
Okta  
One Login  
For an overview of single sign-on in the account, see SSO in your Databricks account console.  
Enable account single sign-on authentication using SAML
Enable account single sign-on authentication using SAML
The following instructions describe how to use SAML 2.0 to authenticate account console users.  
Warning  
To prevent getting locked out of Databricks during single sign-on testing, Databricks recommends keeping the account console open in a different browser window. You can also configure emergency access with security keys to prevent lock out. See Configure emergency access.  
View the account console SSO page and copy the SAML URL:  
As an account admin, log in to the account console and click the Settings icon in the sidebar.  
Click the Single sign-on tab.  
From the drop-down at the top of this tab, select SAML 2.0.  
Copy the value in the Databricks Redirect URI field. You will need the Databricks SAML URL for a later step.  
In another browser window or tab, create a Databricks application in your identity provider:  
Go to your identity provider (IdP).  
Create a new client application (web):  
Use your identity provider’s documentation as needed.  
For the SAML URL field (which might be called a redirect URL), use the Databricks SAML URL that you copied from the Databricks page.  
Copy the following objects and fields from your new Databricks application:  
The x.509 certificate: A digital certificate provided by your Identity Provider for securing communications between Databricks and the Identity Provider  
The single-sign-on (SSO) URL for your identity provider. This is the URL that initiates single sign-on with your identity provider. This is also sometimes referred to as the SAML endpoint.  
The identity provider issuer: This is the unique identifier for your SAML identity provider. This is sometimes referred to as the Entity ID or Issuer URL.  
Set your Databricks account to use your identity provider:  
Return to the browser tab or window with the the Databricks account console SSO page.  
Type or paste the following fields from your identity provider’s Databricks application: the single sign-on URL, the identity provider entity ID, and the x.509 Certificate.  
Click Save.  
Click Test SSO to validate that your SSO configuration is working properly.  
Click Enable SSO to enable single sign-on for your account.  
Test account console login with SSO.  
Grant all account users access to the Databricks application in your identity provider. You might need to modify the access permissions for the application.

Configure unified login
Configure unified login
Once you have enabled SSO in the account console, Databricks recommends enabling unified login. Unified login allows you to use the account console SSO configuration in your Databricks workspaces. If your account was created after June 21, 2023, unified login is enabled on your account by default for all workspaces, new and existing, and it cannot be disabled. To configure unified login, see Enable unified login."
22716	https://docs.databricks.com/en/admin/account-settings-e2/storage.html	"Create an S3 bucket for workspace deployment  
Note  
These instructions apply to accounts created before November 8, 2023. If your Databricks account was created after November 8, 2023, see Manually create a workspace (new Databricks accounts).  
This article describes how to create and configure root storage for a custom Databricks workspace deployment. You can also automate this step and the entire workspace creation by using the AWS Quick Start template or the Databricks Terraform provider to deploy your workspace.  
Requirements
Requirements
You must be a Databricks account admin.

Best practices for root storage creation
Best practices for root storage creation
The bucket you use for workspace deployment is referred to as your workspace’s root storage. Do not use your root storage to store production customer data. Instead, create additional S3 buckets or other data sources for production data and optionally create DBFS mount points for them.  
Additionally, before you create your S3 bucket, review the following best practices:  
The S3 bucket must be in the same AWS region as the Databricks workspace deployment.  
Databricks recommends that you use an S3 bucket that is dedicated to Databricks, unshared with other resources or services.  
Do not reuse a bucket from legacy Databricks workspaces. For example, if you are migrating to E2, create a new AWS bucket for your E2 setup.

Step 1: Create a storage configuration and generate a bucket policy
Step 1: Create a storage configuration and generate a bucket policy
In to the account console, click Cloud resources.  
Click Storage configuration.  
Click Add storage configuration.  
In the Storage configuration name field, enter a human-readable name for your new storage configuration.  
In the Bucket Name field, enter the name of the S3 bucket that you will create.  
Important  
The bucket name cannot include dot notation (.). It must be globally unique and cannot include spaces or uppercase letters. For more bucket naming guidance, see the AWS bucket naming rules.  
Click Generate Policy and copy the policy that is generated. You add this policy to your S3 bucket configuration in AWS in the next step.  
Click Add.

Step 2: Create the S3 bucket
Step 2: Create the S3 bucket
Log into your AWS Console as a user with administrator privileges and go to the S3 service.  
Click the Create bucket button.  
In Bucket name, enter the name for the bucket that you created in Step 1.  
Select the same AWS region that you will use for your Databricks workspace deployment.  
Click Create bucket.  
Click the Permissions tab.  
In the Bucket policy section, click Edit.  
Paste the bucket policy that you generated and copied from Databricks.  
Save the bucket.  
Enable object-level event logging (recommended)  
Databricks strongly recommends that you enable S3 object-level logging for your root storage bucket. This enables faster investigation of any issues that may come up. Be aware that S3 object-level logging can increase AWS usage costs.  
For instructions, see the AWS documentation on CloudTrail event logging for S3 buckets and objects.  
Resolve validation failures  
Bucket policy permissions can take a few minutes to propagate. Retry this procedure if validation fails due to permissions.  
Verify correct permissions  
When creating a storage configuration for your bucket, Databricks checks whether your bucket has been set up with correct permissions. One of these checks writes a file in your bucket and immediately deletes it. However, if the delete operation fails, the temporary object remains at the root of your bucket. The object name begins with databricks-verification-<uuid>.  
If you see this object, it is likely because of a misconfiguration in the bucket policy. Databricks has PUT permissions but not DELETE permissions. Review the bucket policy and verify that the permissions are configured correctly.

Delete a storage configuration
Delete a storage configuration
Storage configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the storage configuration:  
In the account console, click Cloud resources.  
Click Storage configuration.  
On the storage configuration row, click the Actions menu icon, and select Delete.  
You can also click the storage configuration name and click Delete on the pop-up dialog.  
In the confirmation dialog, click Confirm Delete.

Encrypt your root S3 bucket using customer-managed keys (optional)
Encrypt your root S3 bucket using customer-managed keys (optional)
You can encrypt your root S3 bucket using customer-managed keys, which requires using the Account API.  
You can either add an encryption key when you create a new workspace using the Account API or add the key later. For more information, see Step 5: Configure customer-managed keys (optional) and Customer-managed keys for encryption."
22717	https://docs.databricks.com/en/admin/account-settings/account.html	"Manage your subscription  
This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.  
Manage billing
Manage billing
The way you manage billing depends on the type of business relationship you have with Databricks.  
Pay-as-you-go accounts through AWS Marketplace  
If you subscribed to Databricks using AWS Marketplace, your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.  
Pay-as-you-go accounts paid by credit card to Databricks  
If you pay Databricks directly by credit card, follow these instructions to manage billing:  
As the Databricks account owner or an account-level administrator, log in to the Databricks account console.  
Click the Settings icon in the sidebar.  
If no payment method has been added, click Add billing information and fill in the form.  
To edit your existing payment method, click Edit.  
Contract accounts  
If you have a contract with Databricks, Databricks has your billing information on file.

Upgrade your subscription
Upgrade your subscription
Databricks has multiple pricing plans. By default, new accounts are on the Premium plan, which adds audit logging, role-based access control, and other features that give you more control over security, governance, and more. If you don’t know your current plan, account admins can view it in the account console under Subscription and Billing.  
If your current plan is no longer meeting all of your organization’s needs, you can upgrade to the Enterprise plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.  
Note  
Access control settings are disabled by default on workspaces that are upgraded from the Standard plan to the Premium plan or above. Once an access control setting is enabled, it can not be disabled. For more information, see Access controls lists can be enabled on upgraded workspaces.  
Upgrade a pay-as-you-go account  
If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.  
Note  
To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the Databricks Free Trial or contact a Databricks representative. For more information, see Start a Databricks free trial on AWS.  
As a Databricks account administrator, log in to the Databricks account console.  
Click the Settings icon in the sidebar.  
Your current plan is indicated on the page.  
To upgrade to a new plan, click Change plan.  
To select a plan or to see details for a plan, click the tile for that plan.  
Click Save.  
Request a downgrade  
You cannot downgrade your plan from Enterprise to Premium directly in the account console. To request a downgrade:  
Click Change plan.  
Click the Premium tile.  
Click Contact us. A Databricks representative will contact you.  
For more information, see the plans and pricing page.  
Contract accounts  
If you have a contract with Databricks, contact your Databricks account team to manage your subscription level.

Cancel your Databricks subscription
Cancel your Databricks subscription
After you cancel your subscription:  
You can no longer access workspaces, notebooks, or data in your Databricks account.  
In accordance with the Databricks terms of service, any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.  
You can’t sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.  
Note  
Once a subscription is cancelled, Databricks is not responsible for cleaning up the resources attached to the account. Databricks recommends terminating all compute resources before you cancel your subscription. Additionally, terminate any Databricks associated resources from your AWS console.  
Cancel a pay-as-you-go account through AWS Marketplace  
To cancel your subscription:  
As the Databricks account owner or an account administrator, log in to the Databricks account console.  
Go to each workspace and export any data or notebooks you want to import into a different account.  
Delete all workspaces.  
Log in to AWS as a user with the aws-marketplace:Unsubscribe permission in their IAM role.  
Go to AWS Marketplace.  
From the account name at the top right, select Your Marketplace software.  
Click Databricks Data Intelligence Platform.  
From Actions, click Cancel subscription.  
Cancel a pay-as-you-go account through Databricks  
To cancel your subscription:  
As the Databricks account owner or an account-level administrator, log in to the account console.  
Go to each workspace and export any data or notebooks you want to import into a different account.  
Delete all workspaces.  
Click the Settings icon in the sidebar.  
Click Cancel plan.  
In the confirmation dialog, click Confirm delete.  
Contract accounts  
If you have a contract with Databricks, contact your Databricks account team to manage your subscription.

Delete your Databricks account
Delete your Databricks account
When you delete an account, the account owner and all history associated with the management of your organization’s account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and delete all Unity Catalog metastores in the account. After you delete all metastores associated with your organization’s account, you can start the process to delete your account.  
If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at help.databricks.com."
22718	https://docs.databricks.com/en/admin/account-settings/audit-log-delivery.html	"Configure audit log delivery  
This article explains how to configure low-latency delivery of audit logs in JSON file format to an Amazon S3 storage bucket.  
When your audit logs gets delivered to an S3 storage bucket, you can make the data available for usage analysis. Databricks delivers a separate JSON file for each workspace in your account and a separate file for account-level events. For more information on the file schema and audit events, see Audit log reference.  
You can optionally deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery. This allows flexibility, for example setting up workspaces from multiple AWS accounts to deliver to the same S3 bucket. This option requires that you configure an S3 bucket policy that references a cross-account IAM role. Instructions and a policy template are provided for you in Step 3: cross-account support.  
In addition to delivery of logs for running workspaces, logs are delivered for cancelled workspaces to ensure that logs are properly delivered that represent the final day of the workspace.  
Requirements
Requirements
To configure audit log delivery, you must:  
Be an account admin.  
Authenticate to the APIs so you can set up delivery with the Account API. See How to authenticate to the Account API.

High-level flow
High-level flow
This section describes the high-level flow of audit log delivery.  
Step 1: Configure storage: In AWS, create a new S3 bucket. Using Databricks APIs, call the Account API to create a storage configuration object that uses the bucket name.  
Step 2: Configure credentials: In AWS, create the appropriate AWS IAM role. Using Databricks APIs, call the Account API to create a credentials configuration object that uses the IAM role’s ARN.  
(Optional) Step 3: cross-account support: To deliver logs to an AWS account other than the account of the IAM role that you create for log delivery, add an S3 bucket policy. This policy references IDs for the cross-account IAM role that you created in the previous step.  
Step 4: Call the log delivery API: Call the Account API to create a log delivery configuration that uses the credential and storage configuration objects from previous steps.  
After you complete these steps, you can access the JSON files. The delivery location is in the following format:  
<bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs_<internal-id>.json  
Note  
If you configure audit log delivery for the entire account, account-level audit events that are not associated with any single workspace are delivered to the workspaceId=0 partition.

Considerations based on your number of workspaces
Considerations based on your number of workspaces
Your delivery configuration might vary depending on how many workspaces you have and where they are located:  
If you have one workspace in your Databricks account: Follow the instructions as outlined in the high-level flow, creating a single configuration object for your workspace.  
If you have multiple workspaces in the same Databricks account: Do one of the following:  
Share the same configuration (log delivery S3 bucket and IAM role) for all workspaces in the account. This is the only configuration option that also delivers account-level audit logs. It is the default option.  
Use separate configurations for each workspace in the account.  
Use separate configurations for different groups of workspaces, each sharing a configuration.  
If you have multiple workspaces, each associated with a separate Databricks account: Create unique storage and credential configuration objects for each account. You can reuse an S3 bucket or IAM role between these configuration objects.  
Note  
You can configure log delivery with the Account API even if the workspace wasn’t created using the Account API.

How to authenticate to the Account API"
22719	https://docs.databricks.com/en/admin/account-settings/audit-log-delivery.html	"How to authenticate to the Account API
To authenticate to the Account API, you can use Databricks OAuth for service principals, Databricks OAuth for users, or a Databricks account admin’s username and password. Databricks strongly recommends that you use Databricks OAuth for users or service principals. A service principal is an identity that you create in Databricks for use with automated tools, jobs, and applications. See OAuth machine-to-machine (M2M) authentication.  
Use the following examples to authenticate to a Databricks account. You can use OAuth for service principals, OAuth for users, or a user’s username and password (legacy). For background, see:  
For OAuth for service principals, see OAuth machine-to-machine (M2M) authentication.  
For OAuth for users, see OAuth user-to-machine (U2M) authentication.  
For a user’s username and password (legacy), see Basic authentication (legacy).  
For authentication examples, choose from the following:  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Complete the steps to configure OAuth M2M authentication for service principals in the account. See OAuth machine-to-machine (M2M) authentication.  
Identify or manually create a Databricks configuration profile in your .databrickscfg file, with the profile’s fields set correctly for the related host, account_id, and client_id and client_secret mapping to the service principal. See OAuth machine-to-machine (M2M) authentication.  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p MY-AWS-ACCOUNT  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Complete the steps to configure OAuth U2M authentication for users in the account. See OAuth user-to-machine (U2M) authentication.  
Start the user authentication process by running the following Databricks CLI command:  
databricks auth login --host <account-console-url> --account-id <account-id>  
For example:  
databricks auth login --host https://accounts.cloud.databricks.com --account-id 00000000-0000-0000-0000-000000000000  
Note  
If you have an existing Databricks configuration profile with the host and account_id fields already set, you can substitute --host <account-console-url> --account-id <account-id> with --profile <profile-name>.  
Follow the on-screen instructions to have the Databricks CLI automatically create the related Databricks configuration profile in your .databrickscfg file.  
Continue following the on-screen instructions to sign in to your Databricks account through your web browser.  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p ACCOUNT-00000000-0000-0000-0000-000000000000  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Identify or manually create a Databricks configuration profile in your .databrickscfg file, with the profile’s fields set correctly for the related host, account_id, and username and password mapping to your Databricks user account. See Basic authentication (legacy).  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p MY-AWS-ACCOUNT  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.

Audit delivery details"
22720	https://docs.databricks.com/en/admin/account-settings/audit-log-delivery.html	"Audit delivery details
After logging is enabled for your account, Databricks automatically sends audit logs in human-readable format to your delivery location on a periodic basis.  
Latency: After initial setup or other configuration changes, expect some delay before your changes take effect. For initial setup of audit log delivery, it takes up to one hour for log delivery to begin. After log delivery begins, auditable events are typically logged within 15 minutes. Additional configuration changes typically take an hour to take effect.  
Encryption: Databricks encrypts audit logs using Amazon S3 server-side encryption.  
Format: Databricks delivers audit logs in JSON format.  
Location: The delivery location is <bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs_<internal-id>.json. New JSON files are delivered every few minutes, potentially overwriting existing files. The delivery path is defined as part of the configuration. Account-level audit events that are not associated with any single workspace are delivered to the workspaceId=0 partition, if you configured audit logs delivery for the entire account.  
Databricks can overwrite the delivered log files in your bucket at any time. If a file is overwritten, the existing content remains, but there might be additional lines for more auditable events.  
Overwriting ensures exactly-once semantics without requiring read or delete access to your account.

Use the log delivery APIs
Use the log delivery APIs
The log delivery APIs have the following additional features:  
Get all log delivery configurations.  
Get a log delivery configuration by ID.  
Enable or disable a log delivery configuration by ID.  
Log delivery configuration status can be found in the API response’s log_delivery_status object. With log_delivery_status, you can check the status (success or failure) and the last time of an attempt or successful delivery.

Audit log delivery limitations
Audit log delivery limitations
There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create and enable two workspace level delivery configurations per workspace for each log type, meaning the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type.  
You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described."
22721	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Audit log reference  
Note  
This feature requires the Premium plan or above.  
This article provides you with a comprehensive reference of available audit log services and events. By understanding which events are logged in the audit logs, your enterprise can monitor detailed Databricks usage patterns in your account.  
The easiest way to access and query your account’s audit logs is by using system tables (Public Preview).  
If you’d like to configure a regular log delivery, see Configure audit log delivery.  
Audit log services
Audit log services
The following services and their events are logged by default in audit logs.  
Workspace-level services  
Workspace-level audit logs are available for these services:  
Service name  
Description  
accounts  
Events related to accounts, users, groups, and IP access lists.  
clusterPolicies  
Events related to cluster policies.  
clusters  
Events related to clusters.  
dashboards  
Events related to Lakeview dashboard use.  
databrickssql  
Events related to Databricks SQL use.  
dbfs  
Events related to DBFS.  
deltaPipelines  
Events related to Delta Live Table pipelines.  
featureStore  
Events related to the Databricks Feature Store.  
filesystem  
Events related to the Files API.  
genie  
Events related to workspace access by support personnel.  
gitCredentials  
Events related to Git credentials for Databricks Git folders. See also repos.  
globalInitScripts  
Events related to global init scripts.  
groups  
Events related to account and workspace groups.  
iamRole  
Events related to IAM role permissions.  
ingestion  
Events related to file uploads.  
instancePools  
Events related to pools.  
jobs  
Events related to jobs.  
marketplaceConsumer  
Events related to consumer actions in Databricks Marketplace.  
marketplaceProvider  
Events related to provider actions in Databricks Marketplace.  
mlflowAcledArtifact  
Events related to ML Flow artifacts with ACLs.  
mlflowExperiment  
Events related to ML Flow experiments.  
modelRegistry  
Events related to the workspace model registry. For activity logs for models in Unity Catalog, see Unity Catalog events.  
notebook  
Events related to notebooks.  
partnerConnect  
Events related to Partner Connect.  
remoteHistoryService  
Events related to adding a removing GitHub Credentials.  
repos  
Events related to Databricks Git folders. See also gitCredentials.  
secrets  
Events related to secrets.  
serverlessRealTimeInference  
Events related to model serving.  
sqlPermissions  
Events related to the legacy Hive metastore table access control.  
ssh  
Events related to SSH access.  
vectorSearch  
Events related to Databricks Vector Search.  
webTerminal  
Events related to the web terminal feature.  
workspace  
Events related to workspaces.  
Account-level services  
Account-level audit logs are available for these services:  
Service name  
Description  
accountBillableUsage  
Actions related to billable usage access in the account console.  
accounts  
Actions related to account-level access and identity management.  
accountsAccessControl  
Actions related to account-level access control rules.  
accountsManager  
Actions performed in the account console.  
logDelivery  
Log delivery configuration for such as billable usage or audit logs.  
oauth2  
Actions related to OAuth SSO authentication to the account console.  
servicePrincipalCredentials  
Actions related to service principal credentials.  
ssoConfigBackend  
Single sign-on settings for the account.  
unityCatalog  
Actions performed in Unity Catalog. This also includes Delta Sharing events, see Delta Sharing events.  
Additional security monitoring services  
There are additional services and associated actions for workspaces that use the compliance security profile (required for some compliance standards such as FedRAMP, PCI, and HIPAA) or Enhanced security monitoring.  
These are workspace-level services that will only generate in your logs if you are using the compliance security profile or enhanced security monitoring:  
Service name  
Description  
capsule8-alerts-dataplane  
Actions related to file integrity monitoring.  
clamAVScanService-dataplanel  
Actions related to antivirus monitoring.  
monit  
Actions related to the process monitor.  
syslog  
Actions related to the system logs.

Audit log example schema"
22722	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Audit log example schema
In Databricks, audit logs output events in a JSON format. The serviceName and actionName properties identify the event. The naming convention follows the Databricks REST API.  
The following example is for a createMetastoreAssignment event.  
{ ""version"":""2.0"", ""auditLevel"":""ACCOUNT_LEVEL"", ""timestamp"":1629775584891, ""orgId"":""3049056262456431186970"", ""shardName"":""test-shard"", ""accountId"":""77636e6d-ac57-484f-9302-f7922285b9a5"", ""sourceIPAddress"":""10.2.91.100"", ""userAgent"":""curl/7.64.1"", ""sessionId"":""f836a03a-d360-4792-b081-baba525324312"", ""userIdentity"":{ ""email"":""crampton.rods@email.com"", ""subjectName"":null }, ""serviceName"":""unityCatalog"", ""actionName"":""createMetastoreAssignment"", ""requestId"":""ServiceMain-da7fa5878f40002"", ""requestParams"":{ ""workspace_id"":""30490590956351435170"", ""metastore_id"":""abc123456-8398-4c25-91bb-b000b08739c7"", ""default_catalog_name"":""main"" }, ""response"":{ ""statusCode"":200, ""errorMessage"":null, ""result"":null }, ""MAX_LOG_MESSAGE_LENGTH"":16384 }  
Audit log schema considerations  
If actions take a long time, the request and response are logged separately but the request and response pair have the same requestId.  
Automated actions, such as resizing a cluster due to autoscaling or launching a job due to scheduling, are performed by the user System-User.  
The requestParams field is subject to truncation. If the size of its JSON representation exceeds 100 KB, values are truncated and the string ... truncated is appended to truncated entries. In rare cases where a truncated map is still larger than 100 KB, a single TRUNCATED key with an empty value is present instead.

Account events"
22723	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Account events
The following are accounts events logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
accounts  
activateUser  
A user is reactivated after being deactivated. See Deactivate users in workspace.  
targetUserName  
endpoint  
targetUserId  
accounts  
add  
A user is added to a Databricks workspace.  
targetUserName  
endpoint  
targetUserId  
accounts  
addPrincipalToGroup  
A user is added to a workspace-level group.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
addX509  
A user account is added using an X509 certificate for authentication  
accounts  
certLogin  
A user logs in to Databricks using X509 certification.  
user  
accounts  
changeDatabricksSqlAcl  
A user’s Databricks SQL permissions are changed.  
shardName  
targetUserId  
resourceId  
aclPermissionSet  
accounts  
changeDatabricksWorkspaceAcl  
Permissions to a workspace are changed.  
shardName  
targetUserId  
resourceId  
aclPermissionSet  
accounts  
changeDbTokenAcl  
When permissions on a token are changed.  
shardName  
targetUserId  
resourceId  
aclPermissionSet  
accounts  
changePassword  
A user’s password is changed.  
newPasswordSource  
targetUserId  
serviceSource  
wasPasswordChanged  
userId  
accounts  
changePasswordAcl  
Password changing permissions are changed in the account.  
shardName  
targetUserId  
resourceId  
aclPermissionSet  
accounts  
changeServicePrincipalAcls  
When a service principal’s permissions are changed.  
shardName  
targetServicePrincipal  
resourceId  
aclPermissionSet  
accounts  
createGroup  
A workspace-level group is created.  
endpoint  
targetGroupId  
targetGroupName  
accounts  
createIpAccessList  
An IP access list is added to the workspace.  
ipAccessListId  
userId  
accounts  
deactivateUser  
A user is deactivated in the workspace. See Deactivate users in workspace.  
targetUserName  
endpoint  
targetUserId  
accounts  
delete  
A user is deleted from the Databricks workspace.  
targetUserId  
targetUserName  
endpoint  
accounts  
deleteIpAccessList  
An IP access list is deleted from the workspace.  
ipAccessListId  
userId  
accounts  
garbageCollectDbToken  
A user runs a garbage collect command on expired tokens.  
tokenExpirationTime  
tokenClientId  
userId  
tokenCreationTime  
tokenFirstAccessed  
accounts  
generateDbToken  
When someone generates a token from User Settings or when the service generates the token.  
tokenExpirationTime  
tokenCreatedBy  
tokenHash  
userId  
accounts  
IpAccessDenied  
A user attempts to connect to the service through a denied IP.  
path  
userName  
accounts  
ipAccessListQuotaExceeded  
userId  
accounts  
jwtLogin  
User logs into Databricks using a JWT.  
user  
accounts  
login  
User logs into the workspace.  
user  
accounts  
logout  
User logs out of the workspace.  
user  
accounts  
mfaAddKey  
User registers a new security key.  
accounts  
mfaDeleteKey  
User deletes a security key.  
id  
accounts  
mfaLogin  
User logs into Databricks using MFA.  
user  
accounts  
oidcTokenAuthorization  
When an API call is authorized through a generic OIDC/OAuth token.  
user  
accounts  
passwordVerifyAuthentication  
user  
accounts  
reachMaxQuotaDbToken  
When the current number of non-expired tokens exceeds the token quota  
accounts  
removeAdmin  
A user is revoked of workspace admin permissions.  
targetUserName  
endpoint  
targetUserId  
accounts  
removeGroup  
A group is removed from the workspace.  
targetGroupId  
targetGroupName  
endpoint  
accounts  
removePrincipalFromGroup  
A user is removed from a group.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
resetPassword  
A user’s password is reset.  
serviceSource  
userId  
endpoint  
targetUserId  
targetUserName  
wasPasswordChanged  
newPasswordSource  
accounts  
revokeDbToken  
A user’s token is dropped from a workspace. Can be triggered by a user being removed from the Databricks account.  
userId  
accounts  
samlLogin  
User logs in to Databricks through SAML SSO.  
user  
accounts  
setAdmin  
A user is granted account admin permissions.  
endpoint  
targetUserName  
targetUserId  
accounts  
tokenLogin  
A user logs into Databricks using a token.  
tokenId  
user  
accounts  
updateIpAccessList  
An IP access list is changed.  
ipAccessListId  
userId  
accounts  
updateUser  
An account admin updates a user’s account.  
targetUserName  
endpoint  
targetUserId  
accounts  
validateEmail  
When a user validates their email after account creation.  
endpoint  
targetUserName  
targetUserId

Clusters events"
22724	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Clusters events
The following are cluster events logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
clusters  
changeClusterAcl  
A user changes the cluster ACL.  
shardName  
aclPermissionSet  
targetUserId  
resourceId  
clusters  
create  
A user creates a cluster.  
cluster_log_conf  
num_workers  
enable_elastic_disk  
driver_node_type_id  
start_cluster  
docker_image  
ssh_public_keys  
aws_attributes  
acl_path_prefix  
node_type_id  
instance_pool_id  
spark_env_vars  
init_scripts  
spark_version  
cluster_source  
autotermination_minutes  
cluster_name  
autoscale  
custom_tags  
cluster_creator  
enable_local_disk_encryption  
idempotency_token  
spark_conf  
organization_id  
no_driver_daemon  
user_id  
virtual_cluster_size  
apply_policy_default_values  
data_security_mode  
runtime_engine  
clusters  
createResult  
Results from cluster creation. In conjunction with create.  
clusterName  
clusterState  
clusterId  
clusterWorkers  
clusterOwnerUserId  
clusters  
delete  
A cluster is terminated.  
cluster_id  
clusters  
deleteResult  
Results from cluster termination. In conjunction with delete.  
clusterName  
clusterState  
clusterId  
clusterWorkers  
clusterOwnerUserId  
clusters  
edit  
A user makes changes to cluster settings. This logs all changes except for changes in cluster size or autoscaling behavior.  
cluster_log_conf  
num_workers  
enable_elastic_disk  
driver_node_type_id  
start_cluster  
docker_image  
ssh_public_keys  
aws_attributes  
acl_path_prefix  
node_type_id  
instance_pool_id  
spark_env_vars  
init_scripts  
spark_version  
cluster_source  
autotermination_minutes  
cluster_name  
autoscale  
custom_tags  
cluster_creator  
enable_local_disk_encryption  
idempotency_token  
spark_conf  
organization_id  
no_driver_daemon  
user_id  
virtual_cluster_size  
apply_policy_default_values  
data_security_mode  
runtime_engine  
clusters  
permanentDelete  
A cluster is deleted from the UI.  
cluster_id  
clusters  
resize  
Cluster resizes. This is logged on running clusters where the only property that changes is either the cluster size or autoscaling behavior.  
cluster_id  
num_workers  
autoscale  
clusters  
resizeResult  
Results from cluster resize. In conjunction with resize.  
clusterName  
clusterState  
clusterId  
clusterWorkers  
clusterOwnerUserId  
clusters  
restart  
A user restarts a running cluster.  
cluster_id  
clusters  
restartResult  
Results from cluster restart. In conjunction with restart.  
clusterName  
clusterState  
clusterId  
clusterWorkers  
clusterOwnerUserId  
clusters  
start  
A user starts a cluster.  
init_scripts_safe_mode  
cluster_id  
clusters  
startResult  
Results from cluster start. In conjunction with start.  
clusterName  
clusterState  
clusterId  
clusterWorkers  
clusterOwnerUserId

Cluster libraries events
Cluster libraries events
The following are clusterLibraries events logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
clusterLibraries  
installLibraries  
User installs a library on a cluster.  
cluster_id  
libraries  
clusterLibraries  
uninstallLibraries  
User uninstalls a library on a cluster.  
cluster_id  
libraries  
clusterLibraries  
installLibraryOnAllClusters  
A workspace admin schedules a library to install on all cluster.  
user  
library  
clusterLibraries  
uninstallLibraryOnAllClusters  
A workspace admin removes a library from the list to install on all clusters.  
user  
library

Cluster policy events
Cluster policy events
The following are clusterPolicies events logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
clusterPolicies  
create  
A user created a cluster policy.  
name  
clusterPolicies  
edit  
A user edited a cluster policy.  
policy_id  
name  
clusterPolicies  
delete  
A user deleted a cluster policy.  
policy_id  
clusterPolicies  
changeClusterPolicyAcl  
A workspace admin changes permissions for a cluster policy.  
shardName  
targetUserId  
resourceId  
aclPermissionSet

Dashboards events"
22725	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Dashboards events
The following are dashboards events logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
dashboards  
createDashboard  
A user creates a new Lakeview dashboard using the UI or API.  
dashboard_id  
dashboards  
updateDashboard  
A user makes an update to a Lakeview dashboard using the UI or API.  
dashboard_id  
dashboards  
cloneDashboard  
A user clones a Lakeview dashboard.  
source_dashboard_id  
new_dashboard_id  
dashboards  
publishDashboard  
A user publishes a Lakeview dashboard with or without embedded credentials using the UI or API.  
dashboard_id  
credentials_embedded  
warehouse_id  
dashboards  
unpublishDashboard  
A user unpublishes a published Lakeview dashboard using the UI or API.  
dashboard_id  
dashboards  
trashDashboard  
A user moves a Lakeview dashboard to the trash using the UI or API.  
dashboard_id  
dashboards  
restoreDashboard  
A user restores a Lakeview dashboard from the trash.  
dashboard_id  
dashboards  
migrateDashboard  
A user migrates a DBSQL dashboard to a Lakeview dashboard.  
source_dashboard_id  
new_dashboard_id  
dashboards  
createSchedule  
A user creates an email subscription schedule.  
dashboard_id  
schedule_id  
dashboards  
updateSchedule  
A user makes an update to a Lakeview dashboard’s schedule.  
dashboard_id  
schedule_id  
dashboards  
deleteSchedule  
A user deletes a Lakeview dashboard’s schedule.  
dashboard_id  
schedule_id  
dashboards  
createSubscription  
A user subscribes an email destination to a Lakeview dashboard schedule.  
dashboard_id  
schedule_id  
schedule  
dashboards  
deleteSubscription  
A user deletes an email destination from a Lakeview dashboard schedule.  
dashboard_id  
schedule_id

Databricks SQL events"
22726	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Databricks SQL events
The following are databrickssql events logged at the workspace level.  
Note  
If you manage your SQL warehouses using the legacy SQL endpoints API, your SQL warehouse audit events will have different action names. See SQL endpoint logs.  
Service  
Action  
Description  
Request parameters  
databrickssql  
addDashboardWidget  
A widget is added to a dashboard.  
dashboardId  
widgetId  
databrickssql  
cancelQueryExecution  
A query execution is cancelled from the SQL editor UI. This does not include cancellations that originate from the Query History UI or Databricks SQL Execution API.  
queryExecutionId  
databrickssql  
changeWarehouseAcls  
A warehouse manager updates permissions on a SQL warehouse.  
aclPermissionSet  
resourceId  
shardName  
targetUserId  
databrickssql  
changePermissions  
A user updates permissions on an object.  
granteeAndPermission  
objectId  
objectType  
databrickssql  
cloneDashboard  
A user clones a dashboard.  
dashboardId  
databrickssql  
commandSubmit  
Only in verbose audit logs. Generated when a command is submitted to a SQL warehouse, regardless of origin of the request.  
warehouseId  
commandId  
validation  
commandText  
databrickssql  
commandFinish  
Only in verbose audit logs. Generated when a command on a SQL warehouse completes or is canceled, regardless of the origin of the cancellation request.  
warehouseId  
commandId  
databrickssql  
createAlert  
A user creates an alert.  
alertId  
databrickssql  
createNotificationDestination  
A workspace admin creates a notification destination.  
notificationDestinationId  
notificationDestinationType  
databrickssql  
createDashboard  
A user creates a dashboard.  
dashboardId  
databrickssql  
createDataPreviewDashboard  
A user creates a data preview dashboard.  
dashboardId  
databrickssql  
createWarehouse  
A user with the cluster create entitlement creates a SQL warehouse.  
auto_resume  
auto_stop_mins  
channel  
cluster_size  
conf_pairs  
custom_cluster_confs  
enable_databricks_compute  
enable_photon  
enable_serverless_compute  
instance_profile_arn  
max_num_clusters  
min_num_clusters  
name  
size  
spot_instance_policy  
tags  
test_overrides  
databrickssql  
createQuery  
A user creates a query by saving a query draft.  
queryId  
databrickssql  
createQueryDraft  
A user creates a query draft.  
queryId  
databrickssql  
createQuerySnippet  
A user creates a query snippet.  
querySnippetId  
databrickssql  
createSampleDashboard  
A user creates a sample dashboard.  
sampleDashboardId  
databrickssql  
createVisualization  
A user generates a visualization using the SQL editor. Excludes default results tables and visualizations in notebooks that utilize SQL warehouses.  
queryId  
visualizationId  
databrickssql  
deleteAlert  
A user deletes an alert either from the alert interface or through API. Excludes deletions from the file browser UI.  
alertId  
databrickssql  
deleteNotificationDestination  
A workspace admin deletes a notification destination.  
notificationDestinationId  
databrickssql  
deleteDashboard  
A user deletes a dashboard either from the dashboard interface or through API. Excludes deletion via the file browser UI.  
dashboardId  
databrickssql  
deleteDashboardWidget  
A user deletes a dashboard widget.  
widgetId  
databrickssql  
deleteWarehouse  
A warehouse manager deletes a SQL warehouse.  
id  
databrickssql  
deleteQuery  
A user deletes a query, either from the query interface or through API. Excludes deletion via the file browser UI.  
queryId  
databrickssql  
deleteQueryDraft  
A user deletes a query draft.  
queryId  
databrickssql  
deleteQuerySnippet  
A user deletes a query snippet.  
querySnippetId  
databrickssql  
deleteVisualization  
A user deletes a visualization from a query in the SQL Editor.  
visualizationId  
databrickssql  
downloadQueryResult  
A user downloads a query result from the SQL Editor. Excludes downloads from dashboards.  
fileType  
queryId  
queryResultId  
databrickssql  
editWarehouse  
A warehouse manager makes edits to a SQL warehouse.  
auto_stop_mins  
channel  
cluster_size  
confs  
enable_photon  
enable_serverless_compute  
id  
instance_profile_arn  
max_num_clusters  
min_num_clusters  
name  
spot_instance_policy  
tags  
databrickssql  
executeAdhocQuery  
Generated by one of the following:  
A user runs a query draft in the SQL editor  
A query is executed from a visualization aggregation  
A user loads a dashboard and executes underlying queries  
dataSourceId  
databrickssql  
executeSavedQuery  
A user runs a saved query.  
queryId  
databrickssql  
executeWidgetQuery  
Generated by any event that executes a query such that a dashboard panel refreshes. Some examples of applicable events include:  
Refreshing a single panel  
Refreshing an entire dashboard  
Scheduled dashboard executions  
Parameter or filter changes operating over more than 64,000 rows  
widgetId  
databrickssql  
favoriteDashboard  
A user favorites a dashboard.  
dashboardId  
databrickssql  
favoriteQuery  
A user favorites a query.  
queryId  
databrickssql  
forkQuery  
A user clones a query.  
originalQueryId  
queryId  
databrickssql  
listQueries  
A user opens the query listing page or calls the list query API.  
filter_by  
include_metrics  
max_results  
page_token  
databrickssql  
moveDashboardToTrash  
A user moves a dashboard to the trash.  
dashboardId  
databrickssql  
moveQueryToTrash  
A user moves a query to the trash.  
queryId  
databrickssql  
muteAlert  
A user mutes an alert via the API.  
alertId  
databrickssql  
restoreDashboard  
A user restores a dashboard from the trash.  
dashboardId  
databrickssql  
restoreQuery  
A user restores a query from the trash.  
queryId  
databrickssql  
setWarehouseConfig  
A warehouse manager sets the configuration for a SQL warehouse.  
data_access_config  
enable_serverless_compute  
instance_profile_arn  
security_policy  
serverless_agreement  
sql_configuration_parameters  
try_create_databricks_managed_starter_warehouse  
databrickssql  
snapshotDashboard  
A user requests a snapshot of a dashboard. Includes scheduled dashboard snapshots.  
dashboardId  
databrickssql  
startWarehouse  
A SQL warehouse is started.  
id  
databrickssql  
stopWarehouse"
22727	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"databrickssql  
snapshotDashboard  
A user requests a snapshot of a dashboard. Includes scheduled dashboard snapshots.  
dashboardId  
databrickssql  
startWarehouse  
A SQL warehouse is started.  
id  
databrickssql  
stopWarehouse  
A warehouse manager stops a SQL warehouse. Excludes autostopped warehouses.  
id  
databrickssql  
transferObjectOwnership  
A workspace admin transfers the ownership of a dashboard, query, or alert to an active user.  
newOwner  
objectId  
objectType  
databrickssql  
unfavoriteDashboard  
A user removes a dashboard from their favorites.  
dashboardId  
databrickssql  
unfavoriteQuery  
A user removes a query from their favorites.  
queryId  
databrickssql  
unmuteAlert  
A user unmutes an alert via the API  
alertId.  
databrickssql  
updateAlert  
A user makes updates to an alert.  
alertId  
queryId  
databrickssql  
updateNotificationDestination  
A workspace admin makes an update to a notification destination.  
notificationDestinationId  
databrickssql  
updateDashboardWidget  
A user makes an update to a dashboard widget. Excludes changes to axis scales. Examples of applicable updates include:  
Change to widget size or placement  
Adding or removing widget parameters  
widgetId  
databrickssql  
updateDashboard  
A user makes an update to a dashboard property. Excludes changes to schedules and subscriptions. Examples of applicable updates include:  
Change in dashboard name  
Change to the SQL warehouse  
Change to Run As settings  
dashboardId  
databrickssql  
updateOrganizationSetting  
A workspace admin makes updates to the workspace’s SQL settings.  
has_configured_data_access  
has_explored_sql_warehouses  
has_granted_permissions  
databrickssql  
updateQuery  
A user makes an update to a query.  
queryId  
databrickssql  
updateQueryDraft  
A user makes an update to a query draft.  
queryId  
databrickssql  
updateQuerySnippet  
A user makes an update to a query snippet.  
querySnippetId  
databrickssql  
updateVisualization  
A user updates a visualization from either the SQL Editor or the dashboard.  
visualizationId"
22728	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"DBFS events
DBFS events
The following tables include dbfs events logged at the workspace level.  
There are two types of DBFS events: API calls and operational events.  
DBFS API events  
The following DBFS audit events are only logged when written through the DBFS REST API.  
Service  
Action  
Description  
Request parameters  
dbfs  
addBlock  
User appends a block of data to the stream. This is used in conjunction with dbfs/create to stream data to DBFS.  
handle  
data_length  
dbfs  
create  
User opens a stream to write a file to DBFs.  
path  
bufferSize  
overwrite  
dbfs  
delete  
User deletes the file or directory from DBFs.  
recursive  
path  
dbfs  
mkdirs  
User creates a new DBFS directory.  
path  
dbfs  
move  
User moves a file from one location to another location within DBFs.  
dst  
source_path  
src  
destination_path  
dbfs  
put  
User uploads a file through the use of multipart form post to DBFs.  
path  
overwrite  
DBFS operational events  
The following DBFS audit events occur at the compute plane.  
Service  
Action  
Description  
Request parameters  
dbfs  
mount  
User creates a mount point at a certain DBFS location.  
mountPoint  
owner  
dbfs  
unmount  
User removes a mount point at a certain DBFS location.  
mountPoint

Delta pipelines events
Delta pipelines events
Service  
Action  
Description  
Request parameters  
deltaPipelines  
changePipelineAcls  
A user changes permissions on a pipeline.  
shardId  
targetUserId  
resourceId  
aclPermissionSet  
deltaPipelines  
create  
A user creates a Delta Live Tables pipeline.  
allow_duplicate_names  
clusters  
configuration  
continuous  
development  
dry_run  
id  
libraries  
name  
storage  
target  
channel  
edition  
photon  
deltaPipelines  
delete  
A user deletes a Delta Live Tables pipeline.  
pipeline_id  
deltaPipelines  
edit  
A user edits a Delta Live Tables pipeline.  
allow_duplicate_names  
clusters  
configuration  
continuous  
development  
expected_last_modified  
id  
libraries  
name  
pipeline_id  
storage  
target  
channel  
edition  
photon  
deltaPipelines  
startUpdate  
A user restarts a Delta Live Tables pipeline.  
cause  
full_refresh  
job_task  
pipeline_id  
deltaPipelines  
stop  
A user stops a Delta Live Tables pipeline.  
pipeline_id

Feature store events
Feature store events
The following featureStore events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
featureStore  
addConsumer  
A consumer is added to the feature store.  
features  
job_run  
notebook  
featureStore  
addDataSources  
A data source is added to a feature table.  
feature_table  
paths, tables  
featureStore  
addProducer  
A producer is added to a feature table.  
feature_table  
job_run  
notebook  
featureStore  
changeFeatureTableAcl  
Permissions are changed in a feature table.  
aclPermissionSet  
resourceId  
shardName  
targetUserId  
featureStore  
createFeatureTable  
A feature table is created.  
description  
name  
partition_keys  
primary_keys  
timestamp_keys  
featureStore  
createFeatures  
Features are created in a feature table.  
feature_table  
features  
featureStore  
deleteFeatureTable  
A feature table is deleted.  
name  
featureStore  
deleteTags  
Tags are deleted from a feature table.  
feature_table_id  
keys  
featureStore  
getConsumers  
A user makes a call to get the consumers in a feature table.  
feature_table  
featureStore  
getFeatureTable  
A user makes a call to get feature tables.  
name  
featureStore  
getFeatureTablesById  
A user makes a call to get feature table IDs.  
ids  
featureStore  
getFeatures  
A user makes a call to get features.  
feature_table  
max_results  
featureStore  
getModelServingMetadata  
A user makes a call to get Model Serving metadata.  
feature_table_features  
featureStore  
getOnlineStore  
A user makes a call to get online store details.  
cloud  
feature_table  
online_table  
store_type  
featureStore  
getTags  
A user makes a call to get tags for a feature table.  
feature_table_id  
featureStore  
publishFeatureTable  
A feature table is published.  
cloud  
feature_table  
host  
online_table  
port  
read_secret_prefix  
store_type  
write_secret_prefix  
featureStore  
searchFeatureTables  
A user searches for feature tables.  
max_results  
page_token  
text  
featureStore  
setTags  
Tags are added to a feature table.  
feature_table_id  
tags  
featureStore  
updateFeatureTable  
A feature table is updated.  
description  
name

Files API events
Files API events
The following filesystem events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
filesystem  
filesGet  
User downloads file.  
path  
transferredSize  
filesystem  
filesPut  
User uploads file.  
path  
receivedSize  
filesystem  
filesDelete  
User deletes file.  
path  
filesystem  
filesHead  
User gets information about file.  
path

Genie events
Genie events
The following genie events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
genie  
databricksAccess  
A Databricks personnel is authorized to access a customer environment.  
duration  
approver  
reason  
authType  
user

Git credential events
Git credential events
The following gitCredentials events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
gitCredentials  
getGitCredential  
A user gets a git credentials.  
id  
gitCredentials  
listGitCredentials  
A user lists all git credentials  
none  
gitCredentials  
deleteGitCredential  
A user deletes a git credential.  
id  
gitCredentials  
updateGitCredential  
A user updates a git credential.  
id  
git_provider  
git_username  
gitCredentials  
createGitCredential  
A user creates a git credential.  
git_provider  
git_username

Global init scripts events"
22729	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Global init scripts events
The following globalInitScripts events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
globalInitScripts  
create  
A workspace admin creates a global initialization script.  
name  
position  
script-SHA256  
enabled  
globalInitScripts  
update  
A workspace admin updates a global initialization script.  
script_id  
name  
position  
script-SHA256  
enabled  
globalInitScripts  
delete  
A workspace admin deletes a global initialization script.  
script_id

Groups events
Groups events
The following groups events are logged at the workspace level. These actions are related to legacy ACL groups. For actions related to account- and workspace-level groups, see Account events and Account-level account events.  
Service  
Action  
Description  
Request parameters  
groups  
addPrincipalToGroup  
An admin adds a user to a group.  
user_name  
parent_name  
groups  
createGroup  
An admin creates a group.  
group_name  
groups  
getGroupMembers  
An admin views group members.  
group_name  
groups  
getGroups  
An admin views a list of groups  
none  
groups  
getInheritedGroups  
An admin views inherited groups  
none  
groups  
removeGroup  
An admin removes a group.  
group_name

IAM role events
IAM role events
The following iamRole event is logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
iamRole  
changeIamRoleAcl  
A workspace admin changes permissions for an IAM role.  
targetUserId  
shardName  
resourceId  
aclPermissionSet

Ingestion events
Ingestion events
The following ingestion event is logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
ingestion  
proxyFileUpload  
A user uploads a file to their Databricks workspace.  
x-databricks-content-length-0  
x-databricks-total-files

Instance pool events
Instance pool events
The following instancePools events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
instancePools  
changeInstancePoolAcl  
A user changes an instance pool’s permissions.  
shardName  
resourceId  
targetUserId  
aclPermissionSet  
instancePools  
create  
A user creates an instance pool.  
enable_elastic_disk  
preloaded_spark_versions  
idle_instance_autotermination_minutes  
instance_pool_name  
node_type_id  
custom_tags  
max_capacity  
min_idle_instances  
aws_attributes  
instancePools  
delete  
A user deletes an instance pool.  
instance_pool_id  
instancePools  
edit  
A user edits an instance pool.  
instance_pool_name  
idle_instance_autotermination_minutes  
min_idle_instances  
preloaded_spark_versions  
max_capacity  
enable_elastic_disk  
node_type_id  
instance_pool_id  
aws_attributes

Job events"
22730	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Job events
The following jobs events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
jobs  
cancel  
A job run is cancelled.  
run_id  
jobs  
cancelAllRuns  
A user cancels all runs on a job.  
job_id  
jobs  
changeJobAcl  
A user updates permissions on a job.  
shardName  
aclPermissionSet  
resourceId  
targetUserId  
jobs  
create  
A user creates a job.  
spark_jar_task  
email_notifications  
notebook_task  
spark_submit_task  
timeout_seconds  
libraries  
name  
spark_python_task  
job_type  
new_cluster  
existing_cluster_id  
max_retries  
schedule  
run_as  
jobs  
delete  
A user deletes a job.  
job_id  
jobs  
deleteRun  
A user deletes a job run.  
run_id  
jobs  
getRunOutput  
A user makes an API call to get a run output.  
run_id  
is_from_webapp  
jobs  
repairRun  
A user repairs a job run.  
run_id  
latest_repair_id  
rerun_tasks  
jobs  
reset  
A job is reset.  
job_id  
new_settings  
jobs  
resetJobAcl  
A user requests the change of a job’s permissions.  
grants  
job_id  
jobs  
runCommand  
Available when verbose audit logs are enabled. Emitted after a command in a notebook is executed by a job run. A command corresponds to a cell in a notebook.  
jobId  
runId  
notebookId  
executionTime  
status  
commandId  
commandText  
jobs  
runFailed  
A job run fails.  
jobClusterType  
jobTriggerType  
jobId  
jobTaskType  
runId  
jobTerminalState  
idInJob  
orgId  
runCreatorUserName  
jobs  
runNow  
A user triggers an on-demand job run.  
notebook_params  
job_id  
jar_params  
workflow_context  
jobs  
runStart  
Emitted when a job run starts after validation and cluster creation. The request parameters emitted from this event depend on the type of tasks in the job. In addition to the parameters listed, they can include:  
dashboardId (for a SQL dashboard task)  
filePath (for a SQL file task)  
notebookPath (for a notebook task)  
mainClassName (for a Spark JAR task)  
pythonFile (for a Spark JAR task)  
projectDirectory (for a dbt task)  
commands (for a dbt task)  
packageName (for a Python wheel task)  
entryPoint (for a Python wheel task)  
pipelineId (for a pipeline task)  
queryIds (for a SQL query task)  
alertId (for a SQL alert task)  
taskDependencies  
multitaskParentRunId  
orgId  
idInJob  
jobId  
jobTerminalState  
taskKey  
jobTriggerType  
jobTaskType  
runId  
runCreatorUserName  
jobs  
runSucceeded  
A job run is successful.  
idInJob  
jobId  
jobTriggerType  
orgId  
runId  
jobClusterType  
jobTaskType  
jobTerminalState  
runCreatorUserName  
jobs  
runTriggered  
A job schedule is triggered automatically according to its schedule or trigger.  
jobId  
jobTriggeredType  
runId  
jobs  
sendRunWebhook  
A webhook is sent either when the job begins, completes, or fails.  
orgId  
jobId  
jobWebhookId  
jobWebhookEvent  
runId  
jobs  
setTaskValue  
A user sets values for a task.  
run_id  
key  
jobs  
submitRun  
A user submits a one-time run via the API.  
shell_command_task  
run_name  
spark_python_task  
existing_cluster_id  
notebook_task  
timeout_seconds  
libraries  
new_cluster  
spark_jar_task  
jobs  
update  
A user edits a job’s settings.  
job_id  
fields_to_remove  
new_settings  
is_from_dlt

Marketplace consumer events
Marketplace consumer events
The following marketplaceConsumer events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
marketplaceConsumer  
getDataProduct  
A user gets access to a data product through the Databricks Marketplace.  
listing_id  
listing_name  
share_name  
catalog_name  
request_context: Array of information about the account and metastore that got access to the data product  
marketplaceConsumer  
requestDataProduct  
A user requests access to a data product that requires provider approval.  
listing_id  
listing_name  
catalog_name  
request_context: Array of information about the account and metastore requesting access to the data product

Marketplace provider events"
22731	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Marketplace provider events
The following marketplaceProvider events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
marketplaceProvider  
createListing  
A metastore admin creates a listing in their provider profile.  
listing: Array of details about the listing  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
updateListing  
A metastore admin makes an update to a listing in their provider profile.  
id  
listing: Array of details about the listing  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
deleteListing  
A metastore admin deletes a listing in their provider profile.  
id  
request_context: Array of details about the provider’s account and metastore  
marketplaceProvider  
updateConsumerRequestStatus  
A metastore admins approves or denies a data product request.  
listing_id  
request_id  
status  
reason  
share: Array of information about the share  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
createProviderProfile  
A metastore admin creates a provider profile.  
provider: Array of information about the provider  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
updateProviderProfile  
A metastore admin makes an update to their provider profile.  
id  
provider: Array of information about the provider  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
deleteProviderProfile  
A metastore admin deletes their provider profile.  
id  
request_context: Array of information about the provider’s account and metastore  
marketplaceProvider  
uploadFile  
A provider uploads a file to their provider profile.  
request_context: Array of information about the provider’s account and metastore  
marketplace_file_type  
display_name  
mime_type  
file_parent: Array of file parent details  
marketplaceProvider  
deleteFile  
A provider deletes a file from their provider profile.  
file_id  
request_context: Array of information about the provider’s account and metastore

MLflow artifacts with ACL events
MLflow artifacts with ACL events
The following mlflowAcledArtifact events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
mlflowAcledArtifact  
readArtifact  
A user makes call to read an artifact.  
artifactLocation  
experimentId  
runId  
mlflowAcledArtifact  
writeArtifact  
A user makes call to write to an artifact.  
artifactLocation  
experimentId  
runId

MLflow experiment events
MLflow experiment events
The following mlflowExperiment events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
mlflowExperiment  
createMlflowExperiment  
A user creates an MLflow experiment.  
experimentId  
path  
experimentName  
mlflowExperiment  
deleteMlflowExperiment  
A user deletes an MLflow experiment.  
experimentId  
path  
experimentName  
mlflowExperiment  
moveMlflowExperiment  
A user moves an MLflow experiment.  
newPath  
experimentId  
oldPath  
mlflowExperiment  
restoreMlflowExperiment  
A user restores an MLflow experiment.  
experimentId  
path  
experimentName  
mlflowExperiment  
renameMlflowExperiment  
A user renames an MLflow experiment.  
oldName  
newName  
experimentId  
parentPath

MLflow model registry events"
22732	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"MLflow model registry events
The following mlflowModelRegistry events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
modelRegistry  
approveTransitionRequest  
A user approves a model version stage transition request.  
name  
version  
stage  
archive_existing_versions  
modelRegistry  
changeRegisteredModelAcl  
A user updates permissions for a registered model.  
registeredModelId  
userId  
modelRegistry  
createComment  
A user posts a comment on a model version.  
name  
version  
modelRegistry  
createModelVersion  
A user creates a model version.  
name  
source  
run_id  
tags  
run_link  
modelRegistry  
createRegisteredModel  
A user creates a new registered model  
name  
tags  
modelRegistry  
createRegistryWebhook  
User creates a webhook for Model Registry events.  
orgId  
registeredModelId  
events  
description  
status  
creatorId  
httpUrlSpec  
modelRegistry  
createTransitionRequest  
A user creates a model version stage transition request.  
name  
version  
stage  
modelRegistry  
deleteComment  
A user deletes a comment on a model version.  
id  
modelRegistry  
deleteModelVersion  
A user deletes a model version.  
name  
version  
modelRegistry  
deleteModelVersionTag  
A user deletes a model version tag.  
name  
version  
key  
modelRegistry  
deleteRegisteredModel  
A user deletes a registered model  
name  
modelRegistry  
deleteRegisteredModelTag  
A user deletes the tag for a registered model.  
name  
key  
modelRegistry  
deleteRegistryWebhook  
User deletes a Model Registry webhook.  
orgId  
webhookId  
modelRegistry  
deleteTransitionRequest  
A user cancels a model version stage transition request.  
name  
version  
stage  
creator  
modelRegistry  
finishCreateModelVersionAsync  
Completed asynchronous model copying.  
name  
version  
modelRegistry  
generateBatchInferenceNotebook  
Batch inference notebook is autogenerated.  
userId  
orgId  
modelName  
inputTableOpt  
outputTablePathOpt  
stageOrVersion  
modelVersionEntityOpt  
notebookPath  
modelRegistry  
generateDltInferenceNotebook  
Inference notebook for a Delta Live Tables pipeline is autogenerated.  
userId  
orgId  
modelName  
inputTable  
outputTable  
stageOrVersion  
notebookPath  
modelRegistry  
getModelVersionDownloadUri  
A user gets a URI to download the model version.  
name  
version  
modelRegistry  
getModelVersionSignedDownloadUri  
A user gets a URI to download a signed model version.  
name  
version  
path  
modelRegistry  
listModelArtifacts  
A user makes a call to list a model’s artifacts.  
name  
version  
path  
page_token  
modelRegistry  
listRegistryWebhooks  
A user makes a call to list all registry webhooks in the model.  
orgId  
registeredModelId  
modelRegistry  
rejectTransitionRequest  
A user rejects a model version stage transition request.  
name  
version  
stage  
modelRegistry  
renameRegisteredModel  
A user renames a registered model  
name  
new_name  
modelRegistry  
setEmailSubscriptionStatus  
A user updates the email subscription status for a registered model  
modelRegistry  
setModelVersionTag  
A user sets a model version tag.  
name  
version  
key  
value  
modelRegistry  
setRegisteredModelTag  
A user sets a model version tag.  
name  
key  
value  
modelRegistry  
setUserLevelEmailSubscriptionStatus  
A user updates their email notifications status for the whole registry.  
orgId  
userId  
subscriptionStatus  
modelRegistry  
testRegistryWebhook  
A user tests the Model Registry webhook.  
orgId  
webhookId  
modelRegistry  
transitionModelVersionStage  
A user gets a list of all open stage transition requests for the model version.  
name  
version  
stage  
archive_existing_versions  
modelRegistry  
triggerRegistryWebhook  
A Model Registry webhook is triggered by an event.  
orgId  
registeredModelId  
events  
status  
modelRegistry  
updateComment  
A user post an edit to a comment on a model version.  
id  
modelRegistry  
updateRegistryWebhook  
A user updates a Model Registry webhook.  
orgId  
webhookId

Model serving events
Model serving events
The following serverlessRealTimeInference events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
serverlessRealTimeInference  
changeInferenceEndpointAcl  
User updates permissions for an inference endpoint.  
shardName  
targetUserId  
resourceId  
aclPermissionSet  
serverlessRealTimeInference  
createServingEndpoint  
User creates a model serving endpoint.  
name  
config  
serverlessRealTimeInference  
deleteServingEndpoint  
User deletes a model serving endpoint.  
name  
serverlessRealTimeInference  
disable  
User disables model serving for a registered model.  
registered_mode_name  
serverlessRealTimeInference  
enable  
User enables model serving for a registered model.  
registered_mode_name  
serverlessRealTimeInference  
getQuerySchemaPreview  
Users makes a call to get the query schema preview.  
endpoint_name  
serverlessRealTimeInference  
updateServingEndpoint  
User updates a model serving endpoint.  
name  
served_models  
traffic_config

Notebook events"
22733	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Notebook events
The following notebook events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
notebook  
attachNotebook  
A notebook is attached to a cluster.  
path  
clusterId  
notebookId  
notebook  
cloneNotebook  
A user clones a notebook.  
notebookId  
path  
clonedNotebookId  
destinationPath  
notebook  
createNotebook  
A notebook is created.  
notebookId  
path  
notebook  
deleteFolder  
A notebook folder is deleted.  
path  
notebook  
deleteNotebook  
A notebook is deleted.  
notebookId  
notebookName  
path  
notebook  
detachNotebook  
A notebook is detached from a cluster.  
notebookId  
clusterId  
path  
notebook  
downloadLargeResults  
A user downloads query results too large to display in the notebook.  
notebookId  
notebookFullPath  
notebook  
downloadPreviewResults  
A user downloads the query results.  
notebookId  
notebookFullPath  
notebook  
importNotebook  
A user imports a notebook.  
path  
notebook  
moveFolder  
A notebook folder is moved from one location to another.  
oldPath  
newPath  
folderId  
notebook  
moveNotebook  
A notebook is moved from one location to another.  
newPath  
oldPath  
notebookId  
notebook  
renameNotebook  
A notebook is renamed.  
newName  
oldName  
parentPath  
notebookId  
notebook  
restoreFolder  
A deleted folder is restored.  
path  
notebook  
restoreNotebook  
A deleted notebook is restored.  
path  
notebookId  
notebookName  
notebook  
runCommand  
Available when verbose audit logs are enabled. Emitted after Databricks runs a command in a notebook. A command corresponds to a cell in a notebook.  
executionTime is measured in seconds.  
notebookId  
executionTime  
status  
commandId  
commandText  
commandLanguage  
notebook  
takeNotebookSnapshot  
Notebook snapshots are taken when either the job service or mlflow is run.  
path

Partner Connect events
Partner Connect events
The following partnerHub events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
partnerHub  
createOrReusePartnerConnection  
A workspace admin sets up a connection to a partner solution.  
partner_name  
partnerHub  
deletePartnerConnection  
A workspace admin deletes a partner connection.  
partner_name  
partnerHub  
downloadPartnerConnectionFile  
A workspace admin downloads the partner connection file.  
partner_name  
partnerHub  
setupResourcesForPartnerConnection  
A workspace admin sets up resources for a partner connection.  
partner_name

Remote history service events
Remote history service events
The following remoteHistoryService events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
remoteHistoryService  
addUserGitHubCredentials  
User adds Github Credentials  
none  
remoteHistoryService  
deleteUserGitHubCredentials  
User removes Github Credentials  
none  
remoteHistoryService  
updateUserGitHubCredentials  
User updates Github Credentials  
none

Git folder events
Git folder events
The following repos events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
repos  
checkoutBranch  
A user checks out a branch on the repo.  
id  
branch  
repos  
commitAndPush  
A user commits and pushes to a repo.  
id  
message  
files  
checkSensitiveToken  
repos  
createRepo  
A user creates a repo in the workspace  
url  
provider  
path  
repos  
deleteRepo  
A user deletes a repo.  
id  
repos  
discard  
A user discards a commit to a repo.  
id  
file_paths  
repos  
getRepo  
A user makes a call to get information about a single repo.  
id  
repos  
listRepos  
A user makes a call to get all repos they have Manage permissions on.  
path_prefix  
next_page_token  
repos  
pull  
A user pulls the latest commits from a repo.  
id  
repos  
updateRepo  
A user updates the repo to a different branch or tag, or to the latest commit on the same branch.  
id  
branch  
tag  
git_url  
git_provider

Secrets events
Secrets events
The following secrets events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
secrets  
createScope  
User creates a secret scope.  
scope  
initial_manage_principal  
scope_backend_type  
secrets  
deleteAcl  
User deletes ACLs for a secret scope.  
scope  
principal  
secrets  
deleteScope  
User deletes a secret scope.  
scope  
secrets  
deleteSecret  
User deletes a secret from a scope.  
key  
scope  
secrets  
getAcl  
User gets ACLs for a secret scope.  
scope  
principal  
secrets  
getSecret  
User gets a secret from a scope.  
key  
scope  
secrets  
listAcls  
User makes a call to list ACLs for a secret scope.  
scope  
secrets  
listScopes  
User makes a call to list secret scopes  
none  
secrets  
listSecrets  
User makes a call to list secrets within a scope.  
scope  
secrets  
putAcl  
User changes ACLs for a secret scope.  
scope  
principal  
permission  
secrets  
putSecret  
User adds or edits a secret within a scope.  
string_value  
key  
scope

SQL table access events"
22734	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"SQL table access events
Note  
The sqlPermissions service includes events related to the legacy Hive metastore table access control. Databricks recommends that you upgrade the tables managed by the Hive metastore to the Unity Catalog metastore.  
The following sqlPermissions events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
sqlPermissions  
changeSecurableOwner  
Workspace admin or owner of an object transfers object ownership.  
securable  
principal  
sqlPermissions  
createSecurable  
User creates a securable object.  
securable  
sqlPermissions  
denyPermission  
Object owner denies privileges on a securable object.  
permission  
sqlPermissions  
grantPermission  
Object owner grants permission on a securable object.  
permission  
sqlPermissions  
removeAllPermissions  
User drops a securable object.  
securable  
sqlPermissions  
renameSecurable  
User renames a securable object.  
before  
after  
sqlPermissions  
requestPermissions  
User requests permissions on a securable object.  
requests  
sqlPermissions  
revokePermission  
Object owner revokes permissions on their securable object.  
permission  
sqlPermissions  
showPermissions  
User views securable object permissions.  
securable  
principal

SSH events
SSH events
The following ssh events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
ssh  
login  
Agent login of SSH into Spark driver.  
containerId  
userName  
port  
publicKey  
instanceId  
ssh  
logout  
Agent logout of SSH from Spark driver.  
userName  
containerId  
instanceId

Vector search events
Vector search events
The following vectorSearch events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
vectorSearch  
createEndpoint  
User creates a vector search endpoint.  
name  
endpoint_type  
vectorSearch  
deleteEndpoint  
User deletes a vector search endpoint.  
name  
vectorSearch  
createVectorIndex  
User creates a vector search index.  
name  
endpoint_name  
primary_key  
index_type  
delta_sync_index_spec  
direct_access_index_spec  
vectorSearch  
deleteVectorIndex  
User deletes a vector search index.  
name  
endpoint_name  
delete_embedding_writeback_table

Web terminal events
Web terminal events
The following webTerminal events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
webTerminal  
startSession  
User starts a web terminal sessions.  
socketGUID  
clusterId  
serverPort  
ProxyTargetURI  
webTerminal  
closeSession  
User closes a web terminal session.  
socketGUID  
clusterId  
serverPort  
ProxyTargetURI

Workspace events
Workspace events
The following workspace events are logged at the workspace level.  
Service  
Action name  
Description  
Request parameters  
workspace  
changeWorkspaceAcl  
Permissions to the workspace are changed.  
shardName  
targetUserId  
aclPermissionSet  
resourceId  
workspace  
deleteSetting  
A setting is deleted from the workspace.  
settingKeyTypeName  
settingKeyName  
settingTypeName  
settingName  
workspace  
fileCreate  
User creates a file in the workspace.  
path  
workspace  
fileDelete  
User deletes a file in the workspace.  
path  
workspace  
fileEditorOpenEvent  
User opens the file editor.  
notebookId  
path  
workspace  
getRoleAssignment  
User gets a workspace’s user roles.  
account_id  
workspace_id  
workspace  
mintOAuthAuthorizationCode  
Recorded when in-house OAuth authorization code is minted at the workspace level.  
client_id  
workspace  
mintOAuthToken  
OAuth token is minted for workspace.  
grant_type  
scope  
expires_in  
client_id  
workspace  
moveWorkspaceNode  
A workspace admin moves workspace node.  
destinationPath  
path  
workspace  
purgeWorkspaceNodes  
A workspace admin purges workspace nodes.  
treestoreId  
workspace  
reattachHomeFolder  
An existing home folder is re-attached for a user that is re-added to the workspace.  
path  
workspace  
renameWorkspaceNode  
A workspace admin renames workspace nodes.  
path  
destinationPath  
workspace  
unmarkHomeFolder  
Home folder special attributes are removed when a user is removed from the workspace.  
path  
workspace  
updateRoleAssignment  
A workspace admin updates a workspace user’s role.  
account_id  
workspace_id  
principal_id  
workspace  
setSetting  
A workspace admin configures a workspace setting.  
settingKeyTypeName  
settingKeyName  
settingTypeName  
settingName  
settingValueForAudit  
workspace  
workspaceConfEdit  
Workspace admin makes updates to a setting, for example enabling verbose audit logs.  
workspaceConfKeys  
workspaceConfValues  
workspace  
workspaceExport  
User exports a notebook from a workspace.  
workspaceExportDirectDownload  
workspaceExportFormat  
notebookFullPath  
workspace  
workspaceInHouseOAuthClientAuthentication  
OAuth client is authenticated in workspace service.  
user

Billable usage events
Billable usage events
The following accountBillableUsage events are logged at the account level.  
Service  
Action  
Description  
Request parameters  
accountBillableUsage  
getAggregatedUsage  
User accessed aggregated billable usage (usage per day) for the account via the Usage Graph feature.  
account_id  
window_size  
start_time  
end_time  
meter_name  
workspace_ids_filter  
accountBillableUsage  
getDetailedUsage  
User accessed detailed billable usage (usage for each cluster) for the account via the Usage Download feature.  
account_id  
start_month  
end_month  
with_pii

Account-level account events"
22735	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Account-level account events
The following accounts events are logged at the account level.  
Service  
Action  
Description  
Request parameters  
accounts  
accountInHouseOAuthClientAuthentication  
An OAuth client is authenticated.  
endpoint  
accounts  
accountIpAclsValidationFailed  
IP permissions validation fails. Returns statusCode 403.  
sourceIpAddress  
user: logged as an email address  
accounts  
activateUser  
A user is reactivated after being deactivated. See Deactivate users in account.  
targetUserName  
endpoint  
targetUserId  
accounts  
add  
A user is added to the Databricks account.  
targetUserName  
endpoint  
targetUserId  
accounts  
addPrincipalToGroup  
A user is added to an account-level group.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
addPrincipalsToGroup  
Users are added to an account-level group using SCIM provisioning.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
createGroup  
An account-level group is created.  
endpoint  
targetGroupId  
targetGroupName  
accounts  
deactivateUser  
A user is deactivated. See Deactivate users in account.  
targetUserName  
endpoint  
targetUserId  
accounts  
delete  
A user is deleted from the Databricks account.  
targetUserId  
targetUserName  
endpoint  
accounts  
deleteSetting  
Account admin removes a setting from the Databricks account.  
settingKeyTypeName  
settingKeyName  
settingTypeName  
settingName  
settingValueForAudit  
accounts  
garbageCollectDbToken  
A user runs a garbage collect command on expired tokens.  
tokenExpirationTime  
tokenClientId  
userId  
tokenCreationTime  
tokenFirstAccessed  
accounts  
generateDbToken  
User generates a token from User Settings or when the service generates the token.  
tokenExpirationTime  
tokenCreatedBy  
tokenHash  
userId  
accounts  
login  
A user logs into the account console.  
user  
accounts  
logout  
A user logs out of the account console.  
user  
accounts  
mintOAuthAuthorizationCode  
Recorded when in-house OAuth authorization code is minted at the account level.  
client_id  
accounts  
mintOAuthToken  
An account-level OAuth token is issued to the service principal.  
user  
accounts  
oidcBrowserLogin  
A user logs into their account with the OpenID Connect browser workflow.  
user  
accounts  
oidcTokenAuthorization  
An OIDC token is authenticated for an account admin login.  
user  
accounts  
passwordVerifyAuthentication  
A user’s password is verified during account console login.  
user  
accounts  
removeAccountAdmin  
An account admin removes account admin permissions from another user.  
targetUserName  
endpoint  
targetUserId  
accounts  
removeGroup  
A group is removed from the account.  
targetGroupId  
targetGroupName  
endpoint  
accounts  
removePrincipalFromGroup  
A user is removed from an account-level group.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
removePrincipalsFromGroup  
Users are removed from an account-level group using SCIM provisioning.  
targetGroupId  
endpoint  
targetUserId  
targetGroupName  
targetUserName  
accounts  
setAccountAdmin  
An account admin assigns the account admin role to another user.  
targetUserName  
endpoint  
targetUserId  
accounts  
setSetting  
An account admin updates an account-level setting.  
settingKeyTypeName  
settingKeyName  
settingTypeName  
settingName  
settingValueForAudit  
accounts  
tokenLogin  
A user logs into Databricks using a token.  
tokenId  
user  
accounts  
updateUser  
An account admin updates a user account.  
targetUserName  
endpoint  
targetUserId  
accounts  
updateGroup  
An account admin updates an account-level group.  
endpoint  
targetGroupId  
targetGroupName  
accounts  
validateEmail  
When a user validates their email after account creation.  
endpoint  
targetUserName  
targetUserId

Account-level access control events
Account-level access control events
The following accountsAccessControl event is logged at the account level.  
Service  
Action  
Description  
Request parameters  
accountsAccessControl  
updateRuleSet  
When a rule set is changed.  
account_id  
name  
rule_set

Account management events"
22736	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Account management events
The following accountsManager events are logged at the account level. These events have to do with configurations made by account admins in the account console.  
Service  
Action  
Description  
Request parameters  
accountsManager  
acceptTos  
Admin accepts a workspace’s terms of service.  
workspace_id  
accountsManager  
accountUserResetPassword  
Account admin resets a users password. Also logs whether the user changed the password after the reset.  
wasPasswordChanged  
serviceSource  
targetUserId  
userId  
newPasswordSource  
accountsManager  
changeAccountOwner  
Account owner role is transferred to another account admin.  
account_id  
first_name  
last_name  
email  
accountsManager  
consolidateAccounts  
The account was consolidated with another account by Databricks.  
target_account_id  
account_ids_to_consolidate  
accountsManager  
createCredentialsConfiguration  
Account admin created a credentials configuration.  
credentials  
accountsManager  
createCustomerManagedKeyConfiguration  
Account admin created a customer-managed key configuration.  
customer_managed_key  
accountsManager  
createNetworkConfiguration  
Account admin created a network configuration.  
network  
accountsManager  
createPrivateAccessSettings  
Account admin created a private access settings configuration.  
private_access_settings  
accountsManager  
createStorageConfiguration  
Account admin created a storage configuration.  
storage_configuration  
accountsManager  
createVpcEndpoint  
Account admin created a VPC endpoint configuration.  
vpc_endpoint  
accountsManager  
createWorkspaceConfiguration  
Account admin creates a new workspace. The workspace request parameter is an array of deployment information including workspace_name. You can find the workspace_id in the response.result parameter.  
workspace  
accountsManager  
deleteCredentialsConfiguration  
Account admin deleted a credentials configuration.  
account_id  
credentials_id  
accountsManager  
deleteCustomerManagedKeyConfiguration  
Account admin deleted a customer-managed key configuration.  
account_id  
customer_managed_key_id  
accountsManager  
deleteNetworkConfiguration  
Account admin deleted a network configuration.  
account_id  
network_id  
accountsManager  
deletePrivateAccessSettings  
Account admin deleted a private access settings configuration.  
account_id  
private_access_settings_id  
accountsManager  
deleteStorageConfiguration  
Account admin deleted a storage configuration.  
account_id  
storage_configuration_id  
accountsManager  
deleteVpcEndpoint  
Account admin deleted a VPC endpoint configuration.  
account_id  
vpc_endpoint_id  
accountsManager  
deleteWorkspaceConfiguration  
Account admin deleted a workspace.  
account_id  
workspace_id  
accountsManager  
getCredentialsConfiguration  
Account admin requests details about a credentials configuration.  
account_id  
credentials_id  
accountsManager  
getCustomerManagedKeyConfiguration  
Account admin requests details about a customer-managed key configuration.  
account_id  
customer_managed_key_id  
accountsManager  
getNetworkConfiguration  
Account admin requests details about a network configuration.  
account_id  
network_id  
accountsManager  
getPrivateAccessSettings  
Account admin requests details about a private access settings configuration.  
account_id  
private_access_settings_id  
accountsManager  
getStorageConfiguration  
Account admin requests details about a storage configuration.  
account_id  
storage_configuration_id  
accountsManager  
getVpcEndpoint  
Account admin requests details about a VPC endpoint configuration.  
account_id  
vpc_endpoint_id  
accountsManager  
getWorkspaceConfiguration  
Account admin requests details about a workspace.  
account_id  
workspace_id  
accountsManager  
listCredentialsConfigurations  
Account admin lists all credentials configurations in the account.  
account_id  
accountsManager  
listCustomerManagedKeyConfigurations  
Account admin lists all customer-managed key configurations in the account.  
account_id  
accountsManager  
listNetworkConfigurations  
Account admin lists all network configurations in the account.  
account_id  
accountsManager  
listPrivateAccessSettings  
Account admin lists all private access settings configurations in the account.  
account_id  
accountsManager  
listStorageConfigurations  
Account admin lists all storage configurations in the account.  
account_id  
accountsManager  
listSubscriptions  
Account admin lists all account billing subscriptions.  
account_id  
accountsManager  
listVpcEndpoints  
Account admin listed all VPC endpoint configurations for the account.  
account_id  
accountsManager  
listWorkspaceConfigurations  
Account admin lists all workspace in the account.  
account_id  
accountsManager  
listWorkspaceEncryptionKeyRecords  
Account admin lists all encryption key records in a specific workspace.  
account_id  
workspace_id  
accountsManager  
listWorkspaceEncryptionKeyRecordsForAccount  
Account admin lists all encryption key records in the account.  
account_id  
accountsManager  
sendTos  
An email was sent to a workspace admin to accept the Databricks Terms of Service.  
account_id  
workspace_id  
accountsManager  
updateAccount  
The account details were changed internally.  
account_id  
account  
accountsManager  
updateSubscription  
The account billing subscriptions were updated.  
account_id  
subscription_id  
subscription  
accountsManager  
updateWorkspaceConfiguration  
Admin updated the configuration for a workspace.  
account_id  
workspace_id

Log delivery events"
22737	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Log delivery events
The following logDelivery events are logged at the account level.  
Service  
Action  
Description  
Request parameters  
logDelivery  
createLogDeliveryConfiguration  
Admin created a log delivery configuration.  
account_id  
config_id  
logDelivery  
getLogDeliveryConfiguration  
Admin requested details about a log delivery configuration.  
log_delivery_configuration  
logDelivery  
listLogDeliveryConfigurations  
Admin listed all log delivery configurations in the account.  
account_id  
storage_configuration_id  
credentials_id  
status  
logDelivery  
updateLogDeliveryConfiguration  
Admin updated a log delivery configuration.  
config_id  
account_id  
status

Oauth SSO events
Oauth SSO events
The following oauth2 events are logged at the account level and are related to OAuth SSO authentication to the account console.  
Service  
Action  
Description  
Request parameters  
oauth2  
createCustomAppIntegration  
A workspace admin creates custom app integration.  
redirect_url  
name  
token_access_policy  
confidential  
oauth2  
createPublishedAppIntegration  
A workspace admin creates an app integration using a published app integration.  
app_id  
oauth2  
deleteCustomAppIntegration  
A workspace admin deletes custom app integration.  
integration_id  
oauth2  
deletePublishedAppIntegration  
A workspace admin deletes published app integration.  
integration_id  
oauth2  
enrollOAuth  
A workspace admin enrolls account in OAuth.  
enable_all_published_apps  
oauth2  
updateCustomAppIntegration  
A workspace admin updates custom app integration.  
redirect_url  
name  
token_access_policy  
confidential  
oauth2  
updatePublishedAppIntegration  
A workspace admin updates published app integration.  
token_access_policy

Service principal credentials events (Public Preview)
Service principal credentials events (Public Preview)
The following servicePrincipalCredentials events are logged at the account level.  
Service  
Action  
Description  
Request parameters  
servicePrincipalCredentials  
create  
Account admin generates an OAuth secret for the service principal.  
account_id  
service_principal  
secret_id  
servicePrincipalCredentials  
list  
Account admin lists all OAuth secrets under a service principal.  
account_id  
service_principal  
servicePrincipalCredentials  
delete  
Account admin deletes a service principal’s OAuth secret.  
account_id  
service_principal  
secret_id

Single-sign on events
Single-sign on events
The following ssoConfigBackend events are logged at the account level and are related to SSO authentication for the account console.  
Service  
Action  
Description  
Request parameters  
ssoConfigBackend  
create  
Account admin created an account console SSO configuration.  
account_id  
sso_type  
config  
ssoConfigBackend  
get  
Account admin requested details about an account console SSO configuration.  
account_id  
sso_type  
ssoConfigBackend  
update  
Account admin updated an account console SSO configuration.  
account_id  
sso_type  
config

Unity Catalog events"
22738	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Unity Catalog events
The following audit events are related to Unity Catalog. Delta Sharing events are also logged under the unityCatalog service. For Delta Sharing events, see Delta Sharing events. Unity Catalog audit events can be logged at the workspace level or account level depending on the event.  
Service  
Action  
Description  
Request parameters  
unityCatalog  
createMetastore  
Account admin creates a metastore.  
name  
storage_root  
workspace_id  
metastore_id  
unityCatalog  
getMetastore  
Account admin requests metastore ID.  
id  
workspace_id  
metastore_id  
unityCatalog  
getMetastoreSummary  
Account admin requests details about a metastore.  
workspace_id  
metastore_id  
unityCatalog  
listMetastores  
Account admin requests a list of all metastores in an account.  
workspace_id  
unityCatalog  
updateMetastore  
Account admin makes an update to a metastore.  
id  
owner  
workspace_id  
metastore_id  
unityCatalog  
deleteMetastore  
Account admin deletes a metastore.  
id  
force  
workspace_id  
metastore_id  
unityCatalog  
updateMetastoreAssignment  
Account admin makes an update to a metastore’s workspace assignment.  
workspace_id  
metastore_id  
default_catalog_name  
unityCatalog  
createExternalLocation  
Account admin creates an external location.  
name  
skip_validation  
url  
credential_name  
workspace_id  
metastore_id  
unityCatalog  
getExternalLocation  
Account admin requests details about an external location.  
name_arg  
include_browse  
workspace_id  
metastore_id  
unityCatalog  
listExternalLocations  
Account admin request list of all external locations in an account.  
url  
max_results  
workspace_id  
metastore_id  
unityCatalog  
updateExternalLocation  
Account admin makes an update to an external location.  
name_arg  
owner  
workspace_id  
metastore_id  
unityCatalog  
deleteExternalLocation  
Account admin deletes an external location.  
name_arg  
force  
workspace_id  
metastore_id  
unityCatalog  
createCatalog  
User creates a catalog.  
name  
comment  
workspace_id  
metastore_id  
unityCatalog  
deleteCatalog  
User deletes a catalog.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
getCatalog  
User requests details about a catalog.  
name_arg  
dependent  
workspace_id  
metastore_id  
unityCatalog  
updateCatalog  
User updates a catalog.  
name_arg  
isolation_mode  
comment  
workspace_id  
metastore_id  
unityCatalog  
listCatalog  
User makes a call to list all catalogs in the metastore.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
createSchema  
User creates a schema.  
name  
catalog_name  
comment  
workspace_id  
metastore_id  
unityCatalog  
deleteSchema  
User deletes a schema.  
full_name_arg  
force  
workspace_id  
metastore_id  
unityCatalog  
getSchema  
User requests details about a schema.  
full_name_arg  
dependent  
workspace_id  
metastore_id  
unityCatalog  
listSchema  
User requests a list of all schemas in a catalog.  
catalog_name  
unityCatalog  
updateSchema  
User updates a schema.  
full_name_arg  
name  
workspace_id  
metastore_id  
comment  
unityCatalog  
createStagingTable  
name  
catalog_name  
schema_name  
workspace_id  
metastore_id  
unityCatalog  
createTable  
User creates a table. The request parameters differ depending on the type of table created.  
name  
data_source_format  
catalog_name  
schema_name  
storage_location  
columns  
dry_run  
table_type  
view_dependencies  
view_definition  
sql_path  
comment  
unityCatalog  
deleteTable  
User deletes a table.  
full_name_arg  
workspace_id  
metastore_id  
unityCatalog  
getTable  
User requests details about a table.  
include_delta_metadata  
full_name_arg  
dependent  
workspace_id  
metastore_id  
unityCatalog  
privilegedGetTable  
full_name_arg  
unityCatalog  
listTables  
User makes a call to list all tables in a schema.  
catalog_name  
schema_name  
workspace_id  
metastore_id  
include_browse  
unityCatalog  
listTableSummaries  
User gets an array of summaries for tables for a schema and catalog within the metastore.  
catalog_name  
schema_name_pattern  
workspace_id  
metastore_id  
unityCatalog  
updateTables  
User makes an update to a table. The request parameters displayed vary depending on the type of table updates made.  
full_name_arg  
table_type  
table_constraint_list  
data_source_format  
columns  
dependent  
row_filter  
storage_location  
sql_path  
view_definition  
view_dependencies  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
createStorageCredential  
Account admin creates a storage credential. You might see an additional request parameter based on your cloud provider credentials.  
name  
comment  
workspace_id  
metastore_id  
unityCatalog  
listStorageCredentials  
Account admin makes a call to list all storage credentials in the account.  
workspace_id  
metastore_id  
unityCatalog  
getStorageCredential  
Account admin requests details about a storage credential.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
updateStorageCredential  
Account admin makes an update to a storage credential.  
name_arg  
owner  
workspace_id  
metastore_id  
unityCatalog  
deleteStorageCredential  
Account admin deletes a storage credential.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
generateTemporaryTableCredential  
Logged whenever a temporary credential is granted for a table. You can use this event to determine who queried what and when.  
credential_id  
credential_type  
is_permissions_enforcing_client  
table_full_name  
operation  
table_id  
workspace_id  
table_url  
metastore_id  
unityCatalog  
generateTemporaryPathCredential  
Logged whenever a temporary credential is granted for a path.  
url  
operation  
make_path_only_parent  
workspace_id  
metastore_id  
unityCatalog  
getPermissions  
User makes a call to get permission details for a securable object. This call doesn’t return inherited permissions, only explicitly assigned permissions.  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
unityCatalog  
getEffectivePermissions  
User makes a call to get all permission details for a securable object. An effective permissions call returns both explicitly assigned and inherited permissions.  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
unityCatalog  
updatePermissions  
User updates permissions on a securable object.  
securable_type  
changes  
securable_full_name  
workspace_id  
metastore_id  
unityCatalog  
metadataSnapshot"
22739	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"unityCatalog  
updatePermissions  
User updates permissions on a securable object.  
securable_type  
changes  
securable_full_name  
workspace_id  
metastore_id  
unityCatalog  
metadataSnapshot  
User queries the metadata from a previous table version.  
securables  
include_delta_metadata  
workspace_id  
metastore_id  
unityCatalog  
metadataAndPermissionsSnapshot  
User queries the metadata and permissions from a previous table version.  
securables  
include_delta_metadata  
workspace_id  
metastore_id  
unityCatalog  
updateMetadataSnapshot  
User updates the metadata from a previous table version.  
table_list_snapshots  
schema_list_snapshots  
workspace_id  
metastore_id  
unityCatalog  
getForeignCredentials  
User makes a call to get details about a foreign key.  
securables  
workspace_id  
metastore_id  
unityCatalog  
getInformationSchema  
User makes a call to get details about a schema.  
table_name  
page_token  
required_column_names  
row_set_type  
required_column_names  
workspace_id  
metastore_id  
unityCatalog  
createConstraint  
User creates a constraint for a table.  
full_name_arg  
constraint  
workspace_id  
metastore_id  
unityCatalog  
deleteConstraint  
User deletes a constraint for a table.  
full_name_arg  
constraint  
workspace_id  
metastore_id  
unityCatalog  
createPipeline  
User creates a Unity Catalog pipeline.  
target_catalog_name  
has_workspace_definition  
id  
workspace_id  
metastore_id  
unityCatalog  
updatePipeline  
User updates a Unity Catalog pipeline.  
id_arg  
definition_json  
id  
workspace_id  
metastore_id  
unityCatalog  
getPipeline  
User requests details about a Unity Catalog pipeline.  
id  
workspace_id  
metastore_id  
unityCatalog  
deletePipeline  
User deletes a Unity Catalog pipeline.  
id  
workspace_id  
metastore_id  
unityCatalog  
deleteResourceFailure  
Resource fails to delete  
none  
unityCatalog  
createVolume  
User creates a Unity Catalog volume.  
name  
catalog_name  
schema_name  
volume_type  
storage_location  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
getVolume  
User makes a call to get information on a Unity Catalog volume.  
volume_full_name  
workspace_id  
metastore_id  
unityCatalog  
updateVolume  
User updates a Unity Catalog volume’s metadata with the ALTER VOLUME or COMMENT ON calls.  
volume_full_name  
name  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
deleteVolume  
User deletes a Unity Catalog volume.  
volume_full_name  
workspace_id  
metastore_id  
unityCatalog  
listVolumes  
User makes a call to get a list of all Unity Catalog volumes in a schema.  
catalog_name  
schema_name  
workspace_id  
metastore_id  
unityCatalog  
generateTemporaryVolumeCredential  
A temporary credential is generated when a user performs a read or write on a volume. You can use this event to determine who accessed a volume and when.  
volume_id  
volume_full_name  
operation  
volume_storage_location  
credential_id  
credential_type  
workspace_id  
metastore_id  
unityCatalog  
getTagSecurableAssignments  
Tag assignments for a securable are fetched  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
unityCatalog  
getTagSubentityAssignments  
Tag assignments for a subentity are fetched  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
subentity_name  
unityCatalog  
UpdateTagSecurableAssignments  
Tag assignments for a securable are updated  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
changes  
unityCatalog  
UpdateTagSubentityAssignments  
Tag assignments for a subentity are updated  
securable_type  
securable_full_name  
workspace_id  
metastore_id  
subentity_name  
changes  
unityCatalog  
createRegisteredModel  
User creates a Unity Catalog registered model.  
name  
catalog_name  
schema_name  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
getRegisteredModel  
User makes a call to get information on a Unity Catalog registered model.  
full_name_arg  
workspace_id  
metastore_id  
unityCatalog  
updateRegisteredModel  
User updates a Unity Catalog registered model’s metadata.  
full_name_arg  
name  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
deleteRegisteredModel  
User deletes a Unity Catalog registered model.  
full_name_arg  
workspace_id  
metastore_id  
unityCatalog  
listRegisteredModels  
User makes a call to get a list of Unity Catalog registered models in a schema, or list models across catalogs and schemas.  
catalog_name  
schema_name  
max_results  
page_token  
workspace_id  
metastore_id  
unityCatalog  
createModelVersion  
User creates a model version in Unity Catalog.  
catalog_name  
schema_name  
model_name  
source  
comment  
workspace_id  
metastore_id  
unityCatalog  
finalizeModelVersion  
User makes a call to “finalize” a Unity Catalog model version after uploading model version files to its storage location, making it read-only and usable in inference workflows.  
full_name_arg  
version_arg  
workspace_id  
metastore_id  
unityCatalog  
getModelVersion  
User makes a call to get details on a model version.  
full_name_arg  
version_arg  
workspace_id  
metastore_id  
unityCatalog  
getModelVersionByAlias  
User makes a call to get details on a model version using the alias.  
full_name_arg  
include_aliases  
alias_arg  
workspace_id  
metastore_id  
unityCatalog  
updateModelVersion  
User updates a model version’s metadata.  
full_name_arg  
version_arg  
name  
owner  
comment  
workspace_id  
metastore_id  
unityCatalog  
deleteModelVersion  
User deletes a model version.  
full_name_arg  
version_arg  
workspace_id  
metastore_id  
unityCatalog  
listModelVersions  
User makes a call to get a list of Unity Catalog model versions in a registered model.  
catalog_name  
schema_name  
model_name  
max_results  
page_token  
workspace_id  
metastore_id  
unityCatalog  
generateTemporaryModelVersionCredential  
A temporary credential is generated when a user performs a write (during initial model version creaiton) or read (after the model version has been finalized) on a model version. You can use this event to determine who accessed a model version and when.  
full_name_arg  
version_arg  
operation  
model_version_url  
credential_id  
credential_type  
workspace_id  
metastore_id  
unityCatalog"
22740	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"full_name_arg  
version_arg  
operation  
model_version_url  
credential_id  
credential_type  
workspace_id  
metastore_id  
unityCatalog  
setRegisteredModelAlias  
User sets an alias on a Unity Catalog registered model.  
full_name_arg  
alias_arg  
version  
unityCatalog  
deleteRegisteredModelAlias  
User deletes an alias on a Unity Catalog registered model.  
full_name_arg  
alias_arg  
unityCatalog  
getModelVersionByAlias  
User gets a Unity Catalog model version by alias.  
full_name_arg  
alias_arg  
unityCatalog  
createConnection  
A new foreign connection is created.  
name  
connection_type  
workspace_id  
metastore_id  
unityCatalog  
deleteConnection  
A foreign connection is deleted.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
getConnection  
A foreign connection is retrieved.  
name_arg  
workspace_id  
metastore_id  
unityCatalog  
updateConnection  
A foreign connection is updated.  
name_arg  
owner  
workspace_id  
metastore_id  
unityCatalog  
listConnections  
Foreign connections in a metastore are listed.  
workspace_id  
metastore_id  
unityCatalog  
createFunction  
User creates a new function.  
function_info  
workspace_id  
metastore_id  
unityCatalog  
updateFunction  
User updates a function.  
full_name_arg  
owner  
workspace_id  
metastore_id  
unityCatalog  
listFunctions  
User requests a list of all functions within a specific parent catalog or schema.  
catalog_name  
schema_name  
include_browse  
workspace_id  
metastore_id  
unityCatalog  
getFunction  
User requests a function from a parent catalog or schema.  
full_name_arg  
workspace_id  
metastore_id  
unityCatalog  
deleteFunction  
User requests a function from a parent catalog or schema.  
full_name_arg  
workspace_id  
metastore_id  
unityCatalog  
createShareMarketplaceListingLink  
links_infos  
metastore_id  
unityCatalog  
deleteShareMarketplaceListingLink  
links_infos  
metastore_id"
22741	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Delta Sharing events
Delta Sharing events are broken up into two sections: events recorded in the data provider’s account and events recorded in the data recipient’s account.  
Delta Sharing provider events  
The following audit log events are logged in the provider’s account. Actions that are performed by recipients start with the deltaSharing prefix. Each of these logs also includes request_params.metastore_id, which is the metastore that manages the shared data, and userIdentity.email, which is the ID of the user who initiated the activity.  
Service  
Action  
Description  
Request parameters  
unityCatalog  
deltaSharingListShares  
A data recipient requests a list of shares.  
options: The pagination options provided with this request.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingGetShare  
A data recipient requests details about a shares.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListSchemas  
A data recipient requests a list of shared schemas.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
options: The pagination options provided with this request.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListAllTables  
A data recipient requests a list of all shared tables.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListTables  
A data recipient requests a list of shared tables.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
options: The pagination options provided with this request.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingGetTableMetadata  
A data recipient requests a details about a table’s metadata.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
schema: The name of the schema.  
name: The name of the table.  
predicateHints: The predicates included in the query.  
limitHints: The maximum number of rows to return.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingGetTableVersion  
A data recipient requests a details about a table version.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
schema: The name of the schema.  
name: The name of the table.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingQueryTable  
Logged when a data recipient queries a shared table.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
schema: The name of the schema.  
name: The name of the table.  
predicateHints: The predicates included in the query.  
limitHints: The maximum number of rows to return.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingQueryTableChanges  
Logged when a data recipient queries change data for a table.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
schema: The name of the schema.  
name: The name of the table.  
cdf_options: Change data feed options.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingQueriedTable  
Logged after a data recipient gets a response to their query. The response.result field includes more information on the recipient’s query (see Audit and monitor data sharing)  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingQueriedTableChanges  
Logged after a data recipient gets a response to their query. The response.result field includes more information on the recipient’s query (see Audit and monitor data sharing).  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListNotebookFiles  
A data recipient requests a list of shared notebook files.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingQueryNotebookFile  
A data recipient queries a shared notebook file.  
file_name: The name of the notebook file.  
recipient_name: Indicates the recipient executing the action."
22742	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"unityCatalog  
deltaSharingQueryNotebookFile  
A data recipient queries a shared notebook file.  
file_name: The name of the notebook file.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListFunctions  
A data recipient requests a list of functions in a parent schema.  
share: The name of the share.  
schema: The name of the parent schema of the function.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListAllFunctions  
A data recipient requests a list of all shared functions.  
share: The name of the share.  
schema: The name of the parent schema of the function.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListFunctionVersions  
A data recipient requests a list of function versions.  
share: The name of the share.  
schema: The name of the parent schema of the function.  
function: The name of the function.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListVolumes  
A data recipient requests a list of shared volumes in a schema.  
share: The name of the share.  
schema: The parents schema of the volumes.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
deltaSharingListAllVolumes  
A data recipient requests all shared volumes.  
share: The name of the share.  
recipient_name: Indicates the recipient executing the action.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
updateMetastore  
Provider updates their metastore.  
delta_sharing_scope: Values can be INTERNAL or INTERNAL_AND_EXTERNAL.  
delta_sharing_recipient_token_lifetime_in_seconds: If present, indicates that the recipient token lifetime was updated.  
unityCatalog  
createRecipient  
Provider creates a data recipient.  
name: The name of the recipient.  
comment: The comment for the recipient.  
ip_access_list.allowed_ip_addresses: Recipient IP address allowlist.  
unityCatalog  
deleteRecipient  
Provider deletes a data recipient.  
name: The name of the recipient.  
unityCatalog  
getRecipient  
Provider requests details about a data recipient.  
name: The name of the recipient.  
unityCatalog  
listRecipients  
Provider requests a list of all their data recipients.  
none  
unityCatalog  
rotateRecipientToken  
Provider rotates a recipient’s token.  
name: The name of the recipient.  
comment: The comment given in the rotation command.  
unityCatalog  
updateRecipient  
Provider updates a data recipient’s attributes.  
name: The name of the recipient.  
updates: A JSON representation of recipient attributes that were added or removed from the share.  
unityCatalog  
createShare  
Provider updates a data recipient’s attributes.  
name: The name of the share.  
comment: The comment for the share.  
unityCatalog  
deleteShare  
Provider updates a data recipient’s attributes.  
name: The name of the share.  
unityCatalog  
getShare  
Provider requests details about a share.  
name: The name of the share.  
include_shared_objects: Whether the share’s table names were included in the request.  
unityCatalog  
updateShare  
Provider adds or removes data assets from a share.  
name: The name of the share.  
updates: A JSON representation of data assets that were added or removed from the share. Each item includes action (add or remove), name (the actual name of the table), shared_as (the name the asset was shared as, if different from the actual name), and partition_specification (if a partition specification was provided).  
unityCatalog  
listShares  
Provider requests a list of their shares.  
none  
unityCatalog  
getSharePermissions  
Provider requests details on a share’s permissions.  
name: The name of the share.  
unityCatalog  
updateSharePermissions  
Provider updates a share’s permissions.  
name: The name of the share.  
changes: A JSON representation of the updated permissions. Each change includes principal (the user or group to whom permission is granted or revoked), add (the list of permissions that were granted), and remove (the list of permissions that were revoked).  
unityCatalog  
getRecipientSharePermissions  
Provider requests details about a recipient’s share permissions.  
name: The name of the share.  
unityCatalog  
getActivationUrlInfo  
Provider requests details about activity on their activation link.  
recipient_name: The name of the recipient who opened the activation URL.  
is_ip_access_denied: None if there is no IP access list configured. Otherwise, true if the request was denied and false if the request was not denied. sourceIPaddress is the recipient IP address.  
unityCatalog  
generateTemporaryVolumeCredential  
Temporary credential is generated for the recipient to access a shared volume.  
share_name: The name of the share through which the recipient requests.  
share_id: The ID of the share.  
share_owner: The owner of the share.  
recipient_name: The name of the recipient who requests the credential.  
recipient_id: The ID of the recipient.  
volume_full_name: The full 3-level name of the volume.  
volume_id: The ID of the volume.  
volume_storage_location: The cloud path of the volume root.  
operation: Either READ_VOLUME or WRITE_VOLUME. For volume sharing, only READ_VOLUME is supported.  
credential_id: The ID of the credential."
22743	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"volume_storage_location: The cloud path of the volume root.  
operation: Either READ_VOLUME or WRITE_VOLUME. For volume sharing, only READ_VOLUME is supported.  
credential_id: The ID of the credential.  
credential_type: The type of the credential. Value is always StorageCredential.  
workspace_id: Value is always 0 when the request is for shared volumes.  
unityCatalog  
generateTemporaryTableCredential  
Temporary credential is generated for the recipient to access a shared table.  
share_name: The name of the share through which the recipient requests.  
share_id: The ID of the share.  
share_owner: The owner of the share.  
recipient_name: The name of the recipient who requests the credential.  
recipient_id: The ID of the recipient.  
table_full_name: The full 3-level name of the table.  
table_id: The ID of the table.  
table_url: The cloud path of the table root.  
operation: Either READ or READ_WRITE.  
credential_id: The ID of the credential.  
credential_type: The type of the credential. Value is always StorageCredential.  
workspace_id: Value is always 0 when the request is for shared tables.  
Delta Sharing recipient events  
The following events are logged in the data recipient’s account. These events record recipient access of shared data and AI assets, along with events associated with the management of providers. Each of these events also includes the following request parameters:  
recipient_name: The name of the recipient in the data provider’s system.  
metastore_id: The name of the metastore in the data provider’s system.  
sourceIPAddress: The IP address where the request originated.  
Service  
Action  
Description  
Request parameters  
unityCatalog  
deltaSharingProxyGetTableVersion  
A data recipient requests a details on a shared table version.  
share: The name of the share.  
schema: The name of the table’s parent schema.  
name: The name of the table.  
unityCatalog  
deltaSharingProxyGetTableMetadata  
A data recipient requests a details on a shared table’s metadata.  
share: The name of the share.  
schema: The name of the table’s parent schema.  
name: The name of the table.  
unityCatalog  
deltaSharingProxyQueryTable  
A data recipient queries a shared table.  
share: The name of the share.  
schema: The name of the table’s parent schema.  
name: The name of the table.  
limitHints: The maximum number of rows to return.  
predicateHints: The predicates included in the query.  
version: Table version, if change data feed is enabled.  
unityCatalog  
deltaSharingProxyQueryTableChanges  
A data recipient queries change data for a table.  
share: The name of the share.  
schema: The name of the table’s parent schema.  
name: The name of the table.  
cdf_options: Change data feed options.  
unityCatalog  
createProvider  
A data recipient creates a provider object.  
name: The name of the provider.  
comment: The comment for the provider.  
unityCatalog  
updateProvider  
A data recipient updates a provider object.  
name: The name of the provider.  
updates: A JSON representation of provider attributes that were added or removed from the share. Each item includes action (add or remove) and can include name (the new provider name), owner (new owner), and comment.  
unityCatalog  
deleteProvider  
A data recipient deletes a provider object.  
name: The name of the provider.  
unityCatalog  
getProvider  
A data recipient requests details about a provider object.  
name: The name of the provider.  
unityCatalog  
listProviders  
A data recipient requests a list of providers.  
none  
unityCatalog  
activateProvider  
A data recipient activates a provider object.  
name: The name of the provider.  
unityCatalog  
listProviderShares  
A data recipient requests a list of a provider’s shares.  
name: The name of the provider.  
unityCatalog  
generateTemporaryVolumeCredential  
Temporary credential is generated for the recipient to access a shared volume.  
share_name: The name of the share through which the recipient requests.  
volume_full_name: The full 3-level name of the volume.  
volume_id: The ID of the volume.  
operation: Either READ_VOLUME or WRITE_VOLUME. For volume sharing, only READ_VOLUME is supported.  
workspace_id: The ID of the workspace that receives the user request.  
unityCatalog  
generateTemporaryTableCredential  
Temporary credential is generated for the recipient to access a shared table.  
share_name: The name of the share through which the recipient requests.  
table_full_name: The full 3-level name of the table.  
table_id: The ID of the table.  
operation: Either READ or READ_WRITE.  
workspace_id: The ID of the workspace that receives the user request."
22744	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Additional security monitoring events
For Databricks compute resources in the classic compute plane, such as VMs for clusters and pro or classic SQL warehouses, the following features enable additional monitoring agents:  
Enhanced security monitoring  
Compliance security profile. The compliance security profile is required for the compliance controls for PCI-DSS, HIPAA, and FedRAMP Moderate.  
For serverless SQL warehouses, the monitoring agents run if the compliance security profile is enabled and the region supports serverless SQL warehouses with the compliance security profile.  
File integrity monitoring events  
The following capsule8-alerts-dataplane events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
capsule8-alerts-dataplane  
Heartbeat  
A regular event to confirm the monitor is on. Currently runs every 10 minutes.  
instanceId  
capsule8-alerts-dataplane  
Memory Marked Executable  
Memory is often marked executable in order to allow malicious code to execute when an application is being exploited. Alerts when a program sets heap or stack memory permissions to executable. This can cause false positives for certain application servers.  
instanceId  
capsule8-alerts-dataplane  
File Integrity Monitor  
Monitors the integrity of important system files. Alerts on any unauthorized changes to those files. Databricks defines specific sets of system paths on the image, and this set of paths might change over time.  
instanceId  
capsule8-alerts-dataplane  
Systemd Unit File Modified  
Changes to systemd units could result in security controls being relaxed or disabled, or the installation of a malicious service. Alerts whenever a systemd unit file is modified by a program other than systemctl.  
instanceId  
capsule8-alerts-dataplane  
Repeated Program Crashes  
Repeated program crashes could indicate that an attacker is attempting to exploit a memory corruption vulnerability, or that there is a stability issue in the affected application. Alerts when more than 5 instances of an individual program crash via segmentation fault.  
instanceId  
capsule8-alerts-dataplane  
Userfaultfd Usage  
As containers are typically static workloads, this alert could indicate that an attacker has compromised the container and is attempting to install and run a backdoor. Alerts when a file that has been created or modified within 30 minutes is then executed within a container.  
instanceId  
capsule8-alerts-dataplane  
New File Executed in Container  
Memory is often marked executable in order to allow malicious code to execute when an application is being exploited. Alerts when a program sets heap or stack memory permissions to executable. This can cause false positives for certain application servers.  
instanceId  
capsule8-alerts-dataplane  
Suspicious Interactive Shell  
Interactive shells are rare occurrences on modern production infrastructure. Alerts when an interactive shell is started with arguments commonly used for reverse shells.  
instanceId  
capsule8-alerts-dataplane  
User Command Logging Evasion  
Evading command logging is common practice for attackers, but might also indicate that a legitimate user is performing unauthorized actions or trying to evade policy. Alerts when a change to user command history logging is detected, indicating that a user is attempting to evade command logging.  
instanceId  
capsule8-alerts-dataplane  
BPF Program Executed  
Detects some types of kernel backdoors. The loading of a new Berkeley Packet Filter (BPF) program could indicate that an attacker is loading a BPF-based rootkit to gain persistence and avoid detection. Alerts when a process loads a new privileged BPF program, if the process that is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Kernel Module Loaded  
Attackers commonly load malicious kernel modules (rootkits) to evade detection and maintain persistence on a compromised node. Alerts when a kernel module is loaded, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Suspicious Program Name Executed-Space After File  
Attackers might create or rename malicious binaries to include a space at the end of the name in an effort to impersonate a legitimate system program or service. Alerts when a program is executed with a space after the program name.  
instanceId  
capsule8-alerts-dataplane  
Illegal Elevation Of Privileges  
Kernel privilege escalation exploits commonly enable an unprivileged user to gain root privileges without passing standard gates for privilege changes. Alerts when a program attempts to elevate privileges through unusual means. This can issue false positive alerts on nodes with significant workloads.  
instanceId  
capsule8-alerts-dataplane  
Kernel Exploit  
Internal kernel functions are not accessible to regular programs, and if called, are a strong indicator that a kernel exploit has executed and that the attacker has full control of the node. Alerts when a kernel function unexpectedly returns to user space.  
instanceId  
capsule8-alerts-dataplane  
Processor-Level Protections Disabled  
SMEP and SMAP are processor-level protections that increase difficulty for kernel exploits to succeed, and disabling these restrictions is a common early step in kernel exploits. Alerts when a program tampers with the kernel SMEP/SMAP configuration.  
instanceId  
capsule8-alerts-dataplane  
Container Escape via Kernel Exploitation  
Alerts when a program uses kernel functions commonly used in container escape exploits, indicating that an attacker is escalating privileges from container-access to node-access.  
instanceId  
capsule8-alerts-dataplane  
Privileged Container Launched  
Privileged containers have direct access to host resources, leading to a greater impact when compromised. Alerts when a privileged container is launched, if the container isn’t a known privileged image such as kube-proxy. This can issue unwanted alerts for legitimate privileged containers.  
instanceId  
capsule8-alerts-dataplane  
Userland Container Escape  
Many container escapes coerce the host to execute an in-container binary, resulting in the attacker gaining full control of the affected node. Alerts when a container-created file is executed from outside a container.  
instanceId  
capsule8-alerts-dataplane  
AppArmor Disabled In Kernel  
Modification of certain AppArmor attributes can only occur in-kernel, indicating that AppArmor has been disabled by a kernel exploit or rootkit. Alerts when the AppArmor state is changed from the AppArmor configuration detected when the sensor starts.  
instanceId  
capsule8-alerts-dataplane  
AppArmor Profile Modified  
Attackers might attempt to disable enforcement of AppArmor profiles as part of evading detection. Alerts when a command for modifying an AppArmor profile is executed, if it was not executed by a user in an SSH session.  
instanceId  
capsule8-alerts-dataplane  
Boot Files Modified  
If not performed by a trusted source (such as a package manager or configuration management tool), modification of boot files could indicate an attacker modifying the kernel or its options in order to gain persistent access to a host. Alerts when changes are made to files in /boot, indicating installation of a new kernel or boot configuration.  
instanceId  
capsule8-alerts-dataplane  
Log Files Deleted"
22745	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"instanceId  
capsule8-alerts-dataplane  
Log Files Deleted  
Log deletion not performed by a log management tool could indicate that an attacker is trying to remove indicators of compromise. Alerts on deletion of system log files.  
instanceId  
capsule8-alerts-dataplane  
New File Executed  
Newly created files from sources other than system update programs might be backdoors, kernel exploits, or part of an exploitation chain. Alerts when a file that has been created or modified within 30 minutes is then executed, excluding files created by system update programs.  
instanceId  
capsule8-alerts-dataplane  
Root Certificate Store Modified  
Modification of the root certificate store could indicate the installation of a rogue certificate authority, enabling interception of network traffic or bypass of code signature verification. Alerts when a system CA certificate store is changed.  
instanceId  
capsule8-alerts-dataplane  
Setuid/Setgid Bit Set On File  
Setting setuid/setgid bits can be used to provide a persistent method for privilege escalation on a node. Alerts when the setuid or setgid bit is set on a file with the chmod family of system calls.  
instanceId  
capsule8-alerts-dataplane  
Hidden File Created  
Attackers often create hidden files as a means of obscuring tools and payloads on a compromised host. Alerts when a hidden file is created by a process associated with an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Modification Of Common System Utilities  
Attackers might modify system utilities in order to execute malicious payloads whenever these utilities are run. Alerts when a common system utility is modified by an unauthorized process.  
instanceId  
capsule8-alerts-dataplane  
Network Service Scanner Executed  
An attacker or rogue user might use or install these programs to survey connected networks for additional nodes to compromise. Alerts when common network scanning program tools are executed.  
instanceId  
capsule8-alerts-dataplane  
Network Service Created  
Attackers might start a new network service to provide easy access to a host after compromise. Alerts when a program starts a new network service, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Network Sniffing Program Executed  
An attacker or rogue user might execute network sniffing commands to capture credentials, personally-identifiable information (PII), or other sensitive information. Alerts when a program is executed that allows network capture.  
instanceId  
capsule8-alerts-dataplane  
Remote File Copy Detected  
Use of file transfer tools could indicate that an attacker is attempting to move toolsets to additional hosts or exfiltrate data to a remote system. Alerts when a program associated with remote file copying is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Unusual Outbound Connection Detected  
Command and Control channels and cryptocoin miners often create new outbound network connections on unusual ports. Alerts when a program initiates a new connection on an uncommon port, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Data Archived Via Program  
After gaining access to a system, an attacker might create a compressed archive of files to reduce the size of data for exfiltration. Alerts when a data compression program is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Process Injection  
Use of process injection techniques commonly indicates that a user is debugging a program, but might also indicate that an attacker is reading secrets from or injecting code into other processes. Alerts when a program uses ptrace (debugging) mechanisms to interact with another process.  
instanceId  
capsule8-alerts-dataplane  
Account Enumeration Via Program  
Attackers often use account enumeration programs to determine their level of access and to see if other users are currently logged in to the node. Alerts when a program associated with account enumeration is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
File and Directory Discovery Via Program  
Exploring file systems is common post-exploitation behavior for an attacker looking for credentials and data of interest. Alerts when a program associated with file and directory enumeration is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Network Configuration Enumeration Via Program  
Attackers can interrogate local network and route information to identify adjacent hosts and networks ahead of lateral movement. Alerts when a program associated with network configuration enumeration is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Process Enumeration Via Program  
Attackers often list running programs in order to identify the purpose of a node and whether any security or monitoring tools are in place. Alerts when a program associated with process enumeration is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
System Information Enumeration Via Program  
Attackers commonly execute system enumeration commands to determine Linux kernel and distribution versions and features, often to identify if the node is affected by specific vulnerabilities. Alerts when a program associated with system information enumeration is executed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
Scheduled Tasks Modified Via Program  
Modifying scheduled tasks is a common method for establishing persistence on a compromised node. Alerts when the crontab, at, or batch commands are used to modify scheduled task configurations.  
instanceId  
capsule8-alerts-dataplane  
Systemctl Usage Detected  
Changes to systemd units could result in security controls being relaxed or disabled, or the installation of a malicious service. Alerts when the systemctl command is used to modify systemd units.  
instanceId  
capsule8-alerts-dataplane  
User Execution Of su Command  
Explicit escalation to the root user decreases the ability to correlate privileged activity to a specific user. Alerts when the su command is executed.  
instanceId  
capsule8-alerts-dataplane  
User Execution Of sudo Command  
Alerts when the sudo command is executed.  
instanceId  
capsule8-alerts-dataplane  
User Command History Cleared  
Deleting the history file is unusual, commonly performed by attackers hiding activity, or by legitimate users intending to evade audit controls. Alerts when command line history files are deleted.  
instanceId  
capsule8-alerts-dataplane  
New System User Added  
An attacker might add a new user to a host to provide a reliable method of access. Alerts if a new user entity is added to the local account management file /etc/passwd, if the entity is not added by a system update program.  
instanceId  
capsule8-alerts-dataplane  
Password Database Modification"
22746	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"instanceId  
capsule8-alerts-dataplane  
Password Database Modification  
Attackers might directly modify identity-related files to add a new user to the system. Alerts when a file related to user passwords is modified by a program unrelated to updating existing user information.  
instanceId  
capsule8-alerts-dataplane  
SSH Authorized Keys Modification  
Adding a new SSH public key is a common method for gaining persistent access to a compromised host. Alerts when an attempt to write to a user’s SSH authorized_keys file is observed, if the program is already part of an ongoing incident.  
instanceId  
capsule8-alerts-dataplane  
User Account Created Via CLI  
Adding a new user is a common step for attackers when establishing persistence on a compromised node. Alerts when an identity management program is executed by a program other than a package manager.  
instanceId  
capsule8-alerts-dataplane  
User Configuration Changes  
Deleting the history file is unusual, commonly performed by attackers hiding activity, or by legitimate users intending to evade audit controls. Alerts when command line history files are deleted.  
instanceId  
capsule8-alerts-dataplane  
New System User Added  
User profile and configuration files are often modified as a method of persistence in order to execute a program whenever a user logs in. Alerts when .bash_profile and bashrc (as well as related files) are modified by a program other than a system update tool.  
instanceId  
Antivirus monitoring events  
Note  
The response JSON object in these audit logs always has a result field that includes one line of the original scan result. Each scan result is represented typically by multiple audit log records, one for each line of the original scan output. For details of what could appear in this file, see the following third-party documentation.  
The following clamAVScanService-dataplane event is logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
clamAVScanService-dataplane  
clamAVScanAction  
The antivirus monitoring performs a scan. A log will generate for each line of the original scan output.  
instanceId  
System log events  
Note  
The response JSON object in the audit log has a result field that includes the original system log content.  
The following syslog event is logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
syslog  
processEvent  
The system log processes an event.  
instanceId  
processName  
Process monitor log events  
The following monit events are logged at the workspace level.  
Service  
Action  
Description  
Request parameters  
monit  
processNotRunning  
The monitor is not running.  
instanceId  
processName  
monit  
processRestarting  
The monitor is restarting.  
instanceId  
processName  
monit  
processStarted  
The monitor started.  
instanceId  
processName  
monit  
processRunning  
The monitor is running.  
instanceId  
processName"
22747	https://docs.databricks.com/en/admin/account-settings/audit-logs.html	"Deprecated log events
Deprecated log events
Databricks has deprecated the following audit events:  
createAlertDestination (now createNotificationDestination)  
deleteAlertDestination (now deleteNotificationDestination)  
updateAlertDestination (now updateNotificationDestination)  
SQL endpoint logs  
If you create SQL warehouses using the deprecated SQL endpoint API (the former name for SQL warehouses), the corresponding audit event name will include the word Endpoint instead of Warehouse. Besides the name, these events are identical to the SQL warehouse events. To view descriptions and request parameters of these events, see their corresponding warehouse events in Databricks SQL events.  
The SQL endpoint events are:  
changeEndpointAcls  
createEndpoint  
editEndpoint  
startEndpoint  
stopEndpoint  
deleteEndpoint  
setEndpointConfig"
22748	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Deliver and access billable usage logs  
Preview  
This feature is in Public Preview.  
Note  
Billable usage logs do not record usage for all products. Databricks recommends using system tables to view complete usage logs.  
As a Databricks account admin, you can configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate CSV file for each workspace in your account. This CSV file includes historical data about the workspace’s cluster usage in Databricks Units (DBUs), sortable by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see CSV file schema.  
To use an API to download the billable usage logs without configuring log delivery, see Return billable usage logs.  
Account admins can view usage in graph or table form on the Usage page in the account console. That page also includes a usage chart that displays account usage in DBUs, grouped by workload type, and allows you to directly download usage data in CSV format.  
If your account is on another version of the platform, account owners can view usage on the legacy account console’s Usage Overview tab.  
You can optionally deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery. This allows flexibility, for example setting up workspaces from multiple AWS accounts to deliver to the same S3 bucket. This option requires that you configure an S3 bucket policy that references a cross-account IAM role. Instructions and a policy template are provided in this article.  
Account owner and account admin access to the logs depends on how you set up the S3 bucket. Databricks delivers logs to your S3 bucket with AWS’s built-in BucketOwnerFullControl Canned ACL so that account owners and designees can download the logs directly. To support bucket ownership for newly-created objects, you must set your bucket’s S3 Object Ownership setting to the value Bucket owner preferred.  
Important  
If instead you set your bucket’s S3 Object Ownership setting to Object writer, new objects such as your logs remain owned by the uploading account, which is by default the IAM role you created and specified to access your bucket. This can make it difficult to access the logs, because you cannot access them from the AWS console or automation tools that you authenticated with as the bucket owner.  
Databricks recommends that you review Security Best Practices for S3 for guidance around protecting the data in your bucket from unwanted access.  
In addition to delivery of logs for running workspaces, logs are delivered for cancelled workspaces to ensure that logs are properly delivered that represent the final day of the workspace.  
Configuration options
Configuration options
When you configure billable usage log delivery, you have the following options if you have multiple workspaces in your account:  
Share the same configuration (log delivery S3 bucket and IAM role) for all workspaces in the account. This is the default.  
Use separate configurations for each workspace in the account.  
Use separate configurations for different groups of workspaces, each sharing a configuration.  
Note  
Even though you use the Account API to configure log delivery, you can configure log delivery with any workspace, including workspaces that were not created using the Account API.

High-level flow"
22749	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"High-level flow
The high-level flow of billable usage log delivery:  
Configure storage: In AWS, create a new AWS S3 bucket. Using Databricks APIs, call the Account API to create a storage configuration object that uses the bucket name.  
Note  
To deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery, you need to add an S3 bucket policy. You do not add the policy in this step, but in a later one.  
Configure credentials: In AWS, create the appropriate AWS IAM role. Using Databricks APIs, call the Account API to create a credentials configuration object that uses the IAM role’s ARN. The role policy can specify a path prefix for log delivery within your S3 bucket. You can choose to define an IAM role to include multiple path prefixes if you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes.  
Optional cross-account support To deliver logs to an AWS account other than the account of the IAM role that you create for log delivery, add an S3 bucket policy. This policy references IDs for the cross-account IAM role that you created in the previous step.  
Call the log delivery API: Call the Account API to create a log delivery configuration that uses the credential and storage configuration objects from previous steps. This step lets you specify if you want to associate the log delivery configuration for whole account (current and future workspaces) or for a specific set of workspaces.  
Access the CSV files for analysis: The delivery location is <bucket-name>/<prefix>/billable-usage/csv/, where <prefix> is the name of the optional delivery path prefix you set up during log delivery configuration. Files are named workspaceId=<workspace-id>-usageMonth=<month>.csv. Files are delivered daily by overwriting the month’s CSV file for each workspace. You can import this data into Databricks for analysis. There is also a sample notebook that you can use to run a usage analysis dashboard based on these CSV files. See Analyze usage data in Databricks.  
Important  
There is a limit on the number of log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.

Requirements
Requirements
You must be an account admin  
Account ID. You can find the account ID in the account console.

How to authenticate to the Account API"
22750	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"How to authenticate to the Account API
To authenticate to the Account API, you can use Databricks OAuth for service principals, Databricks OAuth for users, or a Databricks account admin’s username and password. Databricks strongly recommends that you use Databricks OAuth for users or service principals. A service principal is an identity that you create in Databricks for use with automated tools, jobs, and applications. See OAuth machine-to-machine (M2M) authentication.  
Use the following examples to authenticate to a Databricks account. You can use OAuth for service principals, OAuth for users, or a user’s username and password (legacy). For background, see:  
For OAuth for service principals, see OAuth machine-to-machine (M2M) authentication.  
For OAuth for users, see OAuth user-to-machine (U2M) authentication.  
For a user’s username and password (legacy), see Basic authentication (legacy).  
For authentication examples, choose from the following:  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Complete the steps to configure OAuth M2M authentication for service principals in the account. See OAuth machine-to-machine (M2M) authentication.  
Identify or manually create a Databricks configuration profile in your .databrickscfg file, with the profile’s fields set correctly for the related host, account_id, and client_id and client_secret mapping to the service principal. See OAuth machine-to-machine (M2M) authentication.  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p MY-AWS-ACCOUNT  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Complete the steps to configure OAuth U2M authentication for users in the account. See OAuth user-to-machine (U2M) authentication.  
Start the user authentication process by running the following Databricks CLI command:  
databricks auth login --host <account-console-url> --account-id <account-id>  
For example:  
databricks auth login --host https://accounts.cloud.databricks.com --account-id 00000000-0000-0000-0000-000000000000  
Note  
If you have an existing Databricks configuration profile with the host and account_id fields already set, you can substitute --host <account-console-url> --account-id <account-id> with --profile <profile-name>.  
Follow the on-screen instructions to have the Databricks CLI automatically create the related Databricks configuration profile in your .databrickscfg file.  
Continue following the on-screen instructions to sign in to your Databricks account through your web browser.  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p ACCOUNT-00000000-0000-0000-0000-000000000000  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.  
Install Databricks CLI version 0.205 or above. See Install or update the Databricks CLI.  
Identify or manually create a Databricks configuration profile in your .databrickscfg file, with the profile’s fields set correctly for the related host, account_id, and username and password mapping to your Databricks user account. See Basic authentication (legacy).  
Run your target Databricks CLI command, where <profile-name> represents the name of the configuration profile in your .databrickscfg file:  
databricks account <command-name> <subcommand-name> -p <profile-name>  
For example, to list all users in the account:  
databricks account users list -p MY-AWS-ACCOUNT  
For a list of available account commands, run the command databricks account -h.  
For a list of available subcommands for an account command, run the command databricks account <command-name> -h.

Step 1: Configure storage"
22751	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Step 1: Configure storage
Databricks delivers the billable usage data to an S3 bucket in your account. You can configure multiple workspaces to use a single S3 bucket, or you can define different workspaces (or groups of workspaces) to use different buckets.  
This procedure describes how to set up a single configuration object with a common configuration for one or more workspaces in the account. To use different storage locations for different workspaces, repeat the procedures in this article for each workspace or group of workspaces.  
Create the S3 bucket, following the instructions in Step 1: Configure audit log storage.  
Important  
To deliver logs to an AWS account other than the one used for your Databricks workspace, you must add an S3 bucket policy. You do not add the bucket policy in this step. See Step 3: Optional cross-account support.  
Create a Databricks storage configuration record that represents your new S3 bucket. Specify your S3 bucket by calling the create new storage configuration API (POST /accounts/<account-id>/storage-configurations).  
Pass the following:  
storage_configuration_name — New unique storage configuration name.  
root_bucket_info — A JSON object that contains a bucket_name field that contains your S3 bucket name.  
Copy the storage_configuration_id value returned in the response body. You will use it to create the log delivery configuration in a later step.  
For example:  
curl -X POST 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/storage-configurations' \ --header 'Authorization: Bearer $OAUTH_TOKEN' \ -d '{ ""storage_configuration_name"": ""databricks-workspace-storageconf-v1"", ""root_bucket_info"": { ""bucket_name"": ""my-company-example-bucket"" } }'  
Response:  
{ ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""account_id"": ""<databricks-account-id>"", ""root_bucket_info"": { ""bucket_name"": ""my-company-example-bucket"" }, ""storage_configuration_name"": ""databricks-workspace-storageconf-v1"", ""creation_time"": 1579754875555 }

Step 2: Configure credentials"
22752	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Step 2: Configure credentials
This procedure describes how to set up a single configuration object with a common configuration for one or more workspaces in the account. To use different credentials for different workspaces, repeat the procedures in this article for each workspace or group of workspaces.  
Note  
To use different S3 bucket names, you need to create separate IAM roles.  
Log into your AWS Console as a user with administrator privileges and go to the IAM service.  
Click the Roles tab in the sidebar.  
Click Create role.  
In Select type of trusted entity, click AWS service.  
In Common Use Cases, click EC2.  
Click the Next: Permissions button.  
Click the Next: Tags button.  
Click the Next: Review button.  
In the Role name field, enter a role name.  
Click Create role. The list of roles displays.  
In the list of roles, click the role you created.  
Add an inline policy.  
On the Permissions tab, click Add inline policy.  
In the policy editor, click the JSON tab.  
Copy this access policy and modify it. Replace the following values in the policy with your own configuration values:  
<s3-bucket-name>: The bucket name of your AWS S3 bucket.  
<s3-bucket-path-prefix>: (Optional) The path to the delivery location in the S3 bucket. If unspecified, the logs are delivered to the root of the bucket. This path must match the delivery_path_prefix argument when you call the log delivery API.  
{ ""Version"":""2012-10-17"", ""Statement"":[ { ""Effect"":""Allow"", ""Action"":[ ""s3:GetBucketLocation"" ], ""Resource"":[ ""arn:aws:s3:::<s3-bucket-name>"" ] }, { ""Effect"":""Allow"", ""Action"":[ ""s3:PutObject"", ""s3:GetObject"", ""s3:DeleteObject"", ""s3:PutObjectAcl"", ""s3:AbortMultipartUpload"" ], ""Resource"":[ ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/"", ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/*"" ] }, { ""Effect"":""Allow"", ""Action"":[ ""s3:ListBucket"", ""s3:ListMultipartUploadParts"", ""s3:ListBucketMultipartUploads"" ], ""Resource"":""arn:aws:s3:::<s3-bucket-name>"", ""Condition"":{ ""StringLike"":{ ""s3:prefix"":[ ""<s3-bucket-path-prefix>"", ""<s3-bucket-path-prefix>/*"" ] } } } ] }  
You can customize the policy use of the path prefix:  
If you do not want to use the bucket path prefix, remove <s3-bucket-path-prefix>/ (including the final slash) from the policy each time it appears.  
If you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes, you can define an IAM role to include multiple path prefixes. There are two separate parts of the policy that reference <s3-bucket-path-prefix>. In each case, duplicate the two adjacent lines that reference the path prefix. Repeat each pair of lines for every new path prefix, for example:  
{ ""Resource"":[ ""arn:aws:s3:::<mybucketname>/field-team/"", ""arn:aws:s3:::<mybucketname>/field-team/*"", ""arn:aws:s3:::<mybucketname>/finance-team/"", ""arn:aws:s3:::<mybucketname>/finance-team/*"" ] }  
Click Review policy.  
In the Name field, enter a policy name.  
Click Create policy.  
If you use service control policies to deny certain actions at the AWS account level, ensure that sts:AssumeRole is whitelisted so Databricks can assume the cross-account role.  
On the role summary page, click the Trust Relationships tab.  
Paste this access policy into the editor and replace the following values in the policy with your own configuration values. The policy uses the Databricks AWS account ID 414351767826. If you are are using Databricks on AWS GovCloud use the Databricks account ID 044793339203.  
<databricks-account-id>: Your Databricks account ID.  
{ ""Version"":""2012-10-17"", ""Statement"":[ { ""Effect"":""Allow"", ""Principal"":{ ""AWS"":""arn:aws:iam::414351767826:role/SaasUsageDeliveryRole-prod-IAMRole-3PLHICCRR1TK"" }, ""Action"":""sts:AssumeRole"", ""Condition"":{ ""StringEquals"":{ ""sts:ExternalId"":[ ""<databricks-account-id>"" ] } } } ] }  
In the role summary, copy the Role ARN and save it for a later step.  
Create a Databricks credentials configuration ID for your AWS role. Call the Create credential configuration API (POST /accounts/<account-id>/credentials). This request establishes cross-account trust and returns a reference ID to use when you create a new workspace.  
Replace <account-id> with your Databricks account ID. In the request body:  
Set credentials_name to a name that is unique within your account.  
Set aws_credentials to an object that contains an sts_role property. That object must specify the role_arn for the role you’ve created.  
The response body will include a credentials_id field, which is the Databricks credentials configuration ID that you need to create the new workspace. Copy this field so you can use it to create the log delivery configuration in a later step.  
For example:"
22753	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"For example:  
curl -X POST 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/credentials' \ --header 'Authorization: Bearer $OAUTH_TOKEN' \ -d '{ ""credentials_name"": ""databricks-credentials-v1"", ""aws_credentials"": { ""sts_role"": { ""role_arn"": ""arn:aws:iam::<aws-account-id>:role/my-company-example-role"" } } }'  
Example response:  
{ ""credentials_id"": ""<databricks-credentials-id>"", ""account_id"": ""<databricks-account-id>"", ""aws_credentials"": { ""sts_role"": { ""role_arn"": ""arn:aws:iam::<aws-account-id>:role/my-company-example-role"", ""external_id"": ""<databricks-account-id>"" } }, ""credentials_name"": ""databricks-credentials-v1"", ""creation_time"": 1579753556257 }  
Copy the credentials_id field from the response for later use."
22754	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Step 3: Optional cross-account support
Step 3: Optional cross-account support
If your S3 bucket is in the same AWS account as the IAM role used for log delivery, skip this step.  
To deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery, add the S3 bucket policy shown below. This policy references IDs for the cross-account IAM role that you created in the previous step.  
In the AWS Console, go to the S3 service.  
Click the bucket name.  
Click the Permissions tab.  
Click the Bucket Policy button.  
Copy and modify this bucket policy.  
Replace <s3-bucket-name> with the S3 bucket name. Replace <customer-iam-role-id> with the role ID of your newly-created IAM role. Replace <s3-bucket-path-prefix> with the bucket path prefix you want. See the notes after the policy sample for information about customizing the path prefix.  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": [""arn:aws:iam::<customer-iam-role-id>""] }, ""Action"": ""s3:GetBucketLocation"", ""Resource"": ""arn:aws:s3:::<s3-bucket-name>"" }, { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""arn:aws:iam::<customer-iam-role-id>"" }, ""Action"": [ ""s3:PutObject"", ""s3:GetObject"", ""s3:DeleteObject"", ""s3:PutObjectAcl"", ""s3:AbortMultipartUpload"", ""s3:ListMultipartUploadParts"" ], ""Resource"": [ ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/"", ""arn:aws:s3:::<s3-bucket-name>/<s3-bucket-path-prefix>/*"" ] }, { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""arn:aws:iam::<customer-iam-role-id>"" }, ""Action"": ""s3:ListBucket"", ""Resource"": ""arn:aws:s3:::<s3-bucket-name>"", ""Condition"": { ""StringLike"": { ""s3:prefix"": [ ""<s3-bucket-path-prefix>"", ""<s3-bucket-path-prefix>/*"" ] } } } ] }  
You can customize the policy use of the path prefix:  
If you do not want to use the bucket path prefix, remove <s3-bucket-path-prefix>/ (including the final slash) from the policy each time it appears.  
If you want log delivery configurations for multiple workspaces that share the same S3 bucket but use different path prefixes, you can define an IAM role to include multiple path prefixes. Two parts of the policy reference <s3-bucket-path-prefix>. In each place, duplicate the two adjacent lines that reference the path prefix. Repeat each pair of lines for each new path prefix. For example:  
{ ""Resource"":[ ""arn:aws:s3:::<mybucketname>/field-team/"", ""arn:aws:s3:::<mybucketname>/field-team/*"", ""arn:aws:s3:::<mybucketname>/finance-team/"", ""arn:aws:s3:::<mybucketname>/finance-team/*"" ] }

Step 4: Call the log delivery API"
22755	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Step 4: Call the log delivery API
To configure log delivery, call the Log delivery configuration API (POST /accounts/<account-id>/log-delivery).  
You need the following values that you copied in the previous steps:  
credentials_id: Your Databricks credential configuration ID, which represents your cross-account role credentials.  
storage_configuration_id: Your Databricks storage configuration ID, which represents your root S3 bucket.  
Also set the following fields:  
log_type: Always set to BILLABLE_USAGE.  
output_format: Always set to CSV. For details of the CSV file format, see Billable usage log schema (legacy).  
delivery_path_prefix: (Optional) Set to the path prefix. This must match the path prefix that you used in your role policy.  
workspace_ids_filter: (Optional) By default, this log configuration applies to all workspaces associated with your account ID. For some types of deployments there is only one workspace per account ID so this field is unnecessary. If your account was created originally for workspace creation with the Account API, you may have multiple workspaces associated with your account ID. You can optionally set this field to array of workspace IDs that this configuration applies to. If you plan to use different log delivery configurations for different workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID is associated in the future with additional workspaces, this configuration also applies to the new workspaces. A workspace might apply to more than one log delivery configuration, in which case the logs are written to multiple locations.  
Important  
There is a limit on the number of log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.  
delivery_start_time: (Optional) The month and year at which log delivery starts. Defaults to current month. Format is text in YYYY-MM format. You can enter any month and year from 2019-03 on.  
For example:  
curl -X POST 'https://accounts.cloud.databricks.com/api/2.0/accounts/<databricks-account-id>/log-delivery' \ --header 'Authorization: Bearer $OAUTH_TOKEN' \ -d '{ ""log_delivery_configuration"": { ""log_type"": ""BILLABLE_USAGE"", ""config_name"": ""billable usage config"", ""output_format"": ""CSV"", ""credentials_id"": ""<databricks-credentials-id>"", ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""delivery_path_prefix"": ""usage-data"", ""delivery_start_time"": ""2020-06"", ""workspace_ids_filter"": [ 6383650456894062, 4102272838062927 ] } }'  
Example response:  
{ ""log_delivery_configuration"": { ""config_id"": ""<config-id>"", ""config_name"": ""billable usage config"", ""log_type"": ""BILLABLE_USAGE"", ""output_format"": ""CSV"", ""account_id"": ""<account-id>"", ""credentials_id"": ""<databricks-credentials-id>"", ""storage_configuration_id"": ""<databricks-storage-config-id>"", ""workspace_ids_filter"": [ 6383650456894062, 4102272838062927 ], ""delivery_path_prefix"": ""usage-data"", ""delivery_start_time"": ""2020-06"", ""status"": ""ENABLED"", ""creation_time"": 1591638409000, ""update_time"": 1593108904000, ""log_delivery_status"": { ""status"": ""CREATED"", ""message"": ""Log Delivery Configuration is successfully created. Status will be updated after the first delivery attempt."" } } }  
Additional features of the log delivery APIs  
The log delivery APIs have additional features. See the API reference documentation for details.  
Additional operations include:  
Get all log delivery configurations  
Get a log delivery configuration by ID  
Enable or disable a log delivery configuration by ID  
Log delivery configuration status can be found in the API response’s log_delivery_status object. With log_delivery_status, you can check the status (success or failure) and the last time of an attempt or successful delivery.  
Important  
There is a limit on the number of log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described."
22756	https://docs.databricks.com/en/admin/account-settings/billable-usage-delivery.html	"Step 5: Access the log files for analysis
Step 5: Access the log files for analysis
Log files are delivered to <bucket-name>/<prefix>/billable-usage/csv/, where <prefix> is the name of the optional delivery path prefix you set up during log delivery configuration. Files are named workspaceId=<workspace-id>-usageMonth=<month>.csv. Files are delivered daily by overwriting the month’s CSV file for each workspace.  
For the CSV schema, see CSV file schema  
For information about how to analyze these files using Databricks, see Analyze usage data in Databricks

Automated configuration using Terraform
Automated configuration using Terraform
You can use Databricks Terraform provider to configure usage log delivery automatically with the help of the databricks_mws_log_delivery resource. Here’s an end-to-end example of usage and audit log delivery:  
variable ""databricks_account_id"" { description = ""Account ID. You can get your account ID in the bottom left corner of the account console. See https://accounts.cloud.databricks.com"" } resource ""aws_s3_bucket"" ""logdelivery"" { bucket = ""${var.prefix}-logdelivery"" acl = ""private"" versioning { enabled = false } force_destroy = true tags = merge(var.tags, { Name = ""${var.prefix}-logdelivery"" }) } resource ""aws_s3_bucket_public_access_block"" ""logdelivery"" { bucket = aws_s3_bucket.logdelivery.id ignore_public_acls = true } data ""databricks_aws_assume_role_policy"" ""logdelivery"" { external_id = var.databricks_account_id for_log_delivery = true } resource ""aws_iam_role"" ""logdelivery"" { name = ""${var.prefix}-logdelivery"" description = ""(${var.prefix}) UsageDelivery role"" assume_role_policy = data.databricks_aws_assume_role_policy.logdelivery.json tags = var.tags } data ""databricks_aws_bucket_policy"" ""logdelivery"" { full_access_role = aws_iam_role.logdelivery.arn bucket = aws_s3_bucket.logdelivery.bucket } resource ""aws_s3_bucket_policy"" ""logdelivery"" { bucket = aws_s3_bucket.logdelivery.id policy = data.databricks_aws_bucket_policy.logdelivery.json } resource ""databricks_mws_credentials"" ""log_writer"" { account_id = var.databricks_account_id credentials_name = ""Usage Delivery"" role_arn = aws_iam_role.logdelivery.arn } resource ""databricks_mws_storage_configurations"" ""log_bucket"" { account_id = var.databricks_account_id storage_configuration_name = ""Usage Logs"" bucket_name = aws_s3_bucket.logdelivery.bucket } resource ""databricks_mws_log_delivery"" ""usage_logs"" { account_id = var.databricks_account_id credentials_id = databricks_mws_credentials.log_writer.credentials_id storage_configuration_id = databricks_mws_storage_configurations.log_bucket.storage_configuration_id delivery_path_prefix = ""billable-usage"" config_name = ""Usage Logs"" log_type = ""BILLABLE_USAGE"" output_format = ""CSV"" } resource ""databricks_mws_log_delivery"" ""audit_logs"" { account_id = var.databricks_account_id credentials_id = databricks_mws_credentials.log_writer.credentials_id storage_configuration_id = databricks_mws_storage_configurations.log_bucket.storage_configuration_id delivery_path_prefix = ""audit-logs"" config_name = ""Audit Logs"" log_type = ""AUDIT_LOGS"" output_format = ""JSON"" }"
22757	https://docs.databricks.com/en/admin/account-settings/index.html	"Manage your Databricks account  
Databricks account-level configurations are managed by account admins. This article includes various settings the account admin can manage through the account console. The other articles in this section cover additional tasks performed by account admins.  
As an account admin, you can also manage your Databricks account using the Account API.  
Manage account console settings
Manage account console settings
The following are account console settings available to account admins.

Locate your account ID
Locate your account ID
To retrieve your account ID, go to the account console and click the down arrow next to your username in the upper right corner. In the drop-down menu you can view and copy your Account ID.  
You must be in the account console to retrieve the account ID, the ID will not display inside a workspace.

Add an account nickname
Add an account nickname
To help easily identify your Databricks account in the Databricks UI, give your account a human-readable nickname. This nickname displays at the top of the account console and in the dropdown menu next to your account ID. Account nicknames are especially useful if you have more than one Databricks account.  
To add an account nickname:  
In the account console, click Settings.  
Click the Account settings tab.  
Under Account name, enter your new account nickname then click Save.  
You can update account nicknames at any time.

Change the account console language settings
Change the account console language settings
The account console is available in multiple languages. To change the account console language, select Settings then go to the Preferences tab.

Manage email preferences
Manage email preferences
Databricks can occasionally send emails with personalized product and feature recommendations based on your use of Databricks. These messages may include information to help users get started with Databricks or learn about new features and previews.  
You can manage whether you receive these emails in the account console:  
Log in to the account console and click the Settings icon in the sidebar.  
In the My preferences section, click the Instructional product and feature emails toggle.  
You can also manage your promotional email communications by clicking Manage under Promotional email communications or by going to the Marketing preference center. Non-admin users can update this setting by clicking the My preferences link next to their workspace in the account console."
22758	https://docs.databricks.com/en/admin/account-settings/no-isolation-shared.html	"Enable admin protection for “No isolation shared” clusters on your account  
Account admins can prevent internal credentials from being automatically generated for Databricks workspace admins on No Isolation Shared clusters. No Isolation Shared clusters are clusters that have the Access mode dropdown set to No isolation shared.  
Important  
The clusters UI recently changed. The No Isolation Shared access mode setting for a cluster previously appeared as the Standard cluster mode. If you used the High Concurrency cluster mode without additional security settings such as table access control (Table ACLs) or credential passthrough, the same settings are used as with Standard cluster mode. The account-level admin setting that this article discusses applies to both the No Isolation Shared access mode and its equivalent legacy cluster modes. For a comparison of the old UI and new UI cluster types, see Clusters UI changes and cluster access modes.  
The admin protection for No Isolation Shared clusters on your account helps protect admin accounts from sharing internal credentials in an environment that is shared with other users. Enabling this setting may impact workloads that are run by admins. See Limitations.  
No Isolation Shared clusters run arbitrary code from multiple users in the same shared environment, similar to what happens on a cloud Virtual Machine that is shared across multiple users. Data or internal credentials provisioned to that environment might be accessible to any code running within that environment. To call Databricks APIs for normal operations, access tokens are provisioned on behalf of users to these clusters. When a higher-privileged user, such as a workspace administrator, runs commands on a cluster, their higher-privileged token is visible in the same environment.  
You can determine which clusters in a workspace have cluster types that are affected by this setting. See Find all your No Isolation Shared clusters (including equivalent legacy cluster modes).  
In addition to this account-level setting, there is a workspace-level setting called Enforce User Isolation. Account admins can enable it to prevent creating or starting a “No isolation shared” cluster access type or its equivalent legacy cluster types.  
Enable the account-level admin protection setting
Enable the account-level admin protection setting
As an account admin, log in to the Account Console.  
Click Settings .  
Click the Feature enablement tab.  
Under Enable Admin Protection for “No Isolation Shared” Clusters, click the setting to enable or disable this feature.  
If the feature is enabled, Databricks prevents automatic generation of Databricks API internal credentials for Databricks workspace admins on No Isolation Shared clusters.  
Changes may take up to two minutes to take effect on all workspaces.

Limitations
Limitations
When used with No Isolation Shared clusters or the equivalent legacy cluster modes, the following Databricks features do not work if you enable admin protection for No Isolation Shared clusters on your account:  
Machine Learning Runtime workloads.  
Workspace files.  
dbutils Secrets utility.  
dbutils Notebook utility.  
Delta Lake operations by admins that create, modify, or update data.  
Other features might not work for admin users on this cluster type because these features rely on automatically generated internal credentials.  
In those cases, Databricks recommends that admins do one of the following:  
Use a different cluster type than “No isolation shared” or its equivalent legacy cluster types.  
Create a non-admin user when using No Isolation Shared clusters.

Find all your No Isolation Shared clusters (including equivalent legacy cluster modes)
Find all your No Isolation Shared clusters (including equivalent legacy cluster modes)
You can determine which clusters in a workspace are affected by this account-level setting.  
Import the following notebook into all your workspaces and run the notebook.  
Get a list of all No Isolation Shared clusters notebook  
Open notebook in new tab Copy link for import"
22759	https://docs.databricks.com/en/admin/account-settings/serverless-quotas.html	"Serverless quotas  
Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. There are initial default quotas for accounts, but Databricks automatically proactively increases quotas based on your type of account and how you use Databricks. Databricks intends for typical customers to not hit quotas limits in normal usage.  
The quota is measured in Databricks Units (DBUs) per hour. A DBU is a normalized unit of processing power on the Databricks platform used for measurement and pricing purposes. See the pricing page.  
Quotas are not intended as a capacity planning mechanism and are not a general purpose way to manage or limit spend.  
Quotas are enforced only for serverless SQL warehouses. When you hit your serverless quota in a region, workspaces cannot launch new serverless SQL warehouses. Hitting this limit does not terminate existing SQL warehouses in the region. However, if you hit the limit, Databricks prevents increasing the number of compute resources in the warehouse.  
Increase serverless quota
Increase serverless quota
If necessary, you can request additional quota increases by emailing help@databricks.com. If you have a support plan, you can also make the request in your support portal. Your request for quota increase must include:  
Set the subject line to ""Serverless quota increase request""  
One or more AWS regions  
Your requested increase for the quota  
Justification or reason for the quota increase"
22760	https://docs.databricks.com/en/admin/account-settings/standard-tier.html	"End of life for Standard tier workspaces on Databricks on AWS  
Databricks has announced the end of life for Standard tier workspaces on Databricks on AWS.  
Customers have until October 1, 2025, to upgrade existing workspaces to the Premium or Enterprise tier. Any remaining Standard tier workspaces on October 1, 2025, will be automatically upgraded to the Premium tier. Refer to the pricing page to assess how this upgrade might impact your bill.  
Starting immediately, all new subscriptions must be created on the Premium or Enterprise tier. Existing Standard tier customers can continue to create new Standard workspaces until Oct 1, 2024, and they have until Oct 1, 2025, to upgrade any new workspace to the Premium or Enterprise tier.  
What features do I get when upgrading from the Standard tier?
What features do I get when upgrading from the Standard tier?
Standard  
Premium  
Enterprise  
Databricks workspace with managed Spark  
✔  
✔  
✔  
Unity Catalog  
✘  
✔  
✔  
Serverless offerings  
✘  
✔  
✔  
SQL  
✘  
✔  
✔  
Mosaic/AI  
✘  
✔  
✔  
Assistant / Databricks Intelligence  
✘  
✔  
✔  
Lakehouse Monitoring / Predictive Optimization  
✘  
✔  
✔  
Governance and Manageability  
✘  
✔  
✔  
Advanced Enterprise Security  
✘  
◑  
✔

Will I lose access to any features?
Will I lose access to any features?
No, you will not lose access to any capabilities. Higher tiers include all the features that are currently available in the Standard tier.

How do I upgrade my workspace to a higher tier?
How do I upgrade my workspace to a higher tier?
Refer to our documentation for details on how to upgrade your account to a higher tier. See Manage your subscription.

How do I know the tier for my workspace?
How do I know the tier for my workspace?
You can view a workspace’s current tier in the Workspaces page your account console."
22761	https://docs.databricks.com/en/admin/account-settings/usage.html	"View billable usage using the account console  
Note  
Usage for some products will not be displayed using this feature. Databricks recommends using system tables to view complete usage logs.  
This article describes how to use the Usage page in the account console to view usage data across workspaces in your account. You can also download billable usage logs using the Account API.  
You can view usage in the account console by either of these units:  
Databricks Unit (DBU): A unit of processing capability per hour, billed on per-second usage.  
Estimated cost (in $USD currency): Cost in dollars is estimated on a linear cost estimate per DBU based on standard cost-per-DBU pricing for your tier for each type of usage. Actual cost varies depending on your exact contract. Adjust cost-per-DBU for each SKU to change the cost estimate using the settings panel.  
Access the Usage page
Access the Usage page
Go to the account console and click the Usage icon.

Usage graph
Usage graph
At the top of the page, a graph shows your account’s usage in DBUs or the estimated cost in $USD. Use the $USD/DBU picker to toggle between these views.  
You can also use the aggregation picker to browse the data by:  
Total usage: A single line for the account’s total usage across all workspaces and SKUs. This is the default selection.  
Workspace: A line for each workspace’s usage.  
SKUs: A line for each SKU’s usage. The set of SKUs shown for your account depends on your contract. SKUs also depend on how you use Databricks. See Pricing.  
Tags: A line for each resource that includes the selected tag.  
If there are more than 10 workspaces, SKUs or tag values, the chart displays the nine with the highest usage. The usage of the remaining workspaces, SKUs or tag values is aggregated and displayed in a single line. This single line is marked as “Combined remaining …(s)”.  
If you browse data by tags, usage without the selected tag is aggregated and displayed in a single line labeled “(… tag absent).”  
Filter by workspace, tag, or SKU  
You can use the graph’s legend or the line picker to filter out the graph lines:  
To show / hide a line, click on its radio button in the graph’s legend or its checkbox in the line picker.  
To only show a single line, double-click on its radio button in the graph’s legend or its checkbox in the line picker. You can then double-click it again to restore the view and show all lines.  
The actual cost varies depending on your contract. You can adjust the cost-per-DBU for each SKU in the settings panel to change the cost estimate.

Usage details
Usage details
In the Usage details panel, you can view a list of detailed usage in DBU or the estimated cost in $USD in a table format. This panel’s data is always aggregated by workspace. The cost is also broken down into SKU groups. To view usage as totals (aggregated) or by SKU, use the graph in the settings panel instead.  
To toggle between estimated cost and DBUs, use the $USD/DBU picker.  
Use the date range picker to specify an exact time range or choose among pre-defined options such as Last 6 months. The time ranges are in UTC time.  
If you have many workspaces, type the workspace name into the search field to filter results.  
The actual cost varies depending on your contract. You can adjust the cost-per-DBU for each SKU in the settings panel to change the cost estimate.

Pricing
Pricing
You can set your actual pricing per DBU by clicking on the vertical ellipsis ⋮ in the top right corner of the Usage page to open the settings panel. Use this panel to adjust estimated pricing per DBU for both the usage and the usage details table on the Usage page.  
Each SKU is listed separately. The set of SKUs shown for your account depends on your contract. How usage applies to SKU depends on what you do with Databricks. For example, it depends whether a notebook is run using an automated cluster or as a scheduled job.

Usage downloads
Usage downloads
There are two types of usage downloads available. You can download the aggregated usage data displayed in your current graph or download overall usage details by date range.  
Download aggregated usage data  
To download aggregated usage data, click the download button in the top right corner of the graph. This downloads the data currently aggregated on the page. The CSV file will include the 999 workspaces, SKUs, or tag values with the highest usage.  
Usage data for unselected workspaces, SKUs, and tags is aggregated into groups labeled “Combined remaining …(s)” and “(… tag absent).”  
Download unaggregated usage data  
In the settings panel you can download all usage data by date. Open the panel by clicking on the kebab menu ⋮ in the top right corner of the Usage page. Scroll to the bottom of the panel.  
Select a time range from the month range picker.  
To include usernames, enable Include usernames (email).  
Click Download.

Analyze usage logs
Analyze usage logs
For information about how to analyze these files using Databricks, see Billable usage log schema (legacy).

Download usage logs with the Account API
Download usage logs with the Account API
You can download billable usage logs using the billable usage log download API.

Configure automated usage log delivery"
22762	https://docs.databricks.com/en/admin/account-settings/usage.html	"Configure automated usage log delivery
To configure automated usage log delivery to an S3 bucket, see Deliver and access billable usage logs."
22763	https://docs.databricks.com/en/admin/account-settings/usage-analysis.html	"Billable usage log schema (legacy)  
Note  
This article includes details about the legacy usage logs, which do not record usage for all products. Databricks recommends using the billable usage system table to access and query complete usage data.  
This article explains how to read and analyze the usage log data downloaded from the account console.  
You can view and download billable usage directly in the account console, or by using the Account API.  
CSV file schema"
22764	https://docs.databricks.com/en/admin/account-settings/usage-analysis.html	"CSV file schema
Column  
Type  
Description  
Example  
workspaceId  
string  
ID of the workspace.  
1234567890123456  
timestamp  
datetime  
End of the hour for the provided usage.  
2019-02-22T09:59:59.999Z  
clusterId  
string  
ID of the cluster (for a cluster) or of the warehouse (for a SQL warehouse)  
Cluster example: 0406-020048-brawl507  
SQL warehouse example: 8e00f0c8b392983e  
clusterName  
string  
User-provided name for the cluster/warehouse.  
Shared Autoscaling  
clusterNodeType  
string  
Instance type of the cluster/warehouse.  
Cluster example: m4.16xlarge  
SQL warehouse example: db.xlarge  
clusterOwnerUserId  
string  
ID of the user who created the cluster/warehouse.  
12345678901234  
clusterCustomTags  
string (“-escaped json)  
Custom tags associated with the cluster/warehouse during this hour.  
""{""""dept"""":""""mktg"""",""""op_phase"""":""""dev""""}""  
sku  
string  
Billing SKU. See the Billing SKUs table for a list of values.  
STANDARD_ALL_PURPOSE_COMPUTE  
dbus  
double  
Number of DBUs used by the user during this hour.  
1.2345  
machineHours  
double  
Total number of machine hours used by all containers in the cluster/warehouse.  
12.345  
clusterOwnerUserName  
string  
Username (email) of the user who created the cluster/warehouse.  
user@yourcompany.com  
tags  
string (“-escaped json)  
Default and custom cluster/warehouse tags, and default and custom instance pool tags (if applicable) associated with the cluster during this hour. See Cluster tags, Warehouse tags, and Pool tags. This is a superset of the clusterCustomTags column.  
""{""""dept"""":""""mktg"""",""""op_phase"""":""""dev"""", """"Vendor"""":""""Databricks"""", """"ClusterId"""":""""0405-020048-brawl507"""", """"Creator"""":""""user@yourcompany.com""""}""  
Billing SKUs  
AWS_ENHANCED_SECURITY_AND_COMPLIANCE  
ENTERPRISE_ALL_PURPOSE_COMPUTE  
ENTERPRISE_ALL_PURPOSE_COMPUTE_(PHOTON)  
ENTERPRISE_DLT_CORE_COMPUTE  
ENTERPRISE_DLT_CORE_COMPUTE_(PHOTON)  
ENTERPRISE_DLT_PRO_COMPUTE  
ENTERPRISE_DLT_PRO_COMPUTE_(PHOTON)  
ENTERPRISE_DLT_ADVANCED_COMPUTE  
ENTERPRISE_DLT_ADVANCED_COMPUTE_(PHOTON)  
ENTERPRISE_JOBS_COMPUTE  
ENTERPRISE_JOBS_COMPUTE_(PHOTON)  
ENTERPRISE_JOBS_LIGHT_COMPUTE  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_US_EAST_N_VIRGINIA  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_US_EAST_OHIO  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_US_WEST_OREGON  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_CANADA  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_EUROPE_IRELAND  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_EUROPE_FRANKFURT  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_AP_SINGAPORE  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_AP_SYDNEY  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_EAST_N_VIRGINIA  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_EAST_OHIO  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_WEST_OREGON  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_CANADA  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_EUROPE_IRELAND  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_EUROPE_FRANKFURT  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_AP_SINGAPORE  
ENTERPRISE_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_AP_SYDNEY  
ENTERPRISE_SERVERLESS_SQL_COMPUTE_US_EAST_N_VIRGINIA  
ENTERPRISE_SERVERLESS_SQL_COMPUTE_US_WEST_OREGON  
ENTERPRISE_SERVERLESS_SQL_COMPUTE_EUROPE_IRELAND  
ENTERPRISE_SERVERLESS_SQL_COMPUTE_AP_SYDNEY  
ENTERPRISE_SQL_COMPUTE  
ENTERPRISE_SQL_PRO_COMPUTE_US_EAST_N_VIRGINIA  
ENTERPRISE_SQL_PRO_COMPUTE_US_EAST_OHIO  
ENTERPRISE_SQL_PRO_COMPUTE_US_WEST_OREGON  
ENTERPRISE_SQL_PRO_COMPUTE_US_WEST_CALIFORNIA  
ENTERPRISE_SQL_PRO_COMPUTE_CANADA  
ENTERPRISE_SQL_PRO_COMPUTE_SA_BRAZIL  
ENTERPRISE_SQL_PRO_COMPUTE_EUROPE_IRELAND  
ENTERPRISE_SQL_PRO_COMPUTE_EUROPE_FRANKFURT  
ENTERPRISE_SQL_PRO_COMPUTE_EUROPE_LONDON  
ENTERPRISE_SQL_PRO_COMPUTE_EUROPE_FRANCE  
ENTERPRISE_SQL_PRO_COMPUTE_AP_SYDNEY  
ENTERPRISE_SQL_PRO_COMPUTE_AP_MUMBAI  
ENTERPRISE_SQL_PRO_COMPUTE_AP_SINGAPORE  
ENTERPRISE_SQL_PRO_COMPUTE_AP_TOKYO  
ENTERPRISE_SQL_PRO_COMPUTE_AP_SEOUL  
PREMIUM_ALL_PURPOSE_COMPUTE  
PREMIUM_ALL_PURPOSE_COMPUTE_(PHOTON)  
PREMIUM_DLT_CORE_COMPUTE  
PREMIUM_DLT_CORE_COMPUTE_(PHOTON)  
PREMIUM_DLT_PRO_COMPUTE  
PREMIUM_DLT_PRO_COMPUTE_(PHOTON)  
PREMIUM_DLT_ADVANCED_COMPUTE  
PREMIUM_DLT_ADVANCED_COMPUTE_(PHOTON)  
PREMIUM_JOBS_COMPUTE  
PREMIUM_JOBS_COMPUTE_(PHOTON)  
PREMIUM_JOBS_LIGHT_COMPUTE  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_US_EAST_N_VIRGINIA  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_US_EAST_OHIO  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_US_WEST_OREGON  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_CANADA  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_EUROPE_IRELAND  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_EUROPE_FRANKFURT  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_AP_SINGAPORE  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_AP_SYDNEY"
22765	https://docs.databricks.com/en/admin/account-settings/usage-analysis.html	"PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_EUROPE_FRANKFURT  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_AP_SINGAPORE  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_AP_SYDNEY  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_EAST_N_VIRGINIA  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_EAST_OHIO  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_US_WEST_OREGON  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_CANADA  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_EUROPE_IRELAND  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_EUROPE_FRANKFURT  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_AP_SINGAPORE  
PREMIUM_SERVERLESS_REAL_TIME_INFERENCE_LAUNCH_AP_SYDNEY  
PREMIUM_SERVERLESS_SQL_COMPUTE_US_EAST_N_VIRGINIA  
PREMIUM_SERVERLESS_SQL_COMPUTE_US_WEST_OREGON  
PREMIUM_SERVERLESS_SQL_COMPUTE_EUROPE_FRANKFURT  
PREMIUM_SERVERLESS_SQL_COMPUTE_EUROPE_IRELAND  
PREMIUM_SERVERLESS_SQL_COMPUTE_AP_SYDNEY  
PREMIUM_SQL_COMPUTE  
PREMIUM_SQL_PRO_COMPUTE_US_EAST_N_VIRGINIA  
PREMIUM_SQL_PRO_COMPUTE_US_EAST_OHIO  
PREMIUM_SQL_PRO_COMPUTE_US_WEST_OREGON  
PREMIUM_SQL_PRO_COMPUTE_US_WEST_CALIFORNIA  
PREMIUM_SQL_PRO_COMPUTE_CANADA  
PREMIUM_SQL_PRO_COMPUTE_SA_BRAZIL  
PREMIUM_SQL_PRO_COMPUTE_EUROPE_IRELAND  
PREMIUM_SQL_PRO_COMPUTE_EUROPE_FRANKFURT  
PREMIUM_SQL_PRO_COMPUTE_EUROPE_LONDON  
PREMIUM_SQL_PRO_COMPUTE_EUROPE_FRANCE  
PREMIUM_SQL_PRO_COMPUTE_AP_SYDNEY  
PREMIUM_SQL_PRO_COMPUTE_AP_MUMBAI  
PREMIUM_SQL_PRO_COMPUTE_AP_SINGAPORE  
PREMIUM_SQL_PRO_COMPUTE_AP_TOKYO  
PREMIUM_SQL_PRO_COMPUTE_AP_SEOUL  
STANDARD_ALL_PURPOSE_COMPUTE  
STANDARD_ALL_PURPOSE_COMPUTE_(PHOTON)  
STANDARD_DLT_CORE_COMPUTE  
STANDARD_DLT_CORE_COMPUTE_(PHOTON)  
STANDARD_DLT_PRO_COMPUTE  
STANDARD_DLT_PRO_COMPUTE_(PHOTON)  
STANDARD_DLT_ADVANCED_COMPUTE  
STANDARD_DLT_ADVANCED_COMPUTE_(PHOTON)  
STANDARD_JOBS_COMPUTE  
STANDARD_JOBS_COMPUTE_(PHOTON)  
STANDARD_JOBS_LIGHT_COMPUTE  
Deprecated SKUs  
The following SKUs have been deprecated:  
Deprecated SKU Name  
Deprecation Date  
Replacement SKUs  
LIGHT_AUTOMATED_NON_OPSEC LIGHT_AUTOMATED_OPSEC  
March 2020  
STANDARD_JOBS_LIGHT_COMPUTE PREMIUM_JOBS_LIGHT_COMPUTE ENTERPRISE_JOBS_LIGHT_COMPUTE  
STANDARD_AUTOMATED_NON_OPSEC STANDARD_AUTOMATED_OPSEC  
March 2020  
STANDARD_JOBS_COMPUTE PREMIUM_JOBS_COMPUTE ENTERPRISE_JOBS_COMPUTE  
STANDARD_INTERACTIVE_NON_OPSEC STANDARD_INTERACTIVE_OPSEC  
March 2020  
STANDARD_ALL_PURPOSE_COMPUTE PREMIUM_ALL_PURPOSE_COMPUTE ENTERPRISE_ALL_PURPOSE_COMPUTE  
ENTERPRISE_ALL_PURPOSE_COMPUTE_(DLT) PREMIUM_ALL_PURPOSE_COMPUTE_(DLT) STANDARD_ALL_PURPOSE_COMPUTE_(DLT)  
April 2022  
ENTERPRISE_DLT_CORE_COMPUTE PREMIUM_DLT_CORE_COMPUTE STANDARD_DLT_CORE_COMPUTE  
ENTERPRISE_SERVERLESS_SQL_COMPUTE PREMIUM_SERVERLESS_SQL_COMPUTE STANDARD_SERVERLESS_SQL_COMPUTE  
June 2022  
ENTERPRISE_SERVERLESS_SQL_COMPUTE_US_EAST_N_VIRGINIA ENTERPRISE_SERVERLESS_SQL_COMPUTE_US_WEST_OREGON ENTERPRISE_SERVERLESS_SQL_COMPUTE_EUROPE_IRELAND ENTERPRISE_SERVERLESS_SQL_COMPUTE_AP_SYDNEY PREMIUM_SERVERLESS_SQL_COMPUTE_US_EAST_N_VIRGINIA PREMIUM_SERVERLESS_SQL_COMPUTE_US_WEST_OREGON PREMIUM_SERVERLESS_SQL_COMPUTE_EUROPE_IRELAND PREMIUM_SERVERLESS_SQL_COMPUTE_AP_SYDNEY"
22766	https://docs.databricks.com/en/admin/account-settings/usage-analysis.html	"Analyze usage data in Databricks
Analyze usage data in Databricks
This section describes how to make the data in the billable usage CSV file available to Databricks for analysis.  
The CSV file uses a format that is standard for commercial spreadsheet applications but requires a modification to be read by Apache Spark. You must use option(""escape"", ""\"""") when you create the usage table in Databricks.  
Total DBUs are the sum of the dbus column.  
Import the log using the Create Table UI  
You can use the Load data using the add data UI to import the CSV file into Databricks for analysis.  
Create a Spark DataFrame  
You can also use the following code to create the usage table from a path to the CSV file:  
df = (spark. read. option(""header"", ""true""). option(""inferSchema"", ""true""). option(""escape"", ""\""""). csv(""/FileStore/tables/usage_data.csv"")) df.createOrReplaceTempView(""usage"")  
If the file is stored in an S3 bucket, for example when it is used with log delivery, the code will look like the following. You can specify a file path or a directory. If you pass a directory, all files are imported. The following example specifies a file.  
df = (spark. read. option(""header"", ""true""). option(""inferSchema"", ""true""). option(""escape"", ""\""""). load(""s3://<bucketname>/<pathprefix>/billable-usage/csv/workspaceId=<workspace-id>-usageMonth=<month>.csv"")) df.createOrReplaceTempView(""usage"")  
The following example imports a directory of billable usage files:  
df = (spark. read. option(""header"", ""true""). option(""inferSchema"", ""true""). option(""escape"", ""\""""). load(""s3://<bucketname>/<pathprefix>/billable-usage/csv/"")) df.createOrReplaceTempView(""usage"")  
Create a Delta table  
To create a Delta table from the DataFrame (df) in the previous example, use the following code:  
(df.write .format(""delta"") .mode(""overwrite"") .saveAsTable(""database_name.table_name"") )  
Warning  
The saved Delta table is not updated automatically when you add or replace new CSV files. If you need the latest data, re-run these commands before you use the Delta table.  
Usage analysis dashboard notebook  
If you use billable usage delivery, you can use the following notebook to run a usage analysis dashboard by providing a path to the S3 bucket where your CSV files are stored and entering report parameters in a widget.  
The widget that you use to enter report parameters appears above the first notebook cell when you import the notebook to your Databricks workspace. The widget does not appear in the browser-only view of the notebook. Here’s an image of the widget:  
Usage analysis dashboard notebook  
Open notebook in new tab Copy link for import"
22767	https://docs.databricks.com/en/admin/account-settings/usage-detail-tags.html	"Monitor usage using tags  
To monitor cost and accurately attribute Databricks usage to your organization’s business units and teams (for chargebacks, for example), you can add custom tags to workspaces and compute resources. Databricks recommends using system tables (Public Preview) to view usage data. See Billable usage system table reference.  
These tags propagate both to usage logs and to AWS EC2 and AWS EBS instances for cost analysis.  
Tagged objects and resources
Tagged objects and resources
You can add custom tags for the following objects managed by Databricks:  
Object  
Tagging interface (UI)  
Tagging interface (API)  
Workspace  
N/A  
Account API  
Pool  
Pools UI in the Databricks workspace  
Instance Pool API  
All-purpose and job compute  
Compute UI in the Databricks workspace  
Clusters API  
SQL warehouse  
SQL warehouse UI in the Databricks workspace  
Warehouses API  
Warning  
Do not assign a custom tag with the key Name to a cluster. Every cluster has a tag Name whose value is set by Databricks. If you change the value associated with the key Name, the cluster can no longer be tracked by Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs.

Default tags
Default tags
Databricks adds the following default tags to all-purpose compute:  
Tag key  
Value  
Vendor  
Constant value: Databricks  
ClusterId  
Databricks internal ID of the cluster  
ClusterName  
Name of the cluster  
Creator  
Username (email address) of the user who created the cluster  
On job clusters, Databricks also applies the following default tags:  
Tag key  
Value  
RunName  
Job name  
JobId  
Job ID  
Databricks adds the following default tags to all pools:  
Tag key  
Value  
Vendor  
Constant value: Databricks  
DatabricksInstancePoolCreatorId  
Databricks internal ID of the user who created the pool  
DatabricksInstancePoolId  
Databricks internal ID of the pool  
On compute used by Lakehouse Monitoring, Databricks also applies the following tags:  
Tag key  
Value  
LakehouseMonitoring  
true  
LakehouseMonitoringTableId  
ID of the monitored table  
LakehouseMonitoringWorkspaceId  
ID of the workspace where the monitor was created  
LakehouseMonitoringMetastoreId  
ID of the metastore where the monitored table exists

Tag propagation  
Tags are propagated to AWS EC2 instances differently depending on whether or not the cluster was created from a pool.  
If a cluster is created from a pool, its EC2 instances inherit only the custom and default workspace tags and pool tags, not the cluster tags. Therefore if you want to create clusters from a pool, make sure to assign all of the custom cluster tags you need to the workspace or pool.  
If a cluster is not created from a pool, its tags propagate as expected to EC2 instances.  
Cluster and pool tags both propagate to DBU usage reports, whether or not the cluster was created from a pool.  
If there is a tag name conflict, Databricks default tags take precedence over custom tags and pool tags take precedence over cluster tags.  
Limitations
Limitations
Tag keys and values can contain only characters from the ISO 8859-1 (latin1) set. Tags containing other characters are ignored.  
If you change tag keys names or values, these changes apply only after cluster restart or pool expansion.  
If the cluster’s custom tags conflict with a pool’s custom tags, the cluster can’t be created.  
It can take up to one hour for custom workspace tags to propagate after any change.  
No more than 20 tags can be assigned to a workspace resource.

Tag enforcement with policies
Tag enforcement with policies
You can enforce tags on clusters using compute policies. For more information, see Custom tag enforcement.

Tag enforcement with IAM role"
22768	https://docs.databricks.com/en/admin/account-settings/usage-detail-tags.html	"Tag enforcement with IAM role
To ensure that certain tags are always populated when compute resources are created, you can apply a specific IAM policy to your account’s primary IAM role (the one created during account setup; contact your AWS administrator if you need access). The IAM policy should include explicit Deny statements for mandatory tag keys and optional values. Cluster creation will fail if required tags with one of the allowed values aren’t provided.  
For example, if you want to enforce Department and Project tags, with only specified values allowed for the former and a free-form non-empty value for the latter, you could apply an IAM policy like this one:  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Sid"": ""MandateLaunchWithTag1"", ""Effect"": ""Deny"", ""Action"": [ ""ec2:RunInstances"", ""ec2:CreateTags"" ], ""Resource"": ""arn:aws:ec2:region:accountId:instance/*"", ""Condition"": { ""StringNotEqualsIgnoreCase"": { ""aws:RequestTag/Department"": [ ""Deptt1"", ""Deptt2"", ""Deptt3"" ] } } }, { ""Sid"": ""MandateLaunchWithTag2"", ""Effect"": ""Deny"", ""Action"": [ ""ec2:RunInstances"", ""ec2:CreateTags"" ], ""Resource"": ""arn:aws:ec2:region:accountId:instance/*"", ""Condition"": { ""StringNotLike"": { ""aws:RequestTag/Project"": ""?*"" } } } ] }  
Both ec2:RunInstances and ec2:CreateTags actions are required for each tag for effective coverage of scenarios in which there are clusters that have only on-demand instances, only spot instances, or both.  
Tip  
Databricks recommends that you add a separate policy statement for each tag. The overall policy might become long, but it is easier to debug. See the IAM Policy Condition Operators Reference for a list of operators that can be used in a policy.  
Cluster creation errors due to an IAM policy show an encoded error message, starting with:  
Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.  
The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the action should not see. See DecodeAuthorizationMessage API (or CLI) for information about how to decode such messages."
22769	https://docs.databricks.com/en/admin/account-settings/verbose-logs.html	"Enable verbose audit logs  
Verbose audit logs are additional audit logs recorded whenever a query or command is run in your workspace. By default, these logs are not enabled in workspaces.  
To enable or disable verbose audit logs, do the following:  
As a workspace admin, go to the Databricks admin settings page.  
Click the Advanced tab.  
Next to Verbose Audit Logs, enable or disable the feature.  
When you enable or disable verbose logging, an auditable event is emitted in the category workspace with action workspaceConfKeys. The workspaceConfKeys request parameter is enableVerboseAuditLogs. The request parameter workspaceConfValues is true (feature enabled) or false (feature disabled).  
Verbose audit log events
Verbose audit log events
When you configure verbose audit logs, your logs include the following additional events:  
Service  
Action name  
Description  
Request parameters  
notebook  
runCommand  
Emitted after an interactive user runs a command in a notebook. A command corresponds to a cell in a notebook.  
notebookId  
executionTime  
status  
commandId  
commandText  
jobs  
runCommand  
Emitted after a command in a notebook is executed by a job run. A command corresponds to a cell in a notebook.  
jobId  
runId  
notebookId  
executionTime  
status  
commandId  
commandText  
databrickssql  
commandSubmit  
Runs when a command is submitted to Databricks SQL.  
commandText  
warehouseId  
commandId  
databrickssql  
commandFinish  
Runs when a command completes or a command is cancelled.  
warehouseId  
commandId  
Check the response field for additional information related to the command result:  
statusCode - The HTTP response code. This will be error 400 if it is a general error.  
errorMessage - Error message.  
Note  
In some cases for certain long-running commands, the errorMessage field might not be populated on failure.  
result: This field is empty."
22770	https://docs.databricks.com/en/admin/cloud-configurations/aws/imdsv2.html	"Enforce AWS Instance Metadata Service v2 on a workspace  
Important  
Because serverless compute resources automatically enforce IMDSv2, this setting does not apply to serverless compute resources.  
Instance metadata service (IMDS) is a service that runs locally on compute instances in AWS and is used to retrieve instance metadata. Crucially for security, instance metadata also includes credentials for the role associated with the instance. See Instance metadata and user data.  
In response to security concerns around IMDS, AWS created IMDSv2 (version 2) which reduces risk from a common attack pattern and replaces the request-and-response flow with a session-oriented flow. For details of the improvements, see this AWS blog article.  
As a workspace admin, you can enforce the use of IMDSv2 on clusters by enabling Enforce AWS instance metadata v2s in the Compute tab of the admin settings page. Databricks recommends that you configure your workspace to enforce IMDSv2. If your workspace was created after October 1, 2022, your workspace has this admin setting enabled by default.  
Requirements
Requirements
IMDSv2 enforcement does not support use of an isolated AWS Glue catalog. To disable isolation, see How to migrate and enforce IMDSv2 for all clusters.  
IMDSv2 enforcement requires use of a supported Databricks Runtime version as listed on Databricks Runtime release notes versions and compatibility, however the Light 2.4 Extended Support version is unsupported.

How to migrate and enforce IMDSv2 for all clusters
How to migrate and enforce IMDSv2 for all clusters
Warning  
Enforcing IMDSv2 causes any existing workloads to fail if they use IMDSv1 to fetch instance metadata.  
To enforce IMDSv2 on new, non-serverless clusters:  
IMDSv2 enforcement does not support use of an isolated AWS Glue catalog. To use Glue catalog, add one Spark conf line to your clusters to disable the isolation mode:  
spark.databricks.hive.metastore.glueCatalog.isolation.enabled false  
Upgrade your code to use IMDSv2.  
Upgrade any existing AWS CLIs and SDKs that your workloads use. Note that Databricks has already upgraded the SDK that is installed by default in the Databricks Runtime. Databricks recommends that you follow AWS’s upgrade guide to ensure a safe transition.  
Modify all notebooks in the workspace to remove any existing IMDSv1 usage and replace with IMDSv2 usage.  
For example, the following is IMDSv1 API client code:  
curl http://169.254.169.254/latest/meta-data/  
For that example, change it to IMDSv2 API client code:  
TOKEN=`curl -X PUT ""http://169.254.169.254/latest/api/token"" \ -H ""X-aws-ec2-metadata-token-ttl-seconds: 21600""` && \ \ curl -H ""X-aws-ec2-metadata-token: $TOKEN"" \ -v http://169.254.169.254/latest/meta-data/  
For more guidance and examples, see the the AWS article Retrieve instance metadata.  
Test your modified code to ensure it works correctly with IMDSv2.  
Enable enforcement of IMDSv2 for the workspace.  
As a workspace admin, Go to the settings page.  
Click the Compute tab.  
Click Enforce AWS instance metadata v2.  
Refresh the page to ensure that the setting took effect.  
Restart any running clusters to ensure that all EC2 instances have IMDSv2 enforced. If clusters are attached to a fleet instance pool, create a new fleet instance pool and recreate the clusters using the new fleet instance pool.  
Monitor the CloudWatch metric MetadataNoToken to ensure that your workspace is not making any active IMDSv1 calls."
22771	https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html	"Permissions in cross-account IAM roles  
This article lists permissions in the cross-account IAM role and the purpose of each role.  
The permissions are different based on how you configure your VPC.  
IAM permissions for Databricks-managed VPCs  
IAM permissions for customer-managed VPC  
IAM permissions for Databricks-managed VPCs"
22772	https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html	"IAM permissions for Databricks-managed VPCs
Databricks requires the following list of IAM permissions to operate and manage clusters in an effective manner. This configuration applies only to workspaces that use the default (Databricks-managed) VPC. To create the AWS cross-account role policy for use with the default Databricks-managed VPC, see Create an IAM role for workspace deployment.  
The following table lists Databricks IAM cross-account role permissions in the default configuration, the resources that they control, and the purpose for each permission.  
AWS IAM permission  
AWS resource  
Purpose  
ec2:AllocateAddress  
Elastic IP address  
Allocates an Elastic IP that is associated with the NAT Gateway used in secure cluster connectivity  
ec2:AssignPrivateIpAddresses  
Network Interface  
Assigns a private IP to EC2 instance.  
ec2:AssociateDhcpOptions  
DHCP  
Associates a set of DHCP options (or no DHCP options) with a VPC.  
ec2:AssociateIamInstanceProfile  
InstanceProfile  
Associate an instance profile with a running EC2 instance. This allows a Databricks pool instance to be used by clusters with different instance profiles throughout its lifetime in the pool.  
ec2:AssociateRouteTable  
RouteTable  
Associates a subnet with a route table.  
ec2:AttachInternetGateway  
InternetGateway  
Attaches an Internet gateway to a VPC, enabling connectivity between the Internet and the VPC. This is currently required to connect to S3 buckets and update code for the workers and spark containers.  
ec2:AttachVolume  
EBS Volume  
Attaches volume for EBS auto-scaling.  
ec2:AuthorizeSecurityGroupEgress  
SecurityGroup  
Adds egress rules to the security groups if required.  
ec2:AuthorizeSecurityGroupIngress  
SecurityGroup  
Adds ingress rules to the security groups.  
ec2:CancelSpotInstanceRequests  
SpotInstance  
Cancels spot instances.  
ec2:CreateDhcpOptions  
Dhcp  
Creates DHCP options.  
ec2:CreateFleet  
EC2 Fleet  
Creates an EC2 fleet (used with Databricks fleet clusters).  
ec2:CreateInternetGateway  
InternetGateway  
Creates an Internet Gateway.  
ec2:CreateLaunchTemplate  
Launch Template  
Creates a launch template (used with Databricks fleet clusters).  
ec2:CreateLaunchTemplateVersion  
Launch Template  
Creates a launch template version (used with Databricks fleet clusters).  
ec2:CreateNatGateway  
NatGateway  
Creates a NAT gateway.  
ec2:CreateRoute  
Route  
Creates routes during workspace setup.  
ec2:CreateRouteTable  
RouteTable  
Creates routes during workspace setup.  
ec2:CreateSecurityGroup  
SecurityGroup  
Creates security groups during initial setup  
ec2:CreateSubnet  
Subnet  
Create subnets for the VPC during workspace setup.  
ec2:CreateTags  
Tags  
Adds tags on Databricks resources.  
ec2:CreateVolume  
EBS Volume  
Creates volume.  
ec2:CreateVpc  
VPC  
Creates the Databricks-managed VPC.  
ec2:CreateVpcEndpoint  
VPCEndpoint  
Creates VPC endpoints as part of configuring the VPC.  
ec2:DeleteDhcpOptions  
DHCPOptions  
Deletes DHCPOptions  
ec2:DeleteFleets  
EC2 Fleet  
Deletes an EC2 fleet (used with Databricks fleet clusters)  
ec2:DeleteInternetGateway  
InternetGateway  
Deletes Internet Gateway during workspace deletion.  
ec2:DeleteLaunchTemplate  
Launch Template  
Deletes a launch template and all its versions.  
ec2:DeleteLaunchTemplateVersions  
Launch Template  
Deletes a version from a launch template (used with Databricks fleet clusters).  
ec2:DeleteNatGateway  
NatGateway  
Deletes NAT gateway as needed to setup the secure cluster connectivity relay.  
ec2:DeleteRoute  
Route  
Deletes routes.  
ec2:DeleteRouteTable  
RouteTable  
Deletes route table.  
ec2:DeleteSecurityGroup  
SecurityGroup  
Deletes security groups during workspace deletion.  
ec2:DeleteSubnet  
Subnet  
Deletes subnet.  
ec2:DeleteTags  
Tags  
Removes tags from cluster resources to allows Databricks pool instances to be reused by clusters with different tags.  
ec2:DeleteVolume  
EBS Volume  
Deletes a volume for EBS auto-scaling. See this page.  
ec2:DeleteVpc  
VPC  
Deletes the VPC when customers during workspace deletion.  
ec2:DeleteVpcEndpoints  
VPCEndpoints  
Deletes the VPC endpoints during workspace deletion.  
ec2:DescribeAvailabilityZones  
AvailabilityZones  
Gets a list of Availability Zones in a region so that Databricks can deploy resources in that zone.  
ec2:DescribeFleetHistory  
EC2 Fleet  
Lists events in an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeFleetInstances  
EC2 Fleet  
Lists instances in an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeFleets  
EC2 Fleet  
Describes details of an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeIamInstanceProfileAssociations  
InstanceProfile  
Checks the current instance profile that is set on an EC2 instance so that the right profile is set on a Databricks pool instance before it’s reused by a cluster.  
ec2:DescribeInstanceStatus  
Instance  
Confirms that Databricks AWS instances are healthy.  
ec2:DescribeInstances  
Instance  
Confirms that Databricks AWS instances are healthy.  
ec2:DescribeInternetGateways  
InternetGateway  
Describes InternetGateway to confirm that Databricks AWS instances have a route to the internet.  
ec2:DescribeLaunchTemplates  
Launch Template  
Deletes a launch template (used with Databricks fleet clusters).  
ec2:DescribeLaunchTemplateVersions  
Launch Template  
Describes details of launch template versions (used with Databricks fleet clusters).  
ec2:DescribeNatGateways  
NATGateway  
Describes a NAT Gateway to confirm that Databricks AWS instances have a route to the internet in the secure cluster connectivity architecture.  
ec2:DescribePrefixLists  
PrefixList  
Creates a prefix list ID to create an outbound security group rule that allows traffic from a VPC so that Databricks can access an AWS service through a gateway VPC endpoint.  
ec2:DescribeReservedInstancesOfferings  
Instance  
Describes Reserved Instance pricing in support of AWS spot instance pricing.  
ec2:DescribeRouteTables  
RouteTable  
Confirms that the route tables are set up correctly in the Databricks-managed VPC.  
ec2:DescribeSecurityGroups  
SecurityGroup"
22773	https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html	"ec2:DescribeRouteTables  
RouteTable  
Confirms that the route tables are set up correctly in the Databricks-managed VPC.  
ec2:DescribeSecurityGroups  
SecurityGroup  
Confirms that AWS security groups are set up correctly.  
ec2:DescribeSpotInstanceRequests  
Instance  
Describes spot instances.  
ec2:DescribeSpotPriceHistory  
SpotInstance  
Describes spot instances.  
ec2:DescribeSubnets  
Subnet  
Confirms that subnets are setup correctly in Databricks VPC.  
ec2:DescribeVolumes  
Volume  
Lists volumes.  
ec2:DescribeVpcs  
VPC  
Confirm that the workspace’s VPC was set up correctly.  
ec2:DetachInternetGateway  
InternetGateway  
Detaches the Databricks created Internet Gateway during workspace deletion.  
ec2:DisassociateIamInstanceProfile  
InstanceProfile  
Disassociates an instance profile from an EC2 instance so that xDatabricks pool instances can be used by clusters with different instance profiles.  
ec2:DisassociateRouteTable  
RouteTable  
Detaches the Databricks created route table during workspace deletion.  
ec2:GetLaunchTemplateData  
Launch Templates  
Gets config of a launch template (used with Databricks fleet clusters).  
ec2:GetSpotPlacementScores  
Availability Zones  
Gets list of AZs with best spot capacity for given instance type(s).  
ec2:ModifyFleet  
EC2 Fleet  
Modifies an EC2 fleet (used with Databricks fleet clusters).  
ec2:ModifyLaunchTemplate  
Launch Templates  
Modifies an existing launch template (used with Databricks fleet clusters).  
ec2:ModifyVpcAttribute  
VPCAttribute  
Configures the Databricks-managed VPC.  
ec2:ReleaseAddress  
Address  
Detach the Databricks created address during workspace deletion.  
ec2:ReplaceIamInstanceProfileAssociation  
InstanceProfile  
Swaps one instance profile for another on an EC2 instance so that Databricks pool instances can be used by clusters with different instance profiles.  
ec2:RequestSpotInstances  
SpotInstance  
Requests spot instances.  
ec2:RevokeSecurityGroupEgress  
SecurityGroup  
Updates Databricks-managed security groups if required.  
ec2:RevokeSecurityGroupIngress  
SecurityGroup  
Updates security groups.  
ec2:RunInstances  
Instance  
Launches AWS instances to create Spark Clusters. Also leveraged during scaling up an existing Spark cluster.  
ec2:TerminateInstances  
Instance  
Terminates Spark EC2 nodes during cluster scale down or to terminate a given Spark cluster.  
iam:CreateServiceLinkedRole  
ServiceLinkedRole  
Sets up support for spot instances.  
iam:PutRolePolicy  
RolePolicy  
Configures Databricks to use spot instances."
22774	https://docs.databricks.com/en/admin/cloud-configurations/aws/permissions.html	"IAM permissions for customer-managed VPC
If you use a customer-managed VPC, there’s a smaller set of permissions needed for the cross-account IAM role. This feature requires the Premium plan or above.  
To create the AWS cross-account role policy for use with a customer-managed VPC, see Option 2: Customer-managed VPC with default restrictions policy.  
The permissions can be scoped down further if needed. To create the AWS cross-account role policy for use with a customer-managed VPC with additional custom restrictions on resources, see Option 3: Customer-managed VPC with custom policy restrictions.  
The following table lists Databricks IAM cross-account role permissions for a customer-managed VPC, the resources that they control, and the purpose for each permission.  
AWS IAM permission  
AWS resource  
Purpose  
ec2:AssociateIamInstanceProfile  
InstanceProfile  
Associates an instance profile with a running EC2 instance so that a Databricks pool instance can be used by clusters with different instance profiles throughout its lifetime in the pool.  
ec2:AttachVolume  
Volume  
Attaches a volume.  
ec2:AuthorizeSecurityGroupEgress  
SecurityGroup  
Add egress rules to the security groups if required.  
ec2:AuthorizeSecurityGroupIngress  
SecurityGroup  
Adds ingress rules to the security groups.  
ec2:CancelSpotInstanceRequests  
SpotInstance  
Cancels spot instances.  
ec2:CreateFleet  
EC2 Fleet  
Creates an EC2 fleet (used with Databricks fleet clusters).  
ec2:CreateLaunchTemplate  
Launch Template  
Creates a launch template (used with Databricks fleet clusters).  
ec2:CreateTags  
Tags  
Adds tags on Databricks resources.  
ec2:CreateVolume  
Volume  
Creates a volume.  
ec2:DeleteFleets  
EC2 Fleet  
Deletes an EC2 fleet (used with Databricks fleet clusters)  
ec2:DeleteLaunchTemplate  
Launch Template  
Deletes a launch template and all its versions.  
ec2:DeleteLaunchTemplateVersions  
Launch Template  
Deletes a version from a launch template (used with Databricks fleet clusters).  
ec2:DeleteTags  
Tags  
Removes tags from cluster resources so that Databricks pool instances can be reused by clusters with different tags.  
ec2:DeleteVolume  
Volume  
Deletes a volume.  
ec2:DescribeAvailabilityZones  
AvailabilityZones  
Gets a list of Availability Zones in a region so that Databricks can deploy the resources in that zone.  
ec2:DescribeIamInstanceProfileAssociations  
InstanceProfile  
Checks the current instance profile set on an EC2 instance to confirm that the right profile is set on a Databricks pool instance before it’s reused by a cluster.  
ec2:DescribeInstanceStatus  
Instance  
Confirms that Databricks AWS instances are healthy.  
ec2:DescribeInstances  
Instance  
Confirm that Databricks AWS instances are healthy.  
ec2:DescribeInternetGateways  
InternetGateway  
Describes InternetGateway to confirm that Databricks AWS instances have a route to the internet.  
ec2:DescribeFleetHistory  
EC2 Fleet  
Lists events in an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeFleetInstances  
EC2 Fleet  
Lists instances in an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeFleets  
EC2 Fleet  
Describes details of an EC2 fleet (used with Databricks fleet clusters).  
ec2:DescribeLaunchTemplates  
Launch Template  
Deletes a launch template (used with Databricks fleet clusters).  
ec2:DescribeLaunchTemplateVersions  
Launch Template  
Describes details of launch template versions (used with Databricks fleet clusters).  
ec2:DescribeNatGateways  
NATGateway  
Describes NATGateway to confirm that Databricks AWS instances have a route to the internet in the secure cluster connectivity architecture.  
ec2:DescribeNetworkAcls  
NetworkAcl  
Confirms the correct Network ACL setup.  
ec2:DescribePrefixLists  
PrefixList  
Gets a list of prefix list IDs to create an outbound security group rule that allows traffic from a VPC to access an AWS service through a gateway VPC endpoint.  
ec2:DescribeReservedInstancesOfferings  
Instance  
Gets Reserved Instance pricing as the starting point for AWS spot instance pricing.  
ec2:DescribeRouteTables  
RouteTable  
Confirms that route tables are set up correctly in the VPC.  
ec2:DescribeSecurityGroups  
SecurityGroup  
Confirms that AWS security groups are set up correctly.  
ec2:DescribeSpotInstanceRequests  
Instance  
Describes spot instance.  
ec2:DescribeSpotPriceHistory  
SpotInstance  
Describes spot instances.  
ec2:DescribeSubnets  
Subnet  
Confirms that subnets are setup correctly in the VPC.  
ec2:DescribeVolumes  
Volume  
List volumes.  
ec2:DescribeVpcAttribute  
VPC  
Describes VPC attributes including but not limited to enableDnsHostnames.  
ec2:DescribeVpcs  
VPC  
Confirms that the Databricks workspace VPC was created.  
ec2:DetachVolume  
Volume  
Detaches an EBS volume from EC2 instances during cluster shutdown.  
ec2:DisassociateIamInstanceProfile  
InstanceProfile  
Disassociates an instance profile from an EC2 instance so that pool instances can be used by clusters with different instance profiles.  
ec2:GetLaunchTemplateData  
Launch Templates  
Gets config of a launch template (used with Databricks fleet clusters).  
ec2:ModifyFleet  
EC2 Fleet  
Modifies an EC2 fleet (used with Databricks fleet clusters).  
ec2:ModifyLaunchTemplate  
Launch Templates  
Modifies an existing launch template (used with Databricks fleet clusters).  
ec2:ReplaceIamInstanceProfileAssociation  
InstanceProfile  
Swaps one instance profile for another on an EC2 instance so that pool instances can be used by clusters with different instance profiles.  
ec2:RequestSpotInstances  
SpotInstance  
Requests spot instances.  
ec2:RevokeSecurityGroupEgress  
SecurityGroup  
Updates Databricks-managed security groups if required  
ec2:RevokeSecurityGroupIngress  
SecurityGroup  
Updates security groups.  
ec2:RunInstances  
Instance  
Launches AWS instances to create Spark Clusters. Also used to scale up an existing Spark cluster.  
ec2:TerminateInstances  
Instance  
Terminates Spark EC2 nodes during cluster scale down or to terminate a Spark cluster.  
iam:CreateServiceLinkedRole  
ServiceLinkedRole  
Sets up support for spot instances.  
iam:PutRolePolicy  
RolePolicy  
Configures Databricks to use spot instances."
22775	https://docs.databricks.com/en/admin/cloud-configurations/aws/sagemaker.html	"Set up AWS authentication for SageMaker deployment  
This article describes how to set up instance profiles to allow you to deploy MLflow models to AWS SageMaker. It is possible to use access keys for an AWS user with similar permissions as the IAM role specified here, but Databricks recommends using instance profiles to give a cluster permission to deploy to SageMaker.  
Step 1: Create an AWS IAM role and attach SageMaker permission policy
Step 1: Create an AWS IAM role and attach SageMaker permission policy
In the AWS console, go to the IAM service.  
Click the Roles tab in the sidebar.  
Click Create role.  
Under Select type of trusted entity, select AWS service.  
Under Choose the service that will use this role, click the EC2 service.  
Click Next: Permissions.  
In the Attach permissions policies screen, select AmazonSageMakerFullAccess.  
Click Next: Review.  
In the Role name field, enter a role name.  
Click Create role.  
In the Roles list, click the role name.  
Make note of your Role ARN, which is of the format arn:aws:iam::<account-id>:role/<role-name>.

Step 2: Add an inline policy for access to SageMaker deployment resources
Step 2: Add an inline policy for access to SageMaker deployment resources
Add a policy to the role.  
Click .  
Paste in the following JSON definition:  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Action"": [ ""s3:PutObjectAcl"", ""s3:PutObjectTagging"" ], ""Resource"": [ ""arn:aws:s3:::mlflow-sagemaker-*-<account-id>"", ""arn:aws:s3:::mlflow-sagemaker-*-<account-id>/*"" ], ""Effect"": ""Allow"" }, { ""Action"": [ ""iam:GetRole"" ], ""Resource"": [ ""arn:aws:iam::<account-id>:role/<role-name>"" ], ""Effect"": ""Allow"" }, { ""Action"": [ ""ecr:DescribeGitRepositories"" ], ""Resource"": [ ""arn:aws:ecr:*:<account-id>:repository/*"" ], ""Effect"": ""Allow"" } ] }  
These permissions are required to allow the Databricks cluster to:  
Obtain the new role’s canonical ARN.  
Upload permission-scoped objects to S3 for use by SageMaker endpoint servers.  
The role’s permissions will look like:

Step 3: Update the role’s trust policy
Step 3: Update the role’s trust policy
Add iam:AssumeRole access to sagemaker.amazonaws.com.  
Go to Role Summary > Trust relationships > Edit trust relationship.  
Paste and save the following JSON:  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Effect"": ""Allow"", ""Principal"": { ""Service"": ""ec2.amazonaws.com"" }, ""Action"": ""sts:AssumeRole"" }, { ""Effect"": ""Allow"", ""Principal"": { ""Service"": ""sagemaker.amazonaws.com"" }, ""Action"": ""sts:AssumeRole"" } ] }  
Your role’s trust relationships should resemble the following:

Step 4: Allow your Databricks workspace AWS role to pass the role
Step 4: Allow your Databricks workspace AWS role to pass the role
Go to your Databricks workspace AWS role.  
Click .  
Paste and save the following JSON definition:  
{ ""Version"": ""2012-10-17"", ""Statement"": [ { ""Action"": [ ""iam:PassRole"" ], ""Resource"": [ ""arn:aws:iam::<account-id>:role/<role-name>"" ], ""Effect"": ""Allow"" } ] }  
where account-id is the ID of the account running the AWS SageMaker service and role-name is the role you defined in Step 1.

Step 5: Create a Databricks cluster instance profile
Step 5: Create a Databricks cluster instance profile
In your Databricks workspace admin settings, go to the Security tab, then click Manage next to Instance profiles.  
Click Add instance profile.  
Paste in the instance profile ARN associated with the AWS role you created. This ARN is of the form arn:aws:iam::<account-id>:instance-profile/<role-name> and can be found in the AWS console:  
Click the Add button.  
For details, see Tutorial: Configure S3 access with an instance profile.

Step 6: Launch a cluster with the instance profile
Step 6: Launch a cluster with the instance profile
See Instance profiles."
22776	https://docs.databricks.com/en/admin/clusters/automatic-cluster-update.html	"Automatic cluster update  
Automatic cluster update ensures that all the clusters in a workspace are periodically updated to the latest host OS image and security updates. Account admins can schedule the maintenance window frequency, start date, and start time.  
If the compliance security profile is enabled, automatic cluster update is automatically enabled, but it can be enabled independently. See Automatic cluster update. If there are no updates for images for running compute resources, by default they are not restarted but you can configure the feature to force restart during the maintenance window.  
This applies to all compute resources that run in the classic compute plane: clusters, pools, classic SQL warehouses, and legacy Model Serving. It does not apply to serverless compute resources.  
Enabling this feature on a workspace requires that you add the Enhanced Security and Compliance add-on as described on the pricing page. This feature also requires the Enterprise pricing tier.  
By default, automatic cluster update is scheduled for the first Sunday of every month at 1:00 AM UTC. Account admins can use the Databricks account console to change the maintenance window frequency, start date, and start time. See Configure automatic cluster update.  
Enable automatic cluster update on a workspace
Enable automatic cluster update on a workspace
Important  
You must be an account admin to configure automatic cluster update. Although this setting is configured for each workspace, the controls for this feature are part of the account console UI, not the workspace admin console.  
If you enable the compliance security profile, this feature is automatically enabled. To separately enable automatic cluster update on a workspace, see Enable enhanced security and compliance features on a workspace.  
You can also set an account-level default for new workspaces to enable automatic cluster update initially. Alternatively, you can set an account-level default to enable the compliance security profile, which automatically enables automatic cluster update. See Set account-level defaults for new workspaces.  
Updates may take up to six hours to propagate to all environments and to downstream systems like billing. Workloads that are actively running continue with the settings that were active at the time of starting the cluster or other compute resource, and new settings will start applying the next time these workloads are started. This means that if a change is made late in the day or on the last day of the month, you might still see usage reported with the old settings early the next day or month.  
Restart any compute resources to ensure they immediately get the latest updates.

Set default for new workspaces to enable automatic cluster update
Set default for new workspaces to enable automatic cluster update
Account admins can set account-level defaults for new workspaces for the compliance security profile with optional compliance standards. See Set account-level defaults for new workspaces.

Configure automatic cluster update
Configure automatic cluster update
Go to the Databricks account console.  
Navigate directly to https://accounts.cloud.databricks.com.  
In the sidebar, click Workspaces.  
Click the name of your workspace.  
Click Security.  
Next to Automatic cluster update, click the Configure button.  
Set the maintenance frequency.  
For monthly schedules, choose which numbered week such as 1st or 3rd.  
For twice-monthly schedules, choose 1st and 3rd or 2nd and 4th.  
Set the day and time for your maintenance window.  
By default, compute resources only restart if updates are available. To force clusters and other compute resources to restart during this maintenance window regardless of the availability of a new update, select Always restart.  
Click Save."
22777	https://docs.databricks.com/en/admin/clusters/manage-ssd.html	"Manage SSD storage  
You can select either gp2 or gp3 as the AWS EBS SSD volume type for all clusters in a Databricks workspace. Databricks recommends you use gp3 for its cost savings compared to gp2. For technical information about gp2 and gp3, see Amazon EBS volume types.  
Note  
By default, the Databricks configuration sets the gp3 volume’s IOPS and throughput IOPS to match the maximum performance of a gp2 volume with the same volume size.  
To switch the Amazon EBS volume type from gp2 to gp3 for all clusters in a workspace, do the following:  
Ensure that you have sufficient quotas for gp3 volumes and gp3 storage modifications. Consider using your current gp2 quotas as a guideline. For more information, see Amazon Elastic Block Store endpoints and quotas.  
Sign in to the Databricks workspace as a Databricks admin.  
Access the workspace admin settings.  
In the Compute section, enable EBS SSD gp3.  
Click Confirm."
22778	https://docs.databricks.com/en/admin/clusters/personal-compute.html	"Manage the Personal Compute policy  
Personal Compute is a Databricks-managed policy available, by default, on all Databricks workspaces. Granting users access to this policy enables them to create single-machine compute resources in Databricks for their individual use. Users can create the personal compute resource quickly using shortcuts in either a notebook or the Compute page.  
With the Personal Compute policy, administrators have a simple solution for providing limited compute creation privileges. This removes the need for admins to grant users general compute privileges or to manually manage the creation of compute resources. This default policy also serves an an example policy for admins to clone and modify as needed, allowing them to set up appropriate compute governance patterns for their community of users.  
By default, all users in a Databricks account have access to the Personal Compute default policy. This can be changed by administrators.  
Personal Compute policy details
Personal Compute policy details
When a user has access to the Personal Compute policy, they can create Personal Compute resources from either the Compute page or a notebook. Personal Compute resources are all-purpose computes (priced according to all-purpose compute pricing) with the following properties:  
Personal Compute resources are single-node compute resources, meaning they have no workers and Spark running in local mode.  
They use the Single User access access mode and are Unity Catalog-compatible.  
The runtime is set to use the latest version of the Databricks Runtime for Machine Learning.  
Both standard instances and GPU-enabled instances are available.  
Auto-termination is set at 72 hours.  
No limits are set on the maximum number of Personal Compute resources individual users can create.

Managing access to the Personal Compute policy
Managing access to the Personal Compute policy
Workspace admins can manage access to the Personal Compute policy on individual workspaces using the policies UI.  
Account admins can enable or disable access to the Personal Compute policy for all users in an account using the Personal Compute account setting.  
From the account console, click Settings.  
Click the Feature enablement tab.  
Enable the Personal Compute setting to give all users in the account access to the Personal Compute policy. Or, switch the setting to Delegate if you want to policy to be managed at the workspace level.

Customize the Personal Compute policy
Customize the Personal Compute policy
Workspace admins can override or add policy rules for most Personal Compute settings.  
Note  
Workspace admins can’t edit the Personal Compute policy name, cluster type, cluster mode, or access mode.  
To customize a workspace’s Personal Compute policy:  
Click Compute in the sidebar.  
Click the Policies tab.  
Select the Personal Compute policy.  
Click Edit.  
Under the Definitions tab, click Edit.  
A modal appears where you can override policy definitions. In the Overrides section, add the updated definitions and click OK.  
In the example below, the spark_version rule is overridden. The spark_version from the default policy is ignored in favor of the override."
22779	https://docs.databricks.com/en/admin/clusters/policies.html	"Create and manage compute policies  
This article explains how to create and manage policies in your workspace. For information on writing policy definitions, see Compute policy reference.  
Note  
Policies require the Premium plan or above.  
What are compute policies?
What are compute policies?
A policy is a tool workspace admins can use to limit a user or group’s compute creation permissions based on a set of policy rules.  
Policies provide the following benefits:  
Limit users to creating clusters with prescribed settings.  
Limit users to creating a certain number of clusters.  
Simplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).  
Control cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).  
Enforce cluster-scoped library installations.

Create a policy
Create a policy
These are the basic instruction to create a policy. To learn how to define a policy, see Compute policy reference.  
Click Compute in the sidebar.  
Click the Policies tab.  
Click Create policy.  
Name the policy. Policy names are case insensitive.  
Optionally, select a policy family from the Family dropdown. This determines the template from which you build the policy.  
Enter a Description of the policy. This helps others know the purpose of the policy.  
In the Definitions tab, enter a policy definition.  
In the Libraries tab, add any compute-scoped libraries that you want the policy to install on the compute. See Add libraries to a policy.  
In the Permissions tab, assign permissions for the policy and optionally set the maximum number of resources a user can create using that policy.  
Click Create.

Use a policy families
Use a policy families
When you create a policy, you can choose to use a policy family. Policy families are Databricks-provide policy templates with pre-populated rules, designed to address common compute use cases.  
When using a policy family, the rules for your policy are inherited from the policy family. After selecting a policy family, you can create the policy as-is, or choose to add rules or override the given rules. For more on policy families, see Default policies and policy families.

Add libraries to a policy
Add libraries to a policy
You can add libraries to a policy so libraries are automatically installed on compute resources. You can add a maximum of 500 libraries to a policy.  
Note  
You may have previously added compute-scoped libraries using init scripts. Databricks recommends using compute policies instead of init scripts to install libraries.  
To add a library to your policy:  
At the bottom of the Create policy page, click the Libraries tab.  
Click Add library.  
Select one of the Library Source options, then follow the instructions as outlined below:  
Library source  
Instructions  
Workspace  
Select a workspace file or upload a Whl, zipped wheelhouse, JAR, ZIP, tar, or requirements.txt file. See Install libraries from workspace files  
Volumes  
Select a Whl, JAR, or requirements.txt file from a volume. See Install libraries from a volume.  
File Path/S3  
Select the library type and provide the full URI to the library object (for example: s3://bucket-name/path/to/library.whl). See Install libraries from object storage.  
PyPI  
Enter a PyPI package name. See PyPI package.  
Maven  
Specify a Maven coordinate. See Maven or Spark package.  
CRAN  
Enter the name of a package. See CRAN package.  
DBFS (Not recommended)  
Load a JAR or Whl file to the DBFS root. This is not recommended, as files stored in DBFS can be modified by any workspace user.  
Click Add.  
Effect of adding libraries to policies  
If you add libraries to a policy:  
Users can’t install or uninstall compute-scoped libraries on compute that uses this policy.  
Libraries configured through the UI, REST API, or CLI on existing compute are removed the next time the compute restarts.  
Dependency libraries for tasks that use this policy in jobs compute resources are disabled.

Policy permissions
Policy permissions
By default, workspace admins have permissions on all policies. Non-admin users must be granted permissions on a policy for them to have access to the policy.  
If a user has unrestricted cluster creation permissions, then they will also have access to the Unrestricted policy. This allows them to create fully configurable compute resources.  
If a user doesn’t have access to any policies, the policy dropdown does not display in their UI.  
Restrict the number of compute resources per users  
Policy permissions allow you to set a max number of compute resources per user. This determines how many resources a user can create using that policy. If the user exceeds the limit, the operation fails.  
To restrict the number of resources a user can create using a policy, enter a value into the Max compute resources per user setting under the Permissions tab in the policies UI.  
Note  
Databricks doesn’t proactively terminate resources to maintain the limit. If a user has three compute resources running with the policy and the workspace admin reduces the limit to one, the three resources will continue to run. Extra resources must be manually terminated to comply with the limit.

Manage a policy"
22780	https://docs.databricks.com/en/admin/clusters/policies.html	"Manage a policy
After you create a policy, you can edit, clone, and delete it.  
You can also monitor the policy’s adoption by viewing the compute resources that use the policy. From the Policies page, click the policy you want to view. Then click the Compute or Jobs tabs to see a list of resources that use the policy.  
Edit a policy  
You might want to edit a policy to update its permissions or its definitions. To edit a policy, select the policy from the Policies page then click Edit. After you click Edit you can click the Permissions tab to update the policy’s permissions. You can also then update the policy’s definition.  
Note  
After you update a policy’s definitions, the compute that uses that policy does not automatically update to adhere to the new policy rules, but the policy rules will be in effect if the user attempts to edit the compute resource.  
Clone a policy  
You can also use the cloning feature to create a new policy from an existing policy. Open the policy you want to clone then click the Clone button. Then change any values of the fields that you want to modify and click Create.  
Delete a policy  
Select the policy from the Policies page then click Delete. When asked if you’re sure you want to delete the policy, click Delete again.  
Any compute governed by a deleted policy can still run, but it cannot be edited unless the user has unrestricted cluster creation permissions."
22781	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"Compute policy reference  
This article is a reference for compute policy definitions. The articles includes a reference of the available policy attributes and limitation types. There are also sample policies you can reference for common use cases.  
What are policy definitions?
What are policy definitions?
Policy definitions are individual policy rules expressed in JSON. A definition can add a rule to any of the attributes controlled with the Clusters API. For example, these definitions set a default autotermination time, forbid users from using pools, and enforce the use of Photon:  
{ ""autotermination_minutes"" : { ""type"" : ""unlimited"", ""defaultValue"" : 4320, ""isOptional"" : true }, ""instance_pool_id"": { ""type"": ""forbidden"", ""hidden"": true }, ""runtime_engine"": { ""type"": ""fixed"", ""value"": ""PHOTON"", ""hidden"": true } }  
There can only be one limitation per attribute. An attribute’s path reflects the API attribute name. For nested attributes, the path concatenates the nested attribute names using dots. Attributes that aren’t defined in a policy definition won’t be limited.

Supported attributes"
22782	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"Supported attributes
Policies support all attributes controlled with the Clusters API. The type of restrictions you can place on attributes may vary per setting based on their type and relation to the UI elements. You cannot use policies to define compute permissions.  
You can also use policies to set the max DBUs per hour and cluster type. See Virtual attribute paths.  
The following table lists the supported policy attribute paths:  
Attribute path  
Type  
Description  
autoscale.max_workers  
optional number  
When hidden, removes the maximum worker number field from the UI.  
autoscale.min_workers  
optional number  
When hidden, removes the minimum worker number field from the UI.  
autotermination_minutes  
number  
A value of 0 represents no auto termination. When hidden, removes the auto termination checkbox and value input from the UI.  
aws_attributes.availability  
string  
Controls AWS availiability (SPOT, ON_DEMAND, or SPOT_WITH_FALLBACK)  
aws_attributes.ebs_volume_count  
number  
The number of AWS EBS volumes.  
aws_attributes.ebs_volume_size  
number  
The size (in GiB) of AWS EBS volumes.  
aws_attributes.ebs_volume_type  
string  
The type of AWS EBS volumes.  
aws_attributes.first_on_demand  
number  
Controls the number of nodes to put on on-demand instances.  
aws_attributes.instance_profile_arn  
string  
Controls the AWS instance profile.  
aws_attributes.spot_bid_price_percent  
number  
Controls the maximum price for AWS spot instances.  
aws_attributes.zone_id  
string  
Controls the AWS zone ID.  
cluster_log_conf.path  
string  
The destination URL of the log files.  
cluster_log_conf.region  
string  
The Region for the S3 location.  
cluster_log_conf.type  
S3, DBFS, or NONE  
The type of log destination.  
cluster_name  
string  
The cluster name.  
custom_tags.*  
string  
Controls specific tag values by appending the tag name, for example: custom_tags.<mytag>.  
data_security_mode  
string  
Sets the access mode of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION (shared access mode in the UI). A value of NONE means no security features are enabled.  
docker_image.basic_auth.password  
string  
The password for the Databricks Container Services image basic authentication.  
docker_image.basic_auth.username  
string  
The user name for the Databricks Container Services image basic authentication.  
docker_image.url  
string  
Controls the Databricks Container Services image URL. When hidden, removes the Databricks Container Services section from the UI.  
driver_node_type_id  
optional string  
When hidden, removes the driver node type selection from the UI.  
enable_elastic_disk  
boolean  
When hidden, removes the Enable autoscaling local storage checkbox from the UI.  
enable_local_disk_encryption  
boolean  
Set to true to enable, or false to disable, encrypting disks that are locally attached to the cluster (as specified through the API).  
init_scripts.*.workspace.destination init_scripts.*.volumes.destination init_scripts.*.s3.destination init_scripts.*.file.destination init_scripts.*.s3.region  
string  
* refers to the index of the init script in the attribute array. See Writing policies for array attributes.  
instance_pool_id  
string  
Controls the pool used by worker nodes if driver_instance_pool_id is also defined, or for all cluster nodes otherwise. If you use pools for worker nodes, you must also use pools for the driver node. When hidden, removes pool selection from the UI.  
driver_instance_pool_id  
string  
If specified, configures a different pool for the driver node than for worker nodes. If not specified, inherits instance_pool_id. If you use pools for worker nodes, you must also use pools for the driver node. When hidden, removes driver pool selection from the UI.  
node_type_id  
string  
When hidden, removes the worker node type selection from the UI.  
num_workers  
optional number  
When hidden, removes the worker number specification from the UI.  
runtime_engine  
string  
Determines whether the cluster uses Photon or not. Possible values are PHOTON or STANDARD.  
single_user_name  
string  
The user name for credential passthrough single user access.  
spark_conf.*  
optional string  
Control specific configuration values by appending the configuration key name. For example, spark_conf.spark.executor.memory.  
spark_env_vars.*  
optional string  
Control specific Spark environment variable values by appending the environment variable, for example: spark_env_vars.<environment variable name>.  
spark_version  
string  
The Spark image version name as specified through the API (the Databricks Runtime). You can also use special policy values that dynamically select the Databricks Runtime. See Special policy values for Databricks Runtime selection.  
ssh_public_keys.*  
string  
* refers to the index of the public key in the attribute array. See Writing policies for array attributes.  
workload_type.clients.jobs  
boolean  
Defines whether the compute resource can be used for jobs. See Prevent compute from being used with jobs.  
workload_type.clients.notebooks  
boolean  
Defines whether the compute resource can be used with notebooks. See Prevent compute from being used with jobs.  
Virtual attribute paths  
This table includes two additional synthetic attributes supported by policies:  
Attribute path  
Type  
Description  
dbus_per_hour  
number  
Calculated attribute representing the maximum DBUs a resource can use on an hourly basis including the driver node. This metric is a direct way to control cost at the individual compute level. Use with range limitation.  
cluster_type  
string  
Represents the type of cluster that can be created:  
all-purpose for Databricks all-purpose compute  
job for job compute created by the job scheduler  
dlt for compute created for Delta Live Tables pipelines  
Allow or block specified types of compute to be created from the policy. If the all-purpose value is not allowed, the policy is not shown in the all-purpose create compute UI. If the job value is not allowed, the policy is not shown in the create job compute UI.  
Special policy values for Databricks Runtime selection  
The spark_version attribute supports special values that dynamically map to a Databricks Runtime version based on the current set of supported Databricks Runtime versions.  
The following values can be used in the spark_version attribute:  
auto:latest: Maps to the latest GA Databricks Runtime version.  
auto:latest-ml: Maps to the latest Databricks Runtime ML version.  
auto:latest-lts: Maps to the latest long-term support (LTS) Databricks Runtime version."
22783	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"auto:latest-ml: Maps to the latest Databricks Runtime ML version.  
auto:latest-lts: Maps to the latest long-term support (LTS) Databricks Runtime version.  
auto:latest-lts-ml: Maps to the latest LTS Databricks Runtime ML version.  
auto:prev-major: Maps to the second-latest GA Databricks Runtime version. For example, if auto:latest is 14.2, then auto:prev-major is 13.3.  
auto:prev-major-ml: Maps to the second-latest GA Databricks Runtime ML version. For example, if auto:latest is 14.2, then auto:prev-major is 13.3.  
auto:prev-lts: Maps to the second-latest LTS Databricks Runtime version. For example, if auto:latest-lts is 13.3, then auto:prev-lts is 12.2.  
auto:prev-lts-ml: Maps to the second-latest LTS Databricks Runtime ML version. For example, if auto:latest-lts is 13.3, then auto:prev-lts is 12.2.  
Note  
Using these values does not make the compute auto-update when a new runtime version is released. A user must explicitly edit the compute for the Databricks Runtime version to change."
22784	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"Supported policy types
This section includes a reference for each of the available policy types. There are two categories of policy types: fixed policies and limiting policies.  
Fixed policies prevent user configuration on an attribute. The two types of fixed policies are:  
Fixed policy  
Forbidden policy  
Limiting policies limit a user’s options for configuring an attribute. Limiting policies also allow you to set default values and make attributes optional. See Additional limiting policy fields.  
Your options for limiting policies are:  
Allowlist policy  
Blocklist policy  
Regex policy  
Range policy  
Unlimited policy  
Fixed policy  
Fixed policies limit the attribute to the specified value. For attribute values other than numeric and boolean, the value must be represented by or convertible to a string.  
With fixed policies, you can also hide the attribute from the UI by setting the hidden field to true.  
interface FixedPolicy { type: ""fixed""; value: string | number | boolean; hidden?: boolean; }  
This example policy fixes the Databricks Runtime version and hides the field from the user’s UI:  
{ ""spark_version"": { ""type"": ""fixed"", ""value"": ""auto:latest-lts"", ""hidden"": true } }  
Forbidden policy  
A forbidden policy prevents users from configuring an attribute. Forbidden policies are only compatible with optional attributes.  
interface ForbiddenPolicy { type: ""forbidden""; }  
This policy forbids attaching pools to the compute for worker nodes. Pools are also forbidden for the driver node, because driver_instance_pool_id inherits the policy.  
{ ""instance_pool_id"": { ""type"": ""forbidden"" } }  
Allowlist policy  
An allowlist policy specifies a list of values the user can choose between when configuring an attribute.  
interface AllowlistPolicy { type: ""allowlist""; values: (string | number | boolean)[]; defaultValue?: string | number | boolean; isOptional?: boolean; }  
This allowlist example allows the user to select between two Databricks Runtime versions:  
{ ""spark_version"": { ""type"": ""allowlist"", ""values"": [ ""13.3.x-scala2.12"", ""12.2.x-scala2.12"" ] } }  
Blocklist policy  
The blocklist policy lists disallowed values. Since the values must be exact matches, this policy might not work as expected when the attribute is lenient in how the value is represented (for example, allowing leading and trailing spaces).  
interface BlocklistPolicy { type: ""blocklist""; values: (string | number | boolean)[]; defaultValue?: string | number | boolean; isOptional?: boolean; }  
This example blocks the user from selecting 7.3.x-scala2.12 as the Databricks Runtime.  
{ ""spark_version"": { ""type"": ""blocklist"", ""values"": [ ""7.3.x-scala2.12"" ] } }  
Regex policy  
A regex policy limits the available values to ones that match the regex. For safety, make sure your regex is anchored to the beginning and end of the string value.  
interface RegexPolicy { type: ""regex""; pattern: string; defaultValue?: string | number | boolean; isOptional?: boolean; }  
This example limits the Databricks Runtime versions a user can select from:  
{ ""spark_version"": { ""type"": ""regex"", ""pattern"": ""13\\.[3456].*"" } }  
Range policy  
A range policy limits the value to a specified range using the minValue and maxValue fields. The value must be a decimal number. The numeric limits must be representable as a double floating point value. To indicate lack of a specific limit, you can omit either minValue or maxValue.  
interface RangePolicy { type: ""range""; minValue?: number; maxValue?: number; defaultValue?: string | number | boolean; isOptional?: boolean; }  
This example limits the maximum amount of workers to 10:  
{ ""num_workers"": { ""type"": ""range"", ""maxValue"": 10 } }  
Unlimited policy  
The unlimited policy is used to make attributes required or to set the default value in the UI.  
interface UnlimitedPolicy { type: ""unlimited""; defaultValue?: string | number | boolean; isOptional?: boolean; }  
This example adds the COST_BUCKET tag to the compute:  
{ ""custom_tags.COST_BUCKET"": { ""type"": ""unlimited"" } }  
To set a default value for a Spark configuration variable, but also allow omitting (removing) it:  
{ ""spark_conf.spark.my.conf"": { ""type"": ""unlimited"", ""isOptional"": true, ""defaultValue"": ""my_value"" } }  
Additional limiting policy fields  
For limiting policy types you can specify two additional fields:  
defaultValue - The value that automatically populates in the create compute UI.  
isOptional - A limiting policy on an attribute automatically makes it required. To make the attribute optional, set the isOptional field to true.  
Note  
Default values don’t automatically get applied to compute created with the Clusters API. To apply default values using the API, add the parameter apply_policy_default_values to the compute definition and set it to true.  
This example policy specifies the default value id1 for the pool for worker nodes, but makes it optional. When creating the compute, you can select a different pool or choose not to use one. If driver_instance_pool_id isn’t defined in the policy or when creating the compute, the same pool is used for worker nodes and the driver node.  
{ ""instance_pool_id"": { ""type"": ""unlimited"", ""isOptional"": true, ""defaultValue"": ""id1"" } }"
22785	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"Writing policies for array attributes
Writing policies for array attributes
You can specify policies for array attributes in two ways:  
Generic limitations for all array elements. These limitations use the * wildcard symbol in the policy path.  
Specific limitations for an array element at a specific index. These limitation use a number in the path.  
For example, for the array attribute init_scripts, the generic paths start with init_scripts.* and the specific paths with init_scripts.<n>, where <n> is an integer index in the array (starting with 0). You can combine generic and specific limitations, in which case the generic limitation applies to each array element that does not have a specific limitation. In each case only one policy limitation will apply.  
The following sections show examples of common examples that use array attributes.  
Require inclusion-specific entries  
You cannot require specific values without specifying the order. For example:  
{ ""init_scripts.0.volumes.destination"": { ""type"": ""fixed"", ""value"": ""<required-script-1>"" }, ""init_scripts.1.volumes.destination"": { ""type"": ""fixed"", ""value"": ""<required-script-2>"" } }  
Require a fixed value of the entire list  
{ ""init_scripts.0.volumes.destination"": { ""type"": ""fixed"", ""value"": ""<required-script-1>"" }, ""init_scripts.*.volumes.destination"": { ""type"": ""forbidden"" } }  
Disallow the use altogether  
{ ""init_scripts.*.volumes.destination"": { ""type"": ""forbidden"" } }  
Allow entries that follow specific restriction  
{ ""init_scripts.*.volumes.destination"": { ""type"": ""regex"", ""pattern"": "".*<required-content>.*"" } }  
Fix a specific set of init scripts  
In case of init_scripts paths, the array can contain one of multiple structures for which all possible variants may need to be handled depending on the use case. For example, to require a specific set of init scripts, and disallow any variant of the other version, you can use the following pattern:  
{ ""init_scripts.0.volumes.destination"": { ""type"": ""fixed"", ""value"": ""<volume-paths>"" }, ""init_scripts.1.volumes.destination"": { ""type"": ""fixed"", ""value"": ""<volume-paths>"" }, ""init_scripts.*.workspace.destination"": { ""type"": ""forbidden"" }, ""init_scripts.*.s3.destination"": { ""type"": ""forbidden"" }, ""init_scripts.*.file.destination"": { ""type"": ""forbidden"" } }

Policy examples"
22786	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"Policy examples
This section includes policy examples you can use as references for creating your own policies. You can also use the Databricks-provided policy families as templates for common policy use cases.  
General compute policy  
Define limits on Delta Live Tables pipeline compute  
Simple medium-sized policy  
Job-only policy  
External metastore policy  
Prevent compute from being used with jobs  
Remove autoscaling policy  
Custom tag enforcement  
General compute policy  
A general purpose compute policy meant to guide users and restrict some functionality, while requiring tags, restricting the maximum number of instances, and enforcing timeout.  
{ ""instance_pool_id"": { ""type"": ""forbidden"", ""hidden"": true }, ""spark_version"": { ""type"": ""unlimited"", ""defaultValue"": ""auto:latest-ml"" }, ""node_type_id"": { ""type"": ""allowlist"", ""values"": [ ""i3.xlarge"", ""i3.2xlarge"", ""i3.4xlarge"" ], ""defaultValue"": ""i3.2xlarge"" }, ""driver_node_type_id"": { ""type"": ""fixed"", ""value"": ""i3.2xlarge"", ""hidden"": true }, ""autoscale.min_workers"": { ""type"": ""fixed"", ""value"": 1, ""hidden"": true }, ""autoscale.max_workers"": { ""type"": ""range"", ""maxValue"": 25, ""defaultValue"": 5 }, ""enable_elastic_disk"": { ""type"": ""fixed"", ""value"": true, ""hidden"": true }, ""autotermination_minutes"": { ""type"": ""fixed"", ""value"": 30, ""hidden"": true }, ""custom_tags.team"": { ""type"": ""fixed"", ""value"": ""product"" } }  
Define limits on Delta Live Tables pipeline compute  
Note  
When using policies to configure Delta Live Tables compute, Databricks recommends applying a single policy to both the default and maintenance compute.  
To configure a policy for a pipeline compute, create a policy with the cluster_type field set to dlt. The following example creates a minimal policy for a Delta Live Tables compute:  
{ ""cluster_type"": { ""type"": ""fixed"", ""value"": ""dlt"" }, ""num_workers"": { ""type"": ""unlimited"", ""defaultValue"": 3, ""isOptional"": true }, ""node_type_id"": { ""type"": ""unlimited"", ""isOptional"": true }, ""spark_version"": { ""type"": ""unlimited"", ""hidden"": true } }  
Simple medium-sized policy  
Allows users to create a medium-sized compute with minimal configuration. The only required field at creation time is compute name; the rest is fixed and hidden.  
{ ""instance_pool_id"": { ""type"": ""forbidden"", ""hidden"": true }, ""spark_conf.spark.databricks.cluster.profile"": { ""type"": ""forbidden"", ""hidden"": true }, ""autoscale.min_workers"": { ""type"": ""fixed"", ""value"": 1, ""hidden"": true }, ""autoscale.max_workers"": { ""type"": ""fixed"", ""value"": 10, ""hidden"": true }, ""autotermination_minutes"": { ""type"": ""fixed"", ""value"": 60, ""hidden"": true }, ""node_type_id"": { ""type"": ""fixed"", ""value"": ""i3.xlarge"", ""hidden"": true }, ""driver_node_type_id"": { ""type"": ""fixed"", ""value"": ""i3.xlarge"", ""hidden"": true }, ""spark_version"": { ""type"": ""fixed"", ""value"": ""auto:latest-ml"", ""hidden"": true }, ""enable_elastic_disk"": { ""type"": ""fixed"", ""value"": false, ""hidden"": true }, ""custom_tags.team"": { ""type"": ""fixed"", ""value"": ""product"" } }  
Job-only policy  
Allows users to create job compute to run jobs. Users cannot create all-purpose compute using this policy.  
{ ""cluster_type"": { ""type"": ""fixed"", ""value"": ""job"" }, ""dbus_per_hour"": { ""type"": ""range"", ""maxValue"": 100 }, ""instance_pool_id"": { ""type"": ""forbidden"", ""hidden"": true }, ""num_workers"": { ""type"": ""range"", ""minValue"": 1 }, ""node_type_id"": { ""type"": ""regex"", ""pattern"": ""[rmci][3-5][rnad]*.[0-8]{0,1}xlarge"" }, ""driver_node_type_id"": { ""type"": ""regex"", ""pattern"": ""[rmci][3-5][rnad]*.[0-8]{0,1}xlarge"" }, ""spark_version"": { ""type"": ""unlimited"", ""defaultValue"": ""auto:latest-lts"" }, ""custom_tags.team"": { ""type"": ""fixed"", ""value"": ""product"" } }  
External metastore policy  
Allows users to create compute with an admin-defined metastore already attached. This is useful to allow users to create their own compute without requiring additional configuration."
22787	https://docs.databricks.com/en/admin/clusters/policy-definition.html	"External metastore policy  
Allows users to create compute with an admin-defined metastore already attached. This is useful to allow users to create their own compute without requiring additional configuration.  
{ ""spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL"": { ""type"": ""fixed"", ""value"": ""jdbc:sqlserver://<jdbc-url>"" }, ""spark_conf.spark.hadoop.javax.jdo.option.ConnectionDriverName"": { ""type"": ""fixed"", ""value"": ""com.microsoft.sqlserver.jdbc.SQLServerDriver"" }, ""spark_conf.spark.databricks.delta.preview.enabled"": { ""type"": ""fixed"", ""value"": ""true"" }, ""spark_conf.spark.hadoop.javax.jdo.option.ConnectionUserName"": { ""type"": ""fixed"", ""value"": ""<metastore-user>"" }, ""spark_conf.spark.hadoop.javax.jdo.option.ConnectionPassword"": { ""type"": ""fixed"", ""value"": ""<metastore-password>"" } }  
Prevent compute from being used with jobs  
This policy prevents users from using the compute to run jobs. Users will only be able to use the compute with notebooks.  
{ ""workload_type.clients.notebooks"": { ""type"": ""fixed"", ""value"": true }, ""workload_type.clients.jobs"": { ""type"": ""fixed"", ""value"": false } }  
Remove autoscaling policy  
This policy disables autoscaling and allows the user to set the number of workers within a given range.  
{ ""num_workers"": { ""type"": ""range"", ""maxValue"": 25, ""minValue"": 1, ""defaultValue"": 5 } }  
Custom tag enforcement  
To add a compute tag rule to a policy, use the custom_tags.<tag-name> attribute.  
For example, any user using this policy needs to fill in a COST_CENTER tag with 9999, 9921, or 9531 for the compute to launch:  
{""custom_tags.COST_CENTER"": {""type"":""allowlist"", ""values"":[""9999"", ""9921"", ""9531"" ]}}"
22788	https://docs.databricks.com/en/admin/clusters/policy-families.html	"Default policies and policy families  
Note  
Policies require the Premium plan or above.  
This article describes the default compute policies available in your workspace and explains how to use them to create custom policies.  
In your workspace, four default policies are designed for four different use cases. The policies are the following:  
Personal Compute  
Shared Compute  
Power User Compute  
Job Compute  
Each policy includes rules that enforce best practices for its specific use case. Workspace admins can override or add rules to these policies.  
Additionally, you can create new custom policies using these default policies by utilizing policy families. See Use policy families to create custom policies.  
Who has access to the default policies?
Who has access to the default policies?
By default, workspace admins have access to all four policies. Non-admins only have access to the Personal Compute policy but can be granted access to any default policy.  
To edit the permissions of a default policy:  
Click Compute in the workspace sidebar.  
Click the Policies tab, then click the policy you want to edit.  
Click the Edit button, then the Permissions tab.  
Update the policy permissions then click the Update button.

Personal Compute
Personal Compute
The Personal Compute policy is available to all users in your workspace by default (unless disabled at the account level).  
The Personal Compute policy allows users to easily create a single-node compute resource with minimal configuration options. See Manage the Personal Compute policy.

Shared Compute
Shared Compute
The Shared Compute policy allows users to create larger multi-node resource intended for multiple users to share.  
Key details for this policy include:  
All-purpose compute only (no jobs)  
Uses the Shared access mode to allow multiple users to use this resource  
Defaults to latest LTS Databricks Runtime version  
By default, can only be used by workspace admins

Power User Compute
Power User Compute
The Power User Compute policy allows users to create larger multi-node resources. The policy is intended for single-user workloads that require more compute resources than Personal Compute allows.  
Key details for this policy include:  
All-purpose compute only (no jobs)  
Uses the Shared access mode to allow multiple users to use this resource  
Defaults to the latest Databricks Runtime ML version (not the latest Databricks Runtime LTS version)  
By default, can only be used by workspace admins

Job Compute
Job Compute
The Job Compute policy allows users to create a general-purpose default compute for jobs.  
Key details for this policy include:  
Job compute only  
Defaults to latest Databricks Runtime LTS version  
By default, only workspace admins can use this policy

Use policy families to create custom policies
Use policy families to create custom policies
A policy family is a Databricks-provided template that you can use to create custom policies. When you create a new policy, you can choose to create it in a policy family. You can select any of the default policies to be the family for your new policy.  
Note  
You cannot create, edit, or delete policy families.  
To create a policy using policy families:  
From the Policies dashboard, click Create policy.  
Enter a name and descrption of the policy.  
From the Family dropdown, select the policy whose definintions you want to inherit.  
After you select a family, the policy definitions get populated automatically.  
Click Edit to add or override definitions in the policy.  
Click OK then Create."
22789	https://docs.databricks.com/en/admin/clusters/web-terminal.html	"Enable the web terminal  
To use the web terminal on your clusters, an account admin can configure the web terminal as follows:  
Log in to the account console.  
Click Settings.  
Click the Feature enablement tab.  
Under Web Terminal, you can configure whether the web terminal feature should be on or off and whether this value should be enforced on all workspaces.  
If the account administrator chooses not to enforce the web terminal feature, workspace administrators can choose to enable or disable the web terminal on a per-workspace basis as follows:  
Go to the settings page.  
Click the Compute tab.  
Click the Web terminal toggle.  
Note  
After you change the setting in either the account console or admin settings, you may need to wait a few minutes for the configuration value to tke effect. If you changed the setting in the admin settings, you also need to refresh your page for changes to take effect in your local session.  
If the account administrator has enforced the web terminal setting on all workspaces, changes in the workspace admin settings page will have no effect.  
Manage the web terminal per cluster
Manage the web terminal per cluster
To prevent users from accessing a cluster using the web terminal:  
Do not grant any users CAN ATTACH TO permission.  
To enable notebook access but not web terminal, set the DISABLE_WEB_TERMINAL=true environment variable in the cluster configuration.

Auto termination
Auto termination
When you log out of a Databricks workspace or the feature is disabled in a workspace, active web terminal sessions terminate within 2 minutes.  
When a cluster owner removes another user’s permission for CAN ATTACH TO on a cluster or the user is removed from the workspace, the user’s active web terminal sessions terminate within 2 minutes.

WAF configuration
WAF configuration
If your Databricks deployment uses a Web Application Firewall (WAF), you must add the path /driver-proxy/* to its allow list.

Requirements
Requirements
Databricks web terminal is available with a few exceptions as noted in limitations."
22790	https://docs.databricks.com/en/admin/disaster-recovery.html	"Disaster recovery  
A clear disaster recovery pattern is critical for a cloud-native data analytics platform such as Databricks. It’s critical that your data teams can use the Databricks platform even in the rare case of a regional service-wide cloud-service provider outage, whether caused by a regional disaster like a hurricane or earthquake, or other source.  
Databricks is often a core part of an overall data ecosystem that includes many services, including upstream data ingestion services (batch/streaming), cloud native storage such as Amazon S3, downstream tools and services such as business intelligence apps, and orchestration tooling. Some of your use cases might be particularly sensitive to a regional service-wide outage.  
This article describes concepts and best practices for a successful interregional disaster recovery solution for the Databricks platform.  
Disaster recovery overview
Disaster recovery overview
Disaster recovery involves a set of policies, tools, and procedures that enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster. A large cloud service like AWS serves many customers and has built-in guards against a single failure. For example, a region is a group of buildings connected to different power sources to guarantee that a single power loss will not shut down a region. However, cloud region failures can happen, and the degree of disruption and its impact on your organization can vary.  
Before implementing a disaster recovery plan, it’s important to understand the difference between disaster recovery (DR) and high availability (HA).  
High availability is a resiliency characteristic of a system. High availability ensures a minimum level of operational performance that is usually defined in terms of consistent uptime or percentage of uptime. High availability is implemented in place (in the same region as your primary system) by designing it as a feature of the primary system. For example, cloud services like AWS have high-availability services such as Amazon S3. High availability does not require significant explicit preparation from the Databricks customer.  
In contrast, a disaster recovery plan requires decisions and solutions that work for your specific organization to handle a larger regional outage for critical systems. This article discusses common disaster recovery terminology, common solutions, and some best practices for disaster recovery plans with Databricks.

Terminology
Terminology
Region terminology  
This article uses the following definitions for regions:  
Primary region: The geographic region in which users run typical daily interactive and automated data analytics workloads.  
Secondary region: The geographic region in which IT teams move data analytics workloads temporarily during an outage in the primary region.  
Geo-redundant storage: AWS has geo-redundant storage across regions for persisted buckets using an asynchronous storage replication process.  
Important  
For disaster recovery processes, Databricks recommends that you do not rely on geo-redundant storage for cross-region duplication of data such as your root S3 bucket. In general, use Deep Clone for Delta Tables and convert data to Delta format to use Deep Clone if possible for other data formats.  
Deployment status terminology  
This article uses the following definitions of deployment status:  
Active deployment: Users can connect to an active deployment of a Databricks workspace and run workloads. Jobs are scheduled periodically using Databricks scheduler or other mechanism. Data streams can be executed on this deployment as well. Some documents might refer to an active deployment as a hot deployment.  
Passive deployment: Processes do not run on a passive deployment. IT teams can setup automated procedures to deploy code, configuration, and other Databricks objects to the passive deployment. A deployment becomes active only if a current active deployment is down. Some documents might refer to a passive deployment as a cold deployment.  
Important  
A project can optionally include multiple passive deployments in different regions to provide additional options for resolving regional outages.  
Generally speaking, a team has only one active deployment at a time, in what is called an active-passive disaster recovery strategy. There is a less common disaster recovery solution strategy called active-active, in which there are two simultaneous active deployments.  
Disaster recovery industry terminology  
There are two important industry terms that you must understand and define for your team:  
Recovery point objective: A recovery point objective (RPO) is the maximum targeted period in which data (transactions) might be lost from an IT service due to a major incident. Your Databricks deployment does not store your main customer data. That is stored in separate systems such as Amazon S3 or other data sources under your control. The Databricks control plane stores some objects in part or in full, such as jobs and notebooks. For Databricks, the RPO is defined as the maximum targeted period in which objects such as job and notebook changes can be lost. Additionally, you are responsible for defining the RPO for your own customer data in Amazon S3 or other data sources under your control.  
Recovery time objective: The recovery time objective (RTO) is the targeted duration of time and a service level within which a business process must be restored after a disaster.  
Disaster recovery and data corruption  
A disaster recovery solution does not mitigate data corruption. Corrupted data in the primary region is replicated from the primary region to a secondary region and is corrupted in both regions. There are other ways to mitigate this kind of failure, for example Delta time travel.

Typical recovery workflow"
22791	https://docs.databricks.com/en/admin/disaster-recovery.html	"Typical recovery workflow
A Databricks disaster recovery scenario typically plays out in the following way:  
A failure occurs in a critical service you use in your primary region. This can be a data source service or a network that impacts the Databricks deployment.  
You investigate the situation with the cloud provider.  
If you conclude that your company cannot wait for the problem to be remediated in the primary region, you may decide you need failover to a secondary region.  
Verify that the same problem does not also impact your secondary region.  
Fail over to a secondary region.  
Stop all activities in the workspace. Users stop workloads. Users or administrators are instructed to make a backup of the recent changes if possible. Jobs are shut down if they haven’t already failed due to the outage.  
Start the recovery procedure in the secondary region. The recovery procedure updates routing and renaming of the connections and network traffic to the secondary region.  
After testing, declare the secondary region operational. Production workloads can now resume. Users can log in to the now active deployment. You can retrigger scheduled or delayed jobs.  
For detailed steps in a Databricks context, see Test failover.  
At some point, the problem in the primary region is mitigated and you confirm this fact.  
Restore (fail back) to your primary region.  
Stop all work on the secondary region.  
Start the recovery procedure in the primary region. The recovery procedure handles routing and renaming of the connection and network traffic back to the primary region.  
Replicate data back to the primary region as needed. To reduce complexity, perhaps minimize how much data needs to be replicated. For example, if some jobs are read-only when run in the secondary deployment, you may not need to replicate that data back to your primary deployment in the primary region. However, you may have one production job that needs to run and may need data replication back to the primary region.  
Test the deployment in the primary region.  
Declare your primary region operational and that it is your active deployment. Resume production workloads.  
For more information about restoring to your primary region, see Test restore (failback).  
Important  
During these steps, some data loss might happen. Your organization must define how much data loss is acceptable and what you can do to mitigate this loss.

Step 1: Understand your business needs
Step 1: Understand your business needs
Your first step is to define and understand your business needs. Define which data services are critical and what is their expected RPO and RTO.  
Research the real-world tolerance of each system, and remember that disaster recovery failover and failback can be costly and carries other risks. Other risks might include data corruption, data duplicated if you write to the wrong storage location, and users who log in and make changes in the wrong places.  
Map all of the Databricks integration points that affect your business:  
Does your disaster recovery solution need to accommodate interactive processes, automated processes, or both?  
Which data services do you use? Some may be on-premises.  
How does input data get to the cloud?  
Who consumes this data? What processes consume it downstream?  
Are there third-party integrations that need to be aware of disaster recovery changes?  
Determine the tools or communication strategies that can support your disaster recovery plan:  
What tools will you use to modify network configurations quickly?  
Can you predefine your configuration and make it modular to accommodate disaster recovery solutions in a natural and maintainable way?  
Which communication tools and channels will notify internal teams and third-parties (integrations, downstream consumers) of disaster recovery failover and failback changes? And how will you confirm their acknowledgement?  
What tools or special support will be needed?  
What services if any will be shut down until complete recovery is in place?

Step 2: Choose a process that meets your business needs"
22792	https://docs.databricks.com/en/admin/disaster-recovery.html	"Step 2: Choose a process that meets your business needs
Your solution must replicate the correct data in both control plane, compute plane, and data sources. Redundant workspaces for disaster recovery must map to different control planes in different regions. You must keep that data in sync periodically using a script-based solution, either a synchronization tool or a CI/CD workflow. There is no need to synchronize data from within the compute plane network itself, such as from Databricks Runtime workers.  
If you use the customer-managed VPC feature (not available with all subscription and deployment types), you can consistently deploy these networks in both regions using template-based tooling such as Terraform.  
Additionally, you need to ensure that your data sources are replicated as needed across regions.  
General best practices  
General best practices for a successful disaster recovery plan include:  
Understand which processes are critical to the business and have to run in disaster recovery.  
Clearly identify which services are involved, which data is being processed, what the data flow is and where it is stored  
Isolate the services and data as much as possible. For example, create a special cloud storage container for the data for disaster recovery or move Databricks objects that are needed during a disaster to a separate workspace.  
It is your responsibility to maintain integrity between primary and secondary deployments for other objects that are not stored in the Databricks Control Plane.  
Warning  
It is a best practice not to store data in the root Amazon S3 bucket that is used for DBFS root access for the workspace. That DBFS root storage is unsupported for production customer data. Databricks also recommends not to store libraries, configuration files, or init scripts in this location.  
For data sources, where possible, it is recommended that you use native AWS tools for replication and redundancy to replicate data to the disaster recovery regions.  
Choose a recovery solution strategy  
Typical disaster recovery solutions involve two (or possibly more) workspaces. There are several strategies you can choose. Consider the potential length of the disruption (hours or maybe even a day), the effort to ensure that the workspace is fully operational, and the effort to restore (fail back) to the primary region.  
Active-passive solution strategy  
An active-passive solution is the most common and the easiest solution, and this type of solution is the focus of this article. An active-passive solution synchronizes data and object changes from your active deployment to your passive deployment. If you prefer, you could have multiple passive deployments in different regions, but this article focuses on the single passive deployment approach. During a disaster recovery event, the passive deployment in the secondary region becomes your active deployment.  
There are two main variants of this strategy:  
Unified (enterprise-wise) solution: Exactly one set of active and passive deployments that support the entire organization.  
Solution by department or project: Each department or project domain maintains a separate disaster recovery solution. Some organizations want to decouple disaster recovery details between departments and use different primary and secondary regions for each team based on the unique needs of each team.  
There are other variants, such as using a passive deployment for read-only use cases. If you have workloads that are read-only, for example user queries, they can run on a passive solution at any time if they do not modify data or Databricks objects such as notebooks or jobs.  
Active-active solution strategy  
In an active-active solution, you run all data processes in both regions at all times in parallel. Your operations team must ensure that a data process such as a job is marked as complete only when it finishes successfully on both regions. Objects cannot be changed in production and must follow a strict CI/CD promotion from development/staging to production.  
An active-active solution is the most complex strategy, and because jobs run in both regions, there is additional financial cost.  
Just as with the active-passive strategy, you can implement this as a unified organization solution or by department.  
You may not need an equivalent workspace in the secondary system for all workspaces, depending on your workflow. For example, perhaps a development or staging workspace may not need a duplicate. With a well-designed development pipeline, you may be able to reconstruct those workspaces easily if needed.  
Choose your tooling  
There are two main approaches for tools to keep data as similar as possible between workspaces in your primary and secondary regions:  
Synchronization client that copies from primary to secondary: A sync client pushes production data and assets from the primary region to the secondary region. Typically this runs on a scheduled basis.  
CI/CD tooling for parallel deployment: For production code and assets, use CI/CD tooling that pushes changes to production systems simultaneously to both regions. For example, when pushing code and assets from staging/development to production, a CI/CD system makes it available in both regions at the same time. The core idea is to treat all artifacts in a Databricks workspace as infrastructure-as-code. Most artifacts could be co-deployed to both primary and secondary workspaces, while some artifacts may need to be deployed only after a disaster recovery event. For tools, see Automation scripts, samples, and prototypes.  
The following diagram contrasts these two approaches.  
Depending on your needs, you could combine the approaches. For example, use CI/CD for notebook source code but use synchronization for configuration like pools and access controls.  
The following table describes how to handle different types of data with each tooling option.  
Description  
How to handle with CI/CD tooling  
How to handle with sync tool  
Source code: notebook source exports and source code for packaged libraries  
Co-deploy both to primary and secondary.  
Synchronize source code from primary to secondary.  
Users and groups  
Manage metadata as config in Git. Alternatively, use the same identity provider (IdP) for both workspaces. Co-deploy user and group data to primary and secondary deployments.  
Use SCIM or other automation for both regions. Manual creation is not recommended, but if used must be done for both at the same time. If you use a manual setup, create a scheduled automated process to compare the list of users and group between the two deployments.  
Pool configurations  
Can be templates in Git. Co-deploy to primary and secondary. However, min_idle_instances in secondary must be zero until the disaster recovery event.  
Pools created with any min_idle_instances when they are synced to secondary workspace using the API or CLI.  
Job configurations  
Can be templates in Git. For primary deployment, deploy the job definition as is. For secondary deployment, deploy the job and set the concurrencies to zero. This disables the job in this deployment and prevents extra runs. Change the concurrencies value after the secondary deployment becomes active.  
If the jobs run on existing <interactive> clusters for some reason, then the sync client needs to map to the corresponding cluster_id in the secondary workspace.  
Access control lists (ACLs)  
Can be templates in Git. Co-deploy to primary and secondary deployments for notebooks, folders, and clusters. However, hold the data for jobs until the disaster recovery event."
