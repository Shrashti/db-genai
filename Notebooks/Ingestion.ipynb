{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Databricks Documentation RAG Ingestion Pipeline\n",
    "### This notebook implements a high-quality RAG ingestion pipeline with:\n",
    "### Data reading from Databricks tables\n",
    "### Semantic chunking for context-aware splitting\n",
    "### Metadata extraction for enhanced retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion_pipeline import (\n",
    "    read_databricks_docs,\n",
    "    SemanticChunker,\n",
    "    MetadataExtractor,\n",
    "    process_document,\n",
    "    process_all_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data from Databricks Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documentation data\n",
    "docs_df = read_databricks_docs(\"databricks_databricks_documentation_dataset.v01.docs\")\n",
    "display(docs_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Test Semantic Chunking on a Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize semantic chunker\n",
    "chunker = SemanticChunker(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    similarity_threshold=0.5,\n",
    "    min_chunk_size=200,\n",
    "    max_chunk_size=1000,\n",
    "    overlap_sentences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample document\n",
    "sample_doc = docs_df.first()\n",
    "chunks = chunker.chunk_text(sample_doc['content'])\n",
    "\n",
    "\n",
    "print(f\"Created {len(chunks)} semantic chunks from sample document\")\n",
    "print(\"\\nChunk Statistics:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  - Characters: {chunk.char_count}\")\n",
    "    print(f\"  - Sentences: {chunk.sentence_count}\")\n",
    "    print(f\"  - Preview: {chunk.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3: Test Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metadata extractor\n",
    "extractor = MetadataExtractor()\n",
    "\n",
    "# Extract metadata from sample document\n",
    "metadata = extractor.extract_metadata(\n",
    "    doc_id=sample_doc['id'],\n",
    "    url=sample_doc['url'],\n",
    "    content=sample_doc['content']\n",
    ")\n",
    "\n",
    "print(\"Extracted Metadata:\")\n",
    "print(f\"  Title: {metadata['title']}\")\n",
    "print(f\"  Document Type: {metadata['document_type']}\")\n",
    "print(f\"  URL Category: {metadata['url_category']}\")\n",
    "print(f\"  URL Path: {metadata['url_path']}\")\n",
    "print(f\"\\nHeaders ({len(metadata['headers'])}):\")\n",
    "for header in metadata['headers'][:5]:\n",
    "    print(f\"  {'#' * header['level']} {header['text']}\")\n",
    "\n",
    "print(f\"\\nCode Blocks: {len(metadata['code_blocks'])}\")\n",
    "for i, cb in enumerate(metadata['code_blocks'][:3]):\n",
    "    print(f\"  Block {i+1}: {cb['language']} ({cb['length']} chars)\")\n",
    "\n",
    "print(f\"\\nLinks: {len(metadata['links'])}\")\n",
    "for link in metadata['links'][:5]:\n",
    "    print(f\"  - {link['text']}: {link['url']}\")\n",
    "\n",
    "print(f\"\\nKeywords:\")\n",
    "for key, values in metadata['keywords'].items():\n",
    "    print(f\"  {key}: {values[:5]}\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "for key, value in metadata['statistics'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Process Single Document with Both Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks = process_document(\n",
    "    doc_id=sample_doc['id'],\n",
    "    url=sample_doc['url'],\n",
    "    content=sample_doc['content'],\n",
    "    chunker=chunker,\n",
    "    extractor=extractor\n",
    ")\n",
    "\n",
    "print(f\"Processed document into {len(processed_chunks)} chunks with metadata\")\n",
    "print(f\"\\nFirst chunk details:\")\n",
    "chunk = processed_chunks[0]\n",
    "for key, value in chunk.items():\n",
    "    if key != 'text':  # Skip full text for brevity\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(f\"\\nText preview: {chunk['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Process All Documents (Batch Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all documents and save to Delta table\n",
    "output_table = \"databricks_databricks_documentation_dataset.v01.processed_chunks\"\n",
    "\n",
    "chunks_df = process_all_documents(\n",
    "    docs_df=docs_df,\n",
    "    output_table=output_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processing statistics\n",
    "print(\"Document Type Distribution:\")\n",
    "chunks_df.groupBy('doc_type').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "\n",
    "print(\"Chunk Size Statistics:\")\n",
    "chunks_df.select('char_count', 'sentence_count').describe().show()\n",
    "\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"Sample Processed Chunks:\")\n",
    "display(chunks_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
