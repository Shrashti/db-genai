{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Guardrails and Conversational Agent for Databricks Documentation\n",
                "\n",
                "This notebook demonstrates:\n",
                "1. **Input Guardrails** - Validate queries are Databricks-related\n",
                "2. **Output Guardrails** - Ensure responses stay on-topic\n",
                "3. **Conversational Memory** - Multi-turn conversations\n",
                "4. **Rejection Handling** - Polite responses for off-topic queries\n",
                "5. **MLflow Tracking** - Monitor guardrail performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required modules\n",
                "import mlflow\n",
                "from databricks_langchain import VectorSearchRetrieverTool, ChatDatabricks\n",
                "import sys
sys.path.insert(0, "./helpers")
from guardrails import InputGuardrail, OutputGuardrail, RejectionHandler, GuardrailMetrics\n",
                "import sys
sys.path.insert(0, "./helpers")
from conversational_agent import DatabricksDocAgent, create_databricks_agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "VECTOR_SEARCH_ENDPOINT_NAME = \"databricks_doc_index\"\n",
                "VECTOR_SEARCH_INDEX = \"workspace.default.databricks_index\"\n",
                "LLM_ENDPOINT = \"databricks-qwen3-next-80b-a3b-instruct\"\n",
                "\n",
                "# Set MLflow experiment\n",
                "mlflow.set_experiment(\"/Users/shrashti.90@gmail.com/databricks-guardrails-agent\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Testing Individual Guardrails\n",
                "\n",
                "Let's first test the input and output guardrails independently."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Input Guardrail Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create input guardrail with different strictness levels\n",
                "input_guardrail_moderate = InputGuardrail(\n",
                "    llm_endpoint=LLM_ENDPOINT,\n",
                "    strictness=\"moderate\",\n",
                "    log_to_mlflow=False  # Disable for testing\n",
                ")\n",
                "\n",
                "# Test queries\n",
                "test_queries = [\n",
                "    # Valid Databricks queries\n",
                "    \"How do I create a cluster in Databricks?\",\n",
                "    \"Explain MLflow experiment tracking\",\n",
                "    \"What is Delta Lake?\",\n",
                "    \"Show me how to use Databricks SQL\",\n",
                "    \n",
                "    # Borderline queries\n",
                "    \"How do I use Apache Spark?\",  # Spark is used in Databricks\n",
                "    \"What is Python?\",  # Too general\n",
                "    \n",
                "    # Off-topic queries\n",
                "    \"What's the weather today?\",\n",
                "    \"Tell me a joke\",\n",
                "    \"How do I cook pasta?\",\n",
                "    \"What is AWS Lambda?\"\n",
                "]\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"INPUT GUARDRAIL TEST (Moderate Strictness)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for query in test_queries:\n",
                "    result = input_guardrail_moderate.validate(query)\n",
                "    \n",
                "    status = \"✅ VALID\" if result.is_valid else \"❌ REJECTED\"\n",
                "    print(f\"\\n{status} | Confidence: {result.confidence:.2f}\")\n",
                "    print(f\"Query: {query}\")\n",
                "    print(f\"Category: {result.category}\")\n",
                "    print(f\"Reason: {result.reason}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare Strictness Levels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test same query with different strictness levels\n",
                "borderline_query = \"How do I use Apache Spark for data processing?\"\n",
                "\n",
                "print(f\"Testing query: '{borderline_query}'\\n\")\n",
                "\n",
                "for strictness in [\"strict\", \"moderate\", \"lenient\"]:\n",
                "    guardrail = InputGuardrail(\n",
                "        llm_endpoint=LLM_ENDPOINT,\n",
                "        strictness=strictness,\n",
                "        log_to_mlflow=False\n",
                "    )\n",
                "    \n",
                "    result = guardrail.validate(borderline_query)\n",
                "    status = \"✅ VALID\" if result.is_valid else \"❌ REJECTED\"\n",
                "    \n",
                "    print(f\"{strictness.upper():10} | {status} | Confidence: {result.confidence:.2f}\")\n",
                "    print(f\"           Reason: {result.reason}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Output Guardrail Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output guardrail\n",
                "output_guardrail = OutputGuardrail(\n",
                "    llm_endpoint=LLM_ENDPOINT,\n",
                "    log_to_mlflow=False\n",
                ")\n",
                "\n",
                "# Test cases: query + response pairs\n",
                "test_cases = [\n",
                "    {\n",
                "        \"query\": \"How do I create a Databricks cluster?\",\n",
                "        \"response\": \"To create a Databricks cluster, go to the Compute section and click 'Create Cluster'. Configure the cluster settings including Spark version, node types, and auto-termination settings.\",\n",
                "        \"expected\": \"valid\"\n",
                "    },\n",
                "    {\n",
                "        \"query\": \"What is MLflow?\",\n",
                "        \"response\": \"MLflow is an open-source platform for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment. It's fully integrated with Databricks.\",\n",
                "        \"expected\": \"valid\"\n",
                "    },\n",
                "    {\n",
                "        \"query\": \"How do I use Databricks?\",\n",
                "        \"response\": \"Let me tell you about my favorite recipe for chocolate cake. First, you need to preheat the oven to 350 degrees...\",\n",
                "        \"expected\": \"invalid\"\n",
                "    },\n",
                "    {\n",
                "        \"query\": \"Explain Delta Lake\",\n",
                "        \"response\": \"Delta Lake is a storage layer that brings ACID transactions to Apache Spark and big data workloads. Also, did you know that the weather is nice today?\",\n",
                "        \"expected\": \"invalid\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"OUTPUT GUARDRAIL TEST\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for i, test in enumerate(test_cases, 1):\n",
                "    result = output_guardrail.validate(test[\"query\"], test[\"response\"])\n",
                "    \n",
                "    status = \"✅ VALID\" if result.is_valid else \"❌ INVALID\"\n",
                "    expected = test[\"expected\"].upper()\n",
                "    \n",
                "    print(f\"\\nTest {i}: {status} (Expected: {expected})\")\n",
                "    print(f\"Query: {test['query']}\")\n",
                "    print(f\"Response: {test['response'][:100]}...\")\n",
                "    print(f\"Confidence: {result.confidence:.2f}\")\n",
                "    print(f\"Reason: {result.reason}\")\n",
                "    if result.metadata and \"issues\" in result.metadata:\n",
                "        print(f\"Issues: {result.metadata['issues']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Complete Conversational Agent\n",
                "\n",
                "Now let's use the full agent with guardrails and conversation memory."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create Agent with Existing Retrieval Tools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create retrieval tools (same as in 02-Knowledge-Base.ipynb)\n",
                "generic_retriever = VectorSearchRetrieverTool(\n",
                "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
                "    index_name=VECTOR_SEARCH_INDEX,\n",
                "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\"],\n",
                "    tool_name=\"generic_doc_retriever\",\n",
                "    tool_description=\"Retrieves generic documentation for generic queries.\",\n",
                "    filters={\"doc_type\": \"general\"},\n",
                "    num_results=5,\n",
                "    disable_notice=True\n",
                ")\n",
                "\n",
                "api_retriever = VectorSearchRetrieverTool(\n",
                "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
                "    index_name=VECTOR_SEARCH_INDEX,\n",
                "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\"],\n",
                "    tool_name=\"api_docs_retriever\",\n",
                "    tool_description=\"Retrieves API reference documentation.\",\n",
                "    filters={\"doc_type\": \"api_reference\"},\n",
                "    num_results=5,\n",
                "    disable_notice=True\n",
                ")\n",
                "\n",
                "tutorial_retriever = VectorSearchRetrieverTool(\n",
                "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
                "    index_name=VECTOR_SEARCH_INDEX,\n",
                "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\"],\n",
                "    tool_name=\"tutorial_retriever\",\n",
                "    tool_description=\"Retrieves tutorial and how-to guides.\",\n",
                "    filters={\"doc_type\": \"tutorial\"},\n",
                "    num_results=5,\n",
                "    disable_notice=True\n",
                ")\n",
                "\n",
                "code_retriever = VectorSearchRetrieverTool(\n",
                "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
                "    index_name=VECTOR_SEARCH_INDEX,\n",
                "    columns=[\"chunk_id\", \"doc_id\", \"text\", \"url\", \"doc_type\", \"has_code\"],\n",
                "    tool_name=\"code_examples_retriever\",\n",
                "    tool_description=\"Retrieves documentation with code examples.\",\n",
                "    filters={\"has_code\": \"true\"},\n",
                "    num_results=5,\n",
                "    disable_notice=True\n",
                ")\n",
                "\n",
                "retrieval_tools = [generic_retriever, api_retriever, tutorial_retriever, code_retriever]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the agent with guardrails\n",
                "agent = DatabricksDocAgent(\n",
                "    retrieval_tools=retrieval_tools,\n",
                "    llm_endpoint=LLM_ENDPOINT,\n",
                "    guardrail_strictness=\"moderate\",\n",
                "    enable_input_guardrail=True,\n",
                "    enable_output_guardrail=True,\n",
                "    max_conversation_history=10,\n",
                "    log_to_mlflow=True\n",
                ")\n",
                "\n",
                "print(\"✅ Agent created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Valid Databricks Queries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with a valid Databricks query\n",
                "with mlflow.start_run(run_name=\"valid_query_test\"):\n",
                "    result = agent.query(\n",
                "        user_query=\"How do I use MLflow with Databricks jobs?\",\n",
                "        conversation_id=\"test_conv_1\"\n",
                "    )\n",
                "    \n",
                "    print(\"=\" * 80)\n",
                "    print(\"QUERY RESULT\")\n",
                "    print(\"=\" * 80)\n",
                "    print(f\"\\nWas Rejected: {result['was_rejected']}\")\n",
                "    print(f\"Conversation ID: {result['conversation_id']}\")\n",
                "    print(f\"\\nLatency: {result['metadata']['latency_ms']:.0f}ms\")\n",
                "    print(f\"Tool Calls: {result['metadata']['tool_calls']}\")\n",
                "    print(f\"Tools Used: {result['metadata']['tools_used']}\")\n",
                "    \n",
                "    if result['guardrail_input']:\n",
                "        print(f\"\\nInput Guardrail:\")\n",
                "        print(f\"  Valid: {result['guardrail_input'].is_valid}\")\n",
                "        print(f\"  Confidence: {result['guardrail_input'].confidence:.2f}\")\n",
                "        print(f\"  Category: {result['guardrail_input'].category}\")\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(\"RESPONSE\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result['response'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Off-Topic Query (Should be Rejected)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with an off-topic query\n",
                "with mlflow.start_run(run_name=\"rejected_query_test\"):\n",
                "    result = agent.query(\n",
                "        user_query=\"What's the best recipe for chocolate cake?\",\n",
                "        conversation_id=\"test_conv_2\"\n",
                "    )\n",
                "    \n",
                "    print(\"=\" * 80)\n",
                "    print(\"OFF-TOPIC QUERY TEST\")\n",
                "    print(\"=\" * 80)\n",
                "    print(f\"\\nWas Rejected: {result['was_rejected']}\")\n",
                "    print(f\"Latency: {result['metadata']['latency_ms']:.0f}ms\")\n",
                "    print(f\"Tool Calls: {result['metadata']['tool_calls']} (should be 0)\")\n",
                "    \n",
                "    if result['guardrail_input']:\n",
                "        print(f\"\\nInput Guardrail:\")\n",
                "        print(f\"  Valid: {result['guardrail_input'].is_valid}\")\n",
                "        print(f\"  Confidence: {result['guardrail_input'].confidence:.2f}\")\n",
                "        print(f\"  Reason: {result['guardrail_input'].reason}\")\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(\"REJECTION MESSAGE\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result['response'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Multi-Turn Conversation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multi-turn conversation test\n",
                "conversation_id = \"multi_turn_test\"\n",
                "\n",
                "queries = [\n",
                "    \"What is Delta Lake?\",\n",
                "    \"How do I create a Delta table?\",\n",
                "    \"Can you show me a code example?\",\n",
                "    \"What about time travel in Delta?\"\n",
                "]\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"MULTI-TURN CONVERSATION TEST\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "with mlflow.start_run(run_name=\"multi_turn_conversation\"):\n",
                "    for i, query in enumerate(queries, 1):\n",
                "        print(f\"\\n{'='*80}\")\n",
                "        print(f\"Turn {i}: {query}\")\n",
                "        print(\"=\" * 80)\n",
                "        \n",
                "        result = agent.query(\n",
                "            user_query=query,\n",
                "            conversation_id=conversation_id,\n",
                "            include_history=True\n",
                "        )\n",
                "        \n",
                "        print(f\"\\nResponse (Turn {i}/{len(queries)}):\")\n",
                "        print(result['response'][:500] + \"...\" if len(result['response']) > 500 else result['response'])\n",
                "        print(f\"\\nTools Used: {result['metadata']['tools_used']}\")\n",
                "        print(f\"Conversation Turn: {result['metadata']['conversation_turn']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### View Conversation History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get conversation history\n",
                "history = agent.get_conversation_history(conversation_id)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(f\"CONVERSATION HISTORY ({len(history)} turns)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for i, turn in enumerate(history, 1):\n",
                "    print(f\"\\n--- Turn {i} ---\")\n",
                "    print(f\"User: {turn['user']}\")\n",
                "    print(f\"Assistant: {turn['assistant'][:200]}...\")\n",
                "    print(f\"Tools: {turn['tool_calls']}\")\n",
                "    print(f\"Rejected: {turn['was_rejected']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Comprehensive Testing with MLflow\n",
                "\n",
                "Test the agent with a variety of queries and track metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive test suite\n",
                "test_suite = [\n",
                "    # Valid Databricks queries\n",
                "    {\"query\": \"How do I create a cluster in Databricks?\", \"expected\": \"valid\"},\n",
                "    {\"query\": \"Explain MLflow experiment tracking\", \"expected\": \"valid\"},\n",
                "    {\"query\": \"What is Unity Catalog?\", \"expected\": \"valid\"},\n",
                "    {\"query\": \"Show me how to use Databricks SQL\", \"expected\": \"valid\"},\n",
                "    {\"query\": \"How do I schedule a Databricks job?\", \"expected\": \"valid\"},\n",
                "    \n",
                "    # Borderline queries\n",
                "    {\"query\": \"How do I use Apache Spark?\", \"expected\": \"borderline\"},\n",
                "    {\"query\": \"What is Python?\", \"expected\": \"borderline\"},\n",
                "    \n",
                "    # Off-topic queries\n",
                "    {\"query\": \"What's the weather today?\", \"expected\": \"invalid\"},\n",
                "    {\"query\": \"Tell me a joke\", \"expected\": \"invalid\"},\n",
                "    {\"query\": \"How do I cook pasta?\", \"expected\": \"invalid\"},\n",
                "]\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"COMPREHENSIVE TEST SUITE\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "results_summary = {\n",
                "    \"total\": 0,\n",
                "    \"valid\": 0,\n",
                "    \"rejected\": 0,\n",
                "    \"total_tool_calls\": 0,\n",
                "    \"total_latency_ms\": 0\n",
                "}\n",
                "\n",
                "with mlflow.start_run(run_name=\"comprehensive_test_suite\"):\n",
                "    for i, test in enumerate(test_suite, 1):\n",
                "        result = agent.query(\n",
                "            user_query=test[\"query\"],\n",
                "            conversation_id=f\"test_{i}\"\n",
                "        )\n",
                "        \n",
                "        results_summary[\"total\"] += 1\n",
                "        if result[\"was_rejected\"]:\n",
                "            results_summary[\"rejected\"] += 1\n",
                "        else:\n",
                "            results_summary[\"valid\"] += 1\n",
                "        \n",
                "        results_summary[\"total_tool_calls\"] += result[\"metadata\"][\"tool_calls\"]\n",
                "        results_summary[\"total_latency_ms\"] += result[\"metadata\"][\"latency_ms\"]\n",
                "        \n",
                "        status = \"❌ REJECTED\" if result[\"was_rejected\"] else \"✅ VALID\"\n",
                "        expected = test[\"expected\"].upper()\n",
                "        \n",
                "        print(f\"\\n{i}. {status} (Expected: {expected})\")\n",
                "        print(f\"   Query: {test['query']}\")\n",
                "        print(f\"   Latency: {result['metadata']['latency_ms']:.0f}ms | Tools: {result['metadata']['tool_calls']}\")\n",
                "    \n",
                "    # Log summary metrics\n",
                "    mlflow.log_metrics({\n",
                "        \"total_queries\": results_summary[\"total\"],\n",
                "        \"valid_queries\": results_summary[\"valid\"],\n",
                "        \"rejected_queries\": results_summary[\"rejected\"],\n",
                "        \"rejection_rate\": results_summary[\"rejected\"] / results_summary[\"total\"],\n",
                "        \"avg_tool_calls\": results_summary[\"total_tool_calls\"] / results_summary[\"valid\"] if results_summary[\"valid\"] > 0 else 0,\n",
                "        \"avg_latency_ms\": results_summary[\"total_latency_ms\"] / results_summary[\"total\"]\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(\"SUMMARY\")\n",
                "    print(\"=\" * 80)\n",
                "    print(f\"Total Queries: {results_summary['total']}\")\n",
                "    print(f\"Valid: {results_summary['valid']}\")\n",
                "    print(f\"Rejected: {results_summary['rejected']}\")\n",
                "    print(f\"Rejection Rate: {results_summary['rejected'] / results_summary['total']:.1%}\")\n",
                "    print(f\"Avg Tool Calls (valid queries): {results_summary['total_tool_calls'] / results_summary['valid'] if results_summary['valid'] > 0 else 0:.1f}\")\n",
                "    print(f\"Avg Latency: {results_summary['total_latency_ms'] / results_summary['total']:.0f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: Guardrail Metrics and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get guardrail metrics summary\n",
                "metrics_summary = agent.get_metrics_summary()\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"GUARDRAIL METRICS SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nTotal Queries: {metrics_summary['total_queries']}\")\n",
                "print(f\"Valid Queries: {metrics_summary['valid_queries']}\")\n",
                "print(f\"Rejected Queries: {metrics_summary['rejected_queries']}\")\n",
                "print(f\"Rejection Rate: {metrics_summary['rejection_rate']:.1%}\")\n",
                "print(f\"\\nAvg Input Confidence: {metrics_summary['avg_input_confidence']:.2f}\")\n",
                "print(f\"Avg Output Confidence: {metrics_summary['avg_output_confidence']:.2f}\")\n",
                "print(f\"\\nTotal Output Checks: {metrics_summary['total_output_checks']}\")\n",
                "print(f\"Valid Outputs: {metrics_summary['valid_outputs']}\")\n",
                "\n",
                "# Export to MLflow\n",
                "agent.export_metrics_to_mlflow()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: Interactive Testing\n",
                "\n",
                "Use this cell to interactively test the agent with your own queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive testing function\n",
                "def ask_agent(query: str, conversation_id: str = \"interactive\"):\n",
                "    \"\"\"Ask the agent a question and display the result.\"\"\"\n",
                "    result = agent.query(\n",
                "        user_query=query,\n",
                "        conversation_id=conversation_id,\n",
                "        include_history=True\n",
                "    )\n",
                "    \n",
                "    print(\"=\" * 80)\n",
                "    if result[\"was_rejected\"]:\n",
                "        print(\"❌ QUERY REJECTED\")\n",
                "    else:\n",
                "        print(\"✅ QUERY ACCEPTED\")\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    print(f\"\\nQuery: {query}\")\n",
                "    print(f\"\\nLatency: {result['metadata']['latency_ms']:.0f}ms\")\n",
                "    print(f\"Tool Calls: {result['metadata']['tool_calls']}\")\n",
                "    \n",
                "    if result['guardrail_input']:\n",
                "        print(f\"\\nInput Guardrail:\")\n",
                "        print(f\"  Category: {result['guardrail_input'].category}\")\n",
                "        print(f\"  Confidence: {result['guardrail_input'].confidence:.2f}\")\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(\"RESPONSE\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result['response'])\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Example usage:\n",
                "# ask_agent(\"How do I use Delta Lake with Databricks?\")\n",
                "# ask_agent(\"What's the capital of France?\")  # Should be rejected"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6: Deployment-Ready Endpoint\n",
                "\n",
                "Example of how to wrap the agent for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deployment wrapper\n",
                "class DatabricksDocEndpoint:\n",
                "    \"\"\"Deployment-ready endpoint wrapper.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.agent = DatabricksDocAgent(\n",
                "            retrieval_tools=retrieval_tools,\n",
                "            llm_endpoint=LLM_ENDPOINT,\n",
                "            guardrail_strictness=\"moderate\",\n",
                "            enable_input_guardrail=True,\n",
                "            enable_output_guardrail=True,\n",
                "            log_to_mlflow=True\n",
                "        )\n",
                "    \n",
                "    def predict(self, model_input: dict) -> dict:\n",
                "        \"\"\"\n",
                "        Prediction method for MLflow model serving.\n",
                "        \n",
                "        Input format:\n",
                "        {\n",
                "            \"query\": \"user question\",\n",
                "            \"conversation_id\": \"optional_conv_id\"\n",
                "        }\n",
                "        \"\"\"\n",
                "        query = model_input.get(\"query\", \"\")\n",
                "        conversation_id = model_input.get(\"conversation_id\", None)\n",
                "        \n",
                "        result = self.agent.query(\n",
                "            user_query=query,\n",
                "            conversation_id=conversation_id\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            \"response\": result[\"response\"],\n",
                "            \"was_rejected\": result[\"was_rejected\"],\n",
                "            \"conversation_id\": result[\"conversation_id\"],\n",
                "            \"metadata\": result[\"metadata\"]\n",
                "        }\n",
                "\n",
                "# Test the endpoint\n",
                "endpoint = DatabricksDocEndpoint()\n",
                "\n",
                "test_input = {\n",
                "    \"query\": \"How do I use MLflow autologging?\",\n",
                "    \"conversation_id\": \"endpoint_test\"\n",
                "}\n",
                "\n",
                "response = endpoint.predict(test_input)\n",
                "print(\"Endpoint Response:\")\n",
                "print(json.dumps(response, indent=2, default=str))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "\n",
                "1. **Input Guardrails** - Validate queries before processing\n",
                "   - Configurable strictness levels (strict/moderate/lenient)\n",
                "   - LLM-based classification\n",
                "   - ~100-200ms latency overhead\n",
                "\n",
                "2. **Output Guardrails** - Ensure response quality\n",
                "   - Detects off-topic content\n",
                "   - Identifies potential hallucinations\n",
                "   - Can trigger regeneration\n",
                "\n",
                "3. **Conversational Memory** - Multi-turn conversations\n",
                "   - Maintains context across turns\n",
                "   - Configurable history length\n",
                "   - Per-conversation isolation\n",
                "\n",
                "4. **Rejection Handling** - Polite off-topic responses\n",
                "   - Helpful suggestions\n",
                "   - No wasted tool calls\n",
                "   - Maintains user experience\n",
                "\n",
                "5. **MLflow Integration** - Complete observability\n",
                "   - Guardrail decisions\n",
                "   - Performance metrics\n",
                "   - Conversation tracking\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Deploy as MLflow model\n",
                "- Add custom guardrail rules\n",
                "- Implement feedback loop\n",
                "- A/B test strictness levels"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}